{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chords Detection"
      ],
      "metadata": {
        "id": "cneAp_pVTP7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "import librosa, librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "1vFsQExwaSpK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename='chords/C_Classic_Jo_1.wav'\n",
        "plt.figure(figsize=(15,5))\n",
        "data,sample_rate=librosa.load(filename)\n",
        "librosa.display.waveshow(data, sr=sample_rate)\n",
        "ipd.Audio(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "3awZVJ1yanV1",
        "outputId": "15e8d7fd-2e80-4b37-c0f9-7b8177187b04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ],
            "text/html": [
              "\n",
              "                <audio  controls=\"controls\" >\n",
              "                    <source src=\"data:audio/x-wav;base64,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\" type=\"audio/x-wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhUcYqu6b0hc",
        "outputId": "e1270cf8-ff06-47a3-e10d-ebbd8d91ca65"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22050"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "metadata = pd.read_csv('chords/metadata/chords.csv')\n",
        "metadata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oPq0L3w3b-V7",
        "outputId": "4f7acab6-9327-4f02-d706-705489c78f4b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            slice_file_name  fold  classID class\n",
              "0   Am_AcustiicPlug14_3.wav     1        0    Am\n",
              "1   Am_AcustiicPlug14_4.wav     1        0    Am\n",
              "2  Am_Classic_Jegundo_1.wav     1        0    Am\n",
              "3  Am_Classic_Jegundo_2.wav     1        0    Am\n",
              "4  Am_Classic_Jegundo_3.wav     1        0    Am"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df5c4561-da34-4953-b00a-0fd1a2a53adf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_file_name</th>\n",
              "      <th>fold</th>\n",
              "      <th>classID</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Am_AcustiicPlug14_3.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Am_AcustiicPlug14_4.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Am_Classic_Jegundo_1.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Am_Classic_Jegundo_2.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Am_Classic_Jegundo_3.wav</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df5c4561-da34-4953-b00a-0fd1a2a53adf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df5c4561-da34-4953-b00a-0fd1a2a53adf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df5c4561-da34-4953-b00a-0fd1a2a53adf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86d90b3e-5ea3-4287-a0fd-d848099430c4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86d90b3e-5ea3-4287-a0fd-d848099430c4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86d90b3e-5ea3-4287-a0fd-d848099430c4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metadata",
              "summary": "{\n  \"name\": \"metadata\",\n  \"rows\": 1407,\n  \"fields\": [\n    {\n      \"column\": \"slice_file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1406,\n        \"samples\": [\n          \"F_AcusticPlug7_4.wav\",\n          \"Em_RetroGB_JO_2.wav\",\n          \"C_Classic_Simona_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2,\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1,\n          5,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Bb\",\n          \"Em\",\n          \"Am\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "metadata.groupby('class').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "d4dA3qqHcbrn",
        "outputId": "6f53f3b3-533b-4d1c-aa95-1f54ce600bb6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata['class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKoQ6vFscvL9",
        "outputId": "a50da8aa-f799-47e0-b094-4ef42c2fb48e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "Bb      176\n",
              "Bdim    176\n",
              "C       176\n",
              "Dm      176\n",
              "Em      176\n",
              "F       176\n",
              "G       176\n",
              "Am      175\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "librosa_data, librosa_sr = librosa.load(filename)\n",
        "print(f'librosa_data {librosa_data}')\n",
        "print(f'librosa_sr {librosa_sr}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouWliD3Bc5R-",
        "outputId": "42f8924d-b40d-44ea-ee32-c90738e1e7a6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "librosa_data [-5.6869801e-05  5.2152136e-06  8.4168380e-05 ...  1.9838037e-03\n",
            "  1.7242797e-03  0.0000000e+00]\n",
            "librosa_sr 22050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mel-Frenquency Cepstral Coefficients (MFCC)\n",
        "mfccs = librosa.feature.mfcc(y=librosa_data, sr=librosa_sr, n_mfcc=40)"
      ],
      "metadata": {
        "id": "giUfbYT9dOEI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr4hWKi0dsN2",
        "outputId": "e7742865-1634-46eb-c6e5-81d38db141a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 184)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdQ6pzzCds5s",
        "outputId": "11c5135d-e276-44a3-ce4c-a22580cee39f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.5615741e+02, -5.5525928e+02, -5.5503943e+02, ...,\n",
              "        -5.3985364e+02, -5.3988257e+02, -5.3648285e+02],\n",
              "       [ 7.4535362e-02,  1.2760322e+00,  1.5549465e+00, ...,\n",
              "         2.2307585e+01,  2.2249075e+01,  2.6810156e+01],\n",
              "       [ 6.5565877e-02,  1.0849549e+00,  1.2819287e+00, ...,\n",
              "         1.9943954e+01,  1.9839462e+01,  2.3699919e+01],\n",
              "       ...,\n",
              "       [-4.0711410e-02, -3.5326970e-01, -6.3930470e-01, ...,\n",
              "        -2.6343274e+00, -2.9391139e+00, -2.7761359e+00],\n",
              "       [-2.0698562e-02, -3.0164260e-01, -4.5864993e-01, ...,\n",
              "         3.3975238e-01,  2.6510641e-01, -1.7730430e-01],\n",
              "       [ 9.5234811e-04, -2.5997955e-01, -2.9510069e-01, ...,\n",
              "         3.0086198e+00,  3.1810246e+00,  2.2343163e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "audio_dataset_path = 'chords/audio'"
      ],
      "metadata": {
        "id": "_k-wBtOddx2Q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract MFCCs from files\n",
        "def feature_extraction(file):\n",
        "  # audio & sample rate\n",
        "  audio, sr = librosa.load(file)\n",
        "  # get mfcc\n",
        "  mfccs_feature = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "  # mean\n",
        "  mfcc_scaled_features = np.mean(mfccs_feature.T, axis=0)\n",
        "\n",
        "  return mfcc_scaled_features"
      ],
      "metadata": {
        "id": "KPZOzb4PeHkv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "extracted_feature = []\n",
        "\n",
        "for index_num,row in tqdm(metadata.iterrows()):\n",
        "  file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
        "  file_classname = row['class']\n",
        "  data = feature_extraction(file_name)\n",
        "  extracted_feature.append([data, file_classname])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPZB6Bq-e-xi",
        "outputId": "29d527ca-c564-4d9d-9d0b-18d5cddf43ce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1407it [01:02, 22.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_feature[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNDkYxvMha4z",
        "outputId": "4e1f016a-0e32-4233-8fec-85d439f6abba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-3.0971078e+02,  1.1506919e+02, -8.8533611e+00,  6.6214233e+01,\n",
              "         1.9402972e-01, -4.6916938e+00,  1.4225300e+01, -1.2353893e+01,\n",
              "         5.9352007e+00,  4.6312776e+00, -1.2980490e+01, -5.9615541e+00,\n",
              "        -8.1475115e+00, -8.3484125e+00, -5.9580154e+00, -5.1721649e+00,\n",
              "        -1.1772553e+01, -1.5589064e+01, -5.9439216e+00, -6.9526234e+00,\n",
              "        -1.4241689e+01, -3.3740153e+00, -1.0240321e+01, -5.3737960e+00,\n",
              "         8.9527899e-01, -8.5322609e+00,  5.1281462e+00,  3.0711598e+00,\n",
              "        -1.0250739e+01, -7.2076473e+00, -9.9633884e+00, -2.3603001e+00,\n",
              "         6.3385353e+00, -6.6727128e+00, -1.4114930e+01, -1.0738763e+01,\n",
              "        -1.4685845e+01, -3.0767446e+00, -1.4012378e+00, -7.3483510e+00],\n",
              "       dtype=float32),\n",
              " 'Am']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_feature_df = pd.DataFrame(extracted_feature, columns=['feature', 'class'])\n",
        "extracted_feature_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eTyF76rVhnBC",
        "outputId": "dcc298e0-bf66-4472-d96e-16057dc379e1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature class\n",
              "0  [-309.7108, 115.06919, -8.853361, 66.21423, 0....    Am\n",
              "1  [-325.1825, 89.518776, -22.43942, 59.707214, -...    Am\n",
              "2  [-426.12827, 68.30331, 17.862837, 11.890995, 1...    Am\n",
              "3  [-426.51755, 73.99858, 20.323807, 10.844854, 1...    Am\n",
              "4  [-378.9357, 90.37813, 20.212006, 14.703562, 15...    Am"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3699152f-caa2-45a1-a48f-e4ec63d519fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-309.7108, 115.06919, -8.853361, 66.21423, 0....</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-325.1825, 89.518776, -22.43942, 59.707214, -...</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-426.12827, 68.30331, 17.862837, 11.890995, 1...</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-426.51755, 73.99858, 20.323807, 10.844854, 1...</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-378.9357, 90.37813, 20.212006, 14.703562, 15...</td>\n",
              "      <td>Am</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3699152f-caa2-45a1-a48f-e4ec63d519fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3699152f-caa2-45a1-a48f-e4ec63d519fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3699152f-caa2-45a1-a48f-e4ec63d519fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0f05945-ec22-4761-9e3b-ab313663044e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0f05945-ec22-4761-9e3b-ab313663044e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0f05945-ec22-4761-9e3b-ab313663044e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "extracted_feature_df",
              "summary": "{\n  \"name\": \"extracted_feature_df\",\n  \"rows\": 1407,\n  \"fields\": [\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Bb\",\n          \"Em\",\n          \"Am\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(extracted_feature_df['feature'].to_list())\n",
        "y = np.array(extracted_feature_df['class'].to_list())\n",
        "\n",
        "X[0:3],y[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTf3Bt7niBrs",
        "outputId": "8471b23d-a967-4bb1-ffb1-28c485667630"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-3.0971078e+02,  1.1506919e+02, -8.8533611e+00,  6.6214233e+01,\n",
              "          1.9402972e-01, -4.6916938e+00,  1.4225300e+01, -1.2353893e+01,\n",
              "          5.9352007e+00,  4.6312776e+00, -1.2980490e+01, -5.9615541e+00,\n",
              "         -8.1475115e+00, -8.3484125e+00, -5.9580154e+00, -5.1721649e+00,\n",
              "         -1.1772553e+01, -1.5589064e+01, -5.9439216e+00, -6.9526234e+00,\n",
              "         -1.4241689e+01, -3.3740153e+00, -1.0240321e+01, -5.3737960e+00,\n",
              "          8.9527899e-01, -8.5322609e+00,  5.1281462e+00,  3.0711598e+00,\n",
              "         -1.0250739e+01, -7.2076473e+00, -9.9633884e+00, -2.3603001e+00,\n",
              "          6.3385353e+00, -6.6727128e+00, -1.4114930e+01, -1.0738763e+01,\n",
              "         -1.4685845e+01, -3.0767446e+00, -1.4012378e+00, -7.3483510e+00],\n",
              "        [-3.2518250e+02,  8.9518776e+01, -2.2439421e+01,  5.9707214e+01,\n",
              "         -1.3036855e+01, -2.3318296e+01,  2.3421345e+00, -2.7998180e+01,\n",
              "         -8.1943722e+00, -2.6314318e+00, -2.3077185e+01, -1.3182934e+01,\n",
              "         -9.9649982e+00, -1.3075550e+01, -1.0693738e+01, -6.8927436e+00,\n",
              "         -1.1910432e+01, -1.7941904e+01, -6.5398364e+00, -1.4168976e-01,\n",
              "          3.8940105e-01,  8.2218075e+00, -3.9416106e+00, -3.9377601e+00,\n",
              "         -1.0962836e+00, -9.0492125e+00,  9.8064632e+00,  1.1295771e+01,\n",
              "         -4.0239658e+00, -4.9236073e+00, -6.8282685e+00,  4.0590243e+00,\n",
              "          1.2668100e+01, -6.1521316e+00, -1.6962093e+01, -1.4052627e+01,\n",
              "         -1.5552823e+01, -3.6144977e+00, -3.0400081e+00, -6.2899704e+00],\n",
              "        [-4.2612827e+02,  6.8303307e+01,  1.7862837e+01,  1.1890995e+01,\n",
              "          1.1611858e+01,  7.3079739e+00,  3.0166605e+00, -4.4861344e-01,\n",
              "         -2.8294258e+00, -3.1524732e+00, -5.8674488e+00, -4.4728932e+00,\n",
              "         -1.1684706e+00, -1.2095257e+00, -4.3664474e+00, -4.5354924e+00,\n",
              "         -6.0193338e+00, -5.1682143e+00, -2.5965528e+00, -3.8237879e+00,\n",
              "         -7.1712108e+00, -4.5192785e+00, -4.5874443e+00, -2.1573601e+00,\n",
              "          1.0645887e+00,  3.2257268e-01, -1.8537500e+00, -9.3839663e-01,\n",
              "         -1.0285034e+00, -3.4612603e+00, -4.9105597e+00,  2.8044057e-01,\n",
              "          3.0407476e+00, -8.3972329e-01, -7.5922804e+00, -5.8881178e+00,\n",
              "          1.5251702e-01, -6.8503857e-01, -4.3943939e+00, -2.8524528e+00]],\n",
              "       dtype=float32),\n",
              " array(['Am', 'Am', 'Am'], dtype='<U4'))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiUDUivMivmd",
        "outputId": "6ccf5861-be38-4284-d5fe-01025921fefa"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1407, 40), (1407,))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s-xp8wei505",
        "outputId": "7b5fb505-3b02-4c42-ddd2-6ec4c22413ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Am', 'Am', 'Am', ..., 'G', 'G', 'G'], dtype='<U4')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "y = to_categorical(label_encoder.fit_transform(y))\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dLjIpnmi9Zc",
        "outputId": "7d0e7a9e-fc22-450a-d03a-fd3d1a3eb0bc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "AyGCD1fWjyEY"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_train.shape, y_train, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-evq1NqFkHfX",
        "outputId": "8df8f208-1352-4cbc-b796-5f88704235c5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-373.22406  ,   21.667364 ,  -30.649807 , ...,   -1.6544251,\n",
              "            7.711043 ,    7.065225 ],\n",
              "        [-305.3827   ,  108.18011  ,  -12.004074 , ...,   -8.002079 ,\n",
              "           -8.157497 ,   -8.001125 ],\n",
              "        [-345.12994  ,   95.994606 ,    8.139511 , ...,   -6.2793517,\n",
              "           -8.103151 ,   -7.636834 ],\n",
              "        ...,\n",
              "        [-597.8758   ,  125.73642  ,   64.215614 , ...,   -1.670605 ,\n",
              "            2.5509121,    2.8766572],\n",
              "        [-274.99677  ,   89.3358   ,  -38.20967  , ...,   -6.7423306,\n",
              "           -8.745375 ,   -9.4262   ],\n",
              "        [-370.272    ,   81.09562  ,  -24.82861  , ...,  -16.228205 ,\n",
              "           -9.717879 ,   -2.4480333]], dtype=float32),\n",
              " (1125, 40),\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 1., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " (1125, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, X_test.shape, y_test, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OkKhhkEkO3k",
        "outputId": "399f2918-df77-4447-ceee-0b0ea5d048fb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-2.27037354e+02,  7.42858734e+01, -2.44091396e+01, ...,\n",
              "         -1.68179977e+00,  9.19192600e+00, -5.35246491e-01],\n",
              "        [-4.06912506e+02,  1.18390968e+02,  2.67664089e+01, ...,\n",
              "         -3.44206643e+00,  2.29387999e+00,  9.56597328e+00],\n",
              "        [-3.36270111e+02,  9.48563766e+01, -4.00220909e+01, ...,\n",
              "         -6.84437227e+00, -3.78074914e-01,  5.68849182e+00],\n",
              "        ...,\n",
              "        [-2.15829742e+02,  1.27222214e+02, -5.46982040e+01, ...,\n",
              "         -4.55747700e+00, -7.31461573e+00,  5.07300854e-01],\n",
              "        [-4.61144287e+02,  7.40537720e+01,  1.93616867e+01, ...,\n",
              "         -7.54623294e-01, -4.88550305e-01,  1.29764438e+00],\n",
              "        [-2.92398102e+02,  1.01955246e+02, -1.68932476e+01, ...,\n",
              "         -1.68453693e+01, -6.65383816e+00, -1.60117042e+00]], dtype=float32),\n",
              " (282, 40),\n",
              " array([[0., 0., 0., ..., 0., 1., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 1., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.]], dtype=float32),\n",
              " (282, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the model"
      ],
      "metadata": {
        "id": "aZcBM7j_komv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "9bsJGslLkiS2"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num of classes\n",
        "num_labels = y.shape[1]"
      ],
      "metadata": {
        "id": "y-zc0PKUlC-G"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model\n",
        "model=Sequential()\n",
        "# First layer\n",
        "model.add(Dense(100,input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# Second layer\n",
        "model.add(Dense(200))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# Third layer\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Final/Output layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "iFNxayo7lLK6"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlQbGj3-lhaP",
        "outputId": "754085f7-7c45-4931-a240-874230ed5af5"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 100)               4100      \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 200)               20200     \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 200)               0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 200)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 8)                 808       \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 8)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45208 (176.59 KB)\n",
            "Trainable params: 45208 (176.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
      ],
      "metadata": {
        "id": "Pj84nSalljM9"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training the model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from datetime import datetime\n",
        "\n",
        "num_epochs = 1000\n",
        "num_batch_size = 32\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/guitar_chords.hdf5',\n",
        "                               verbose=1, save_best_only=True)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "start = datetime.now()\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhXDs09Mltsz",
        "outputId": "cceb443a-c49d-4a06-846e-9c78325bb7c4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 54.7180 - accuracy: 0.1297 \n",
            "Epoch 1: val_loss improved from inf to 2.60688, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 1s 11ms/step - loss: 43.4921 - accuracy: 0.1307 - val_loss: 2.6069 - val_accuracy: 0.1489\n",
            "Epoch 2/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 17.7336 - accuracy: 0.1375\n",
            "Epoch 2: val_loss improved from 2.60688 to 2.08319, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 15.6213 - accuracy: 0.1351 - val_loss: 2.0832 - val_accuracy: 0.0957\n",
            "Epoch 3/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 9.2075 - accuracy: 0.1384 \n",
            "Epoch 3: val_loss improved from 2.08319 to 2.07835, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 8.3916 - accuracy: 0.1369 - val_loss: 2.0783 - val_accuracy: 0.1489\n",
            "Epoch 4/1000\n",
            "16/36 [============>.................] - ETA: 0s - loss: 5.9333 - accuracy: 0.1055\n",
            "Epoch 4: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 5.3530 - accuracy: 0.1271 - val_loss: 2.0791 - val_accuracy: 0.1489\n",
            "Epoch 5/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 3.9280 - accuracy: 0.1375\n",
            "Epoch 5: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 3.7118 - accuracy: 0.1316 - val_loss: 2.0793 - val_accuracy: 0.1489\n",
            "Epoch 6/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 3.0173 - accuracy: 0.1310\n",
            "Epoch 6: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.9250 - accuracy: 0.1369 - val_loss: 2.0794 - val_accuracy: 0.1489\n",
            "Epoch 7/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.7719 - accuracy: 0.1200\n",
            "Epoch 7: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.7719 - accuracy: 0.1200 - val_loss: 2.0796 - val_accuracy: 0.1489\n",
            "Epoch 8/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.4151 - accuracy: 0.1503\n",
            "Epoch 8: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.5024 - accuracy: 0.1262 - val_loss: 2.0797 - val_accuracy: 0.0993\n",
            "Epoch 9/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.4040 - accuracy: 0.1354\n",
            "Epoch 9: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.3574 - accuracy: 0.1351 - val_loss: 2.0800 - val_accuracy: 0.0993\n",
            "Epoch 10/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 2.2695 - accuracy: 0.1328\n",
            "Epoch 10: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.2685 - accuracy: 0.1298 - val_loss: 2.0801 - val_accuracy: 0.0993\n",
            "Epoch 11/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.2525 - accuracy: 0.1324\n",
            "Epoch 11: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.2525 - accuracy: 0.1324 - val_loss: 2.0803 - val_accuracy: 0.0993\n",
            "Epoch 12/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 2.2585 - accuracy: 0.1447\n",
            "Epoch 12: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.2416 - accuracy: 0.1253 - val_loss: 2.0806 - val_accuracy: 0.0993\n",
            "Epoch 13/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 2.1812 - accuracy: 0.1406\n",
            "Epoch 13: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1853 - accuracy: 0.1422 - val_loss: 2.0807 - val_accuracy: 0.0993\n",
            "Epoch 14/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.2036 - accuracy: 0.1354\n",
            "Epoch 14: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1673 - accuracy: 0.1422 - val_loss: 2.0808 - val_accuracy: 0.0993\n",
            "Epoch 15/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.1766 - accuracy: 0.1071\n",
            "Epoch 15: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1542 - accuracy: 0.1244 - val_loss: 2.0810 - val_accuracy: 0.0993\n",
            "Epoch 16/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 2.2285 - accuracy: 0.1290\n",
            "Epoch 16: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.2196 - accuracy: 0.1316 - val_loss: 2.0812 - val_accuracy: 0.0993\n",
            "Epoch 17/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 2.1438 - accuracy: 0.1398\n",
            "Epoch 17: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1506 - accuracy: 0.1307 - val_loss: 2.0813 - val_accuracy: 0.0993\n",
            "Epoch 18/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.1640 - accuracy: 0.1172\n",
            "Epoch 18: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1358 - accuracy: 0.1360 - val_loss: 2.0813 - val_accuracy: 0.0993\n",
            "Epoch 19/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 2.1492 - accuracy: 0.1266\n",
            "Epoch 19: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1416 - accuracy: 0.1378 - val_loss: 2.0812 - val_accuracy: 0.0993\n",
            "Epoch 20/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.1502 - accuracy: 0.1437\n",
            "Epoch 20: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1292 - accuracy: 0.1431 - val_loss: 2.0814 - val_accuracy: 0.0993\n",
            "Epoch 21/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 2.1030 - accuracy: 0.1337\n",
            "Epoch 21: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.1216 - accuracy: 0.1351 - val_loss: 2.0815 - val_accuracy: 0.0993\n",
            "Epoch 22/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.1066 - accuracy: 0.1387\n",
            "Epoch 22: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.1066 - accuracy: 0.1387 - val_loss: 2.0817 - val_accuracy: 0.0993\n",
            "Epoch 23/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 2.1189 - accuracy: 0.1365\n",
            "Epoch 23: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2.1117 - accuracy: 0.1360 - val_loss: 2.0816 - val_accuracy: 0.0993\n",
            "Epoch 24/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.1118 - accuracy: 0.1360\n",
            "Epoch 24: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2.1118 - accuracy: 0.1360 - val_loss: 2.0817 - val_accuracy: 0.0993\n",
            "Epoch 25/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 2.1066 - accuracy: 0.1351\n",
            "Epoch 25: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 2.1019 - accuracy: 0.1360 - val_loss: 2.0820 - val_accuracy: 0.0993\n",
            "Epoch 26/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 2.1010 - accuracy: 0.1321\n",
            "Epoch 26: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.1009 - accuracy: 0.1316 - val_loss: 2.0820 - val_accuracy: 0.0993\n",
            "Epoch 27/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 2.0865 - accuracy: 0.1496\n",
            "Epoch 27: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.0865 - accuracy: 0.1369 - val_loss: 2.0820 - val_accuracy: 0.0993\n",
            "Epoch 28/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 2.0938 - accuracy: 0.1505\n",
            "Epoch 28: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.0925 - accuracy: 0.1396 - val_loss: 2.0821 - val_accuracy: 0.0993\n",
            "Epoch 29/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.0944 - accuracy: 0.1422\n",
            "Epoch 29: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2.0944 - accuracy: 0.1422 - val_loss: 2.0820 - val_accuracy: 0.0993\n",
            "Epoch 30/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 2.0927 - accuracy: 0.1498\n",
            "Epoch 30: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 2.0908 - accuracy: 0.1467 - val_loss: 2.0811 - val_accuracy: 0.1028\n",
            "Epoch 31/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 2.0833 - accuracy: 0.1361\n",
            "Epoch 31: val_loss did not improve from 2.07835\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2.0797 - accuracy: 0.1387 - val_loss: 2.0810 - val_accuracy: 0.1028\n",
            "Epoch 32/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 2.0741 - accuracy: 0.1471\n",
            "Epoch 32: val_loss improved from 2.07835 to 2.07698, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 2.0745 - accuracy: 0.1431 - val_loss: 2.0770 - val_accuracy: 0.1064\n",
            "Epoch 33/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 2.0649 - accuracy: 0.1449\n",
            "Epoch 33: val_loss did not improve from 2.07698\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 2.0649 - accuracy: 0.1431 - val_loss: 2.0776 - val_accuracy: 0.1028\n",
            "Epoch 34/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 2.0628 - accuracy: 0.1518\n",
            "Epoch 34: val_loss improved from 2.07698 to 2.07644, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 2.0623 - accuracy: 0.1529 - val_loss: 2.0764 - val_accuracy: 0.1064\n",
            "Epoch 35/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 2.0638 - accuracy: 0.1581\n",
            "Epoch 35: val_loss improved from 2.07644 to 2.06196, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 2.0722 - accuracy: 0.1564 - val_loss: 2.0620 - val_accuracy: 0.1170\n",
            "Epoch 36/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 2.0752 - accuracy: 0.1543\n",
            "Epoch 36: val_loss did not improve from 2.06196\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.0698 - accuracy: 0.1564 - val_loss: 2.0641 - val_accuracy: 0.1170\n",
            "Epoch 37/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 2.0387 - accuracy: 0.1505\n",
            "Epoch 37: val_loss improved from 2.06196 to 2.04008, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.0388 - accuracy: 0.1556 - val_loss: 2.0401 - val_accuracy: 0.1312\n",
            "Epoch 38/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0914 - accuracy: 0.1622\n",
            "Epoch 38: val_loss did not improve from 2.04008\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0783 - accuracy: 0.1529 - val_loss: 2.0550 - val_accuracy: 0.1206\n",
            "Epoch 39/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0373 - accuracy: 0.1592\n",
            "Epoch 39: val_loss improved from 2.04008 to 2.02687, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0443 - accuracy: 0.1644 - val_loss: 2.0269 - val_accuracy: 0.1312\n",
            "Epoch 40/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0435 - accuracy: 0.1726\n",
            "Epoch 40: val_loss did not improve from 2.02687\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0493 - accuracy: 0.1609 - val_loss: 2.0269 - val_accuracy: 0.1348\n",
            "Epoch 41/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0244 - accuracy: 0.1533\n",
            "Epoch 41: val_loss improved from 2.02687 to 2.01420, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0348 - accuracy: 0.1547 - val_loss: 2.0142 - val_accuracy: 0.1418\n",
            "Epoch 42/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 2.0387 - accuracy: 0.1644\n",
            "Epoch 42: val_loss did not improve from 2.01420\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0387 - accuracy: 0.1644 - val_loss: 2.0193 - val_accuracy: 0.1418\n",
            "Epoch 43/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 2.0233 - accuracy: 0.1562\n",
            "Epoch 43: val_loss improved from 2.01420 to 2.00311, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 2.0258 - accuracy: 0.1564 - val_loss: 2.0031 - val_accuracy: 0.1454\n",
            "Epoch 44/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.0071 - accuracy: 0.1750\n",
            "Epoch 44: val_loss improved from 2.00311 to 1.99440, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0252 - accuracy: 0.1680 - val_loss: 1.9944 - val_accuracy: 0.1489\n",
            "Epoch 45/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 2.0133 - accuracy: 0.1632\n",
            "Epoch 45: val_loss improved from 1.99440 to 1.98547, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0171 - accuracy: 0.1689 - val_loss: 1.9855 - val_accuracy: 0.1560\n",
            "Epoch 46/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.0154 - accuracy: 0.1734\n",
            "Epoch 46: val_loss did not improve from 1.98547\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0183 - accuracy: 0.1644 - val_loss: 1.9896 - val_accuracy: 0.1489\n",
            "Epoch 47/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 2.0284 - accuracy: 0.1608\n",
            "Epoch 47: val_loss did not improve from 1.98547\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0279 - accuracy: 0.1618 - val_loss: 1.9856 - val_accuracy: 0.1525\n",
            "Epoch 48/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 2.0127 - accuracy: 0.1719\n",
            "Epoch 48: val_loss improved from 1.98547 to 1.97765, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 2.0083 - accuracy: 0.1707 - val_loss: 1.9776 - val_accuracy: 0.1560\n",
            "Epoch 49/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.0016 - accuracy: 0.1656\n",
            "Epoch 49: val_loss improved from 1.97765 to 1.96197, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9913 - accuracy: 0.1787 - val_loss: 1.9620 - val_accuracy: 0.1702\n",
            "Epoch 50/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 2.0040 - accuracy: 0.1795\n",
            "Epoch 50: val_loss improved from 1.96197 to 1.94863, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0025 - accuracy: 0.1796 - val_loss: 1.9486 - val_accuracy: 0.1738\n",
            "Epoch 51/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.0059 - accuracy: 0.1625\n",
            "Epoch 51: val_loss improved from 1.94863 to 1.94857, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9969 - accuracy: 0.1698 - val_loss: 1.9486 - val_accuracy: 0.1738\n",
            "Epoch 52/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0249 - accuracy: 0.1726\n",
            "Epoch 52: val_loss improved from 1.94857 to 1.93683, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 2.0140 - accuracy: 0.1733 - val_loss: 1.9368 - val_accuracy: 0.1738\n",
            "Epoch 53/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 1.9844 - accuracy: 0.1866\n",
            "Epoch 53: val_loss did not improve from 1.93683\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1.9822 - accuracy: 0.1849 - val_loss: 1.9492 - val_accuracy: 0.1738\n",
            "Epoch 54/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 2.0161 - accuracy: 0.1719\n",
            "Epoch 54: val_loss did not improve from 1.93683\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0162 - accuracy: 0.1707 - val_loss: 1.9396 - val_accuracy: 0.1738\n",
            "Epoch 55/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 1.9760 - accuracy: 0.1815\n",
            "Epoch 55: val_loss did not improve from 1.93683\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0030 - accuracy: 0.1716 - val_loss: 1.9380 - val_accuracy: 0.1738\n",
            "Epoch 56/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 2.0139 - accuracy: 0.1548\n",
            "Epoch 56: val_loss did not improve from 1.93683\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0122 - accuracy: 0.1689 - val_loss: 1.9372 - val_accuracy: 0.1738\n",
            "Epoch 57/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.9832 - accuracy: 0.1719\n",
            "Epoch 57: val_loss improved from 1.93683 to 1.93535, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9860 - accuracy: 0.1760 - val_loss: 1.9354 - val_accuracy: 0.1738\n",
            "Epoch 58/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.9729 - accuracy: 0.1911\n",
            "Epoch 58: val_loss improved from 1.93535 to 1.92148, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9735 - accuracy: 0.1902 - val_loss: 1.9215 - val_accuracy: 0.1738\n",
            "Epoch 59/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 1.9931 - accuracy: 0.1752\n",
            "Epoch 59: val_loss did not improve from 1.92148\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9935 - accuracy: 0.1751 - val_loss: 1.9233 - val_accuracy: 0.1738\n",
            "Epoch 60/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 2.0287 - accuracy: 0.1760\n",
            "Epoch 60: val_loss did not improve from 1.92148\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.9996 - accuracy: 0.1796 - val_loss: 1.9531 - val_accuracy: 0.1702\n",
            "Epoch 61/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 1.9800 - accuracy: 0.1752\n",
            "Epoch 61: val_loss did not improve from 1.92148\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9732 - accuracy: 0.1796 - val_loss: 1.9368 - val_accuracy: 0.1738\n",
            "Epoch 62/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 1.9761 - accuracy: 0.1927\n",
            "Epoch 62: val_loss improved from 1.92148 to 1.91483, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9884 - accuracy: 0.1831 - val_loss: 1.9148 - val_accuracy: 0.1738\n",
            "Epoch 63/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 1.9939 - accuracy: 0.1743\n",
            "Epoch 63: val_loss did not improve from 1.91483\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 2.0006 - accuracy: 0.1698 - val_loss: 1.9282 - val_accuracy: 0.1738\n",
            "Epoch 64/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 1.9748 - accuracy: 0.1597\n",
            "Epoch 64: val_loss did not improve from 1.91483\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.9763 - accuracy: 0.1751 - val_loss: 1.9175 - val_accuracy: 0.1809\n",
            "Epoch 65/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 1.9605 - accuracy: 0.1899\n",
            "Epoch 65: val_loss improved from 1.91483 to 1.91031, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1.9595 - accuracy: 0.1929 - val_loss: 1.9103 - val_accuracy: 0.1950\n",
            "Epoch 66/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 2.0032 - accuracy: 0.1743\n",
            "Epoch 66: val_loss improved from 1.91031 to 1.90491, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9978 - accuracy: 0.1733 - val_loss: 1.9049 - val_accuracy: 0.1844\n",
            "Epoch 67/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 1.9800 - accuracy: 0.1974\n",
            "Epoch 67: val_loss did not improve from 1.90491\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.9728 - accuracy: 0.1911 - val_loss: 1.9090 - val_accuracy: 0.1950\n",
            "Epoch 68/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 1.9807 - accuracy: 0.1760\n",
            "Epoch 68: val_loss improved from 1.90491 to 1.90423, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9690 - accuracy: 0.1902 - val_loss: 1.9042 - val_accuracy: 0.1986\n",
            "Epoch 69/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 1.9837 - accuracy: 0.1783\n",
            "Epoch 69: val_loss did not improve from 1.90423\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.9847 - accuracy: 0.1769 - val_loss: 1.9172 - val_accuracy: 0.1879\n",
            "Epoch 70/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 1.9625 - accuracy: 0.1802\n",
            "Epoch 70: val_loss improved from 1.90423 to 1.87672, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 1.9644 - accuracy: 0.1796 - val_loss: 1.8767 - val_accuracy: 0.2270\n",
            "Epoch 71/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 1.9506 - accuracy: 0.2013\n",
            "Epoch 71: val_loss improved from 1.87672 to 1.78157, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9512 - accuracy: 0.1991 - val_loss: 1.7816 - val_accuracy: 0.2730\n",
            "Epoch 72/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.9471 - accuracy: 0.2000\n",
            "Epoch 72: val_loss did not improve from 1.78157\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.9387 - accuracy: 0.2009 - val_loss: 1.8134 - val_accuracy: 0.2518\n",
            "Epoch 73/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.9142 - accuracy: 0.1982\n",
            "Epoch 73: val_loss improved from 1.78157 to 1.69986, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.9131 - accuracy: 0.1982 - val_loss: 1.6999 - val_accuracy: 0.2660\n",
            "Epoch 74/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.8877 - accuracy: 0.2161\n",
            "Epoch 74: val_loss improved from 1.69986 to 1.66895, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.8877 - accuracy: 0.2169 - val_loss: 1.6689 - val_accuracy: 0.2660\n",
            "Epoch 75/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.9023 - accuracy: 0.2080\n",
            "Epoch 75: val_loss improved from 1.66895 to 1.66317, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1.9028 - accuracy: 0.2080 - val_loss: 1.6632 - val_accuracy: 0.3085\n",
            "Epoch 76/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 1.8507 - accuracy: 0.2382\n",
            "Epoch 76: val_loss improved from 1.66317 to 1.59917, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.8507 - accuracy: 0.2382 - val_loss: 1.5992 - val_accuracy: 0.3936\n",
            "Epoch 77/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 1.8178 - accuracy: 0.2693\n",
            "Epoch 77: val_loss did not improve from 1.59917\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.8190 - accuracy: 0.2649 - val_loss: 1.6117 - val_accuracy: 0.3440\n",
            "Epoch 78/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 1.8102 - accuracy: 0.2795\n",
            "Epoch 78: val_loss improved from 1.59917 to 1.57542, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.8016 - accuracy: 0.2747 - val_loss: 1.5754 - val_accuracy: 0.4220\n",
            "Epoch 79/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.7914 - accuracy: 0.2531\n",
            "Epoch 79: val_loss did not improve from 1.57542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.7900 - accuracy: 0.2631 - val_loss: 1.5903 - val_accuracy: 0.3617\n",
            "Epoch 80/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.8290 - accuracy: 0.2703\n",
            "Epoch 80: val_loss improved from 1.57542 to 1.55166, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.8065 - accuracy: 0.2631 - val_loss: 1.5517 - val_accuracy: 0.4113\n",
            "Epoch 81/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 1.7585 - accuracy: 0.3040\n",
            "Epoch 81: val_loss improved from 1.55166 to 1.52539, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 1.7585 - accuracy: 0.3040 - val_loss: 1.5254 - val_accuracy: 0.4610\n",
            "Epoch 82/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.7260 - accuracy: 0.3047\n",
            "Epoch 82: val_loss improved from 1.52539 to 1.48904, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.7323 - accuracy: 0.3049 - val_loss: 1.4890 - val_accuracy: 0.4433\n",
            "Epoch 83/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 1.6986 - accuracy: 0.3212\n",
            "Epoch 83: val_loss improved from 1.48904 to 1.47968, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.7202 - accuracy: 0.2969 - val_loss: 1.4797 - val_accuracy: 0.4007\n",
            "Epoch 84/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.6923 - accuracy: 0.2859\n",
            "Epoch 84: val_loss improved from 1.47968 to 1.46070, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 1.6752 - accuracy: 0.3093 - val_loss: 1.4607 - val_accuracy: 0.4752\n",
            "Epoch 85/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 1.7030 - accuracy: 0.3174\n",
            "Epoch 85: val_loss improved from 1.46070 to 1.43997, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.6803 - accuracy: 0.3244 - val_loss: 1.4400 - val_accuracy: 0.4539\n",
            "Epoch 86/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.6163 - accuracy: 0.3527\n",
            "Epoch 86: val_loss improved from 1.43997 to 1.38867, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 1.6138 - accuracy: 0.3529 - val_loss: 1.3887 - val_accuracy: 0.4823\n",
            "Epoch 87/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.6297 - accuracy: 0.3516\n",
            "Epoch 87: val_loss improved from 1.38867 to 1.36436, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.3600 - val_loss: 1.3644 - val_accuracy: 0.4823\n",
            "Epoch 88/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.5400 - accuracy: 0.3750\n",
            "Epoch 88: val_loss improved from 1.36436 to 1.31122, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.5620 - accuracy: 0.3822 - val_loss: 1.3112 - val_accuracy: 0.5177\n",
            "Epoch 89/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 1.5558 - accuracy: 0.3844\n",
            "Epoch 89: val_loss improved from 1.31122 to 1.28183, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.5313 - accuracy: 0.4027 - val_loss: 1.2818 - val_accuracy: 0.5390\n",
            "Epoch 90/1000\n",
            "17/36 [=============>................] - ETA: 0s - loss: 1.4984 - accuracy: 0.4210\n",
            "Epoch 90: val_loss improved from 1.28183 to 1.17133, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 1.4578 - accuracy: 0.4258 - val_loss: 1.1713 - val_accuracy: 0.5603\n",
            "Epoch 91/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 1.4304 - accuracy: 0.4259\n",
            "Epoch 91: val_loss improved from 1.17133 to 1.12653, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 1.4357 - accuracy: 0.4062 - val_loss: 1.1265 - val_accuracy: 0.5426\n",
            "Epoch 92/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 1.4339 - accuracy: 0.3895\n",
            "Epoch 92: val_loss improved from 1.12653 to 1.10201, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.4694 - accuracy: 0.3858 - val_loss: 1.1020 - val_accuracy: 0.5922\n",
            "Epoch 93/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 1.3617 - accuracy: 0.4598\n",
            "Epoch 93: val_loss improved from 1.10201 to 1.02876, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 1.3747 - accuracy: 0.4427 - val_loss: 1.0288 - val_accuracy: 0.5319\n",
            "Epoch 94/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 1.3149 - accuracy: 0.4615\n",
            "Epoch 94: val_loss improved from 1.02876 to 1.01970, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.3240 - accuracy: 0.4604 - val_loss: 1.0197 - val_accuracy: 0.5780\n",
            "Epoch 95/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 1.2884 - accuracy: 0.4870\n",
            "Epoch 95: val_loss improved from 1.01970 to 0.93891, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.2709 - accuracy: 0.4924 - val_loss: 0.9389 - val_accuracy: 0.7057\n",
            "Epoch 96/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 1.2874 - accuracy: 0.4676\n",
            "Epoch 96: val_loss improved from 0.93891 to 0.91946, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 1.2853 - accuracy: 0.4676 - val_loss: 0.9195 - val_accuracy: 0.6135\n",
            "Epoch 97/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 1.2270 - accuracy: 0.5165\n",
            "Epoch 97: val_loss improved from 0.91946 to 0.86280, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.2276 - accuracy: 0.5156 - val_loss: 0.8628 - val_accuracy: 0.6844\n",
            "Epoch 98/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 1.1853 - accuracy: 0.5257\n",
            "Epoch 98: val_loss improved from 0.86280 to 0.82474, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.1876 - accuracy: 0.5236 - val_loss: 0.8247 - val_accuracy: 0.7376\n",
            "Epoch 99/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 1.1592 - accuracy: 0.5417\n",
            "Epoch 99: val_loss improved from 0.82474 to 0.80825, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 1.1616 - accuracy: 0.5422 - val_loss: 0.8082 - val_accuracy: 0.7872\n",
            "Epoch 100/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 1.0911 - accuracy: 0.5804\n",
            "Epoch 100: val_loss improved from 0.80825 to 0.77148, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 1.0881 - accuracy: 0.5813 - val_loss: 0.7715 - val_accuracy: 0.7908\n",
            "Epoch 101/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 1.0869 - accuracy: 0.5795\n",
            "Epoch 101: val_loss improved from 0.77148 to 0.74969, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 1.0873 - accuracy: 0.5804 - val_loss: 0.7497 - val_accuracy: 0.8227\n",
            "Epoch 102/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 1.1238 - accuracy: 0.5718\n",
            "Epoch 102: val_loss improved from 0.74969 to 0.70170, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 1.0786 - accuracy: 0.5849 - val_loss: 0.7017 - val_accuracy: 0.8404\n",
            "Epoch 103/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 1.0219 - accuracy: 0.6125\n",
            "Epoch 103: val_loss improved from 0.70170 to 0.62320, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 1.0258 - accuracy: 0.6133 - val_loss: 0.6232 - val_accuracy: 0.8582\n",
            "Epoch 104/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.9719 - accuracy: 0.6375\n",
            "Epoch 104: val_loss improved from 0.62320 to 0.56523, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.9363 - accuracy: 0.6418 - val_loss: 0.5652 - val_accuracy: 0.8688\n",
            "Epoch 105/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.8817 - accuracy: 0.6754\n",
            "Epoch 105: val_loss improved from 0.56523 to 0.53465, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.8820 - accuracy: 0.6729 - val_loss: 0.5347 - val_accuracy: 0.8759\n",
            "Epoch 106/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.9496 - accuracy: 0.6382\n",
            "Epoch 106: val_loss improved from 0.53465 to 0.51985, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.9266 - accuracy: 0.6604 - val_loss: 0.5198 - val_accuracy: 0.8830\n",
            "Epoch 107/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.8270 - accuracy: 0.6748\n",
            "Epoch 107: val_loss improved from 0.51985 to 0.45363, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.8157 - accuracy: 0.6773 - val_loss: 0.4536 - val_accuracy: 0.9007\n",
            "Epoch 108/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.7588 - accuracy: 0.7437\n",
            "Epoch 108: val_loss improved from 0.45363 to 0.44559, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.7911 - accuracy: 0.7289 - val_loss: 0.4456 - val_accuracy: 0.8794\n",
            "Epoch 109/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.7743 - accuracy: 0.7281\n",
            "Epoch 109: val_loss improved from 0.44559 to 0.41788, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.7831 - accuracy: 0.7244 - val_loss: 0.4179 - val_accuracy: 0.8901\n",
            "Epoch 110/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.7373 - accuracy: 0.7371\n",
            "Epoch 110: val_loss improved from 0.41788 to 0.40190, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.7306 - accuracy: 0.7396 - val_loss: 0.4019 - val_accuracy: 0.8794\n",
            "Epoch 111/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.7185 - accuracy: 0.7491\n",
            "Epoch 111: val_loss improved from 0.40190 to 0.36677, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.7214 - accuracy: 0.7493 - val_loss: 0.3668 - val_accuracy: 0.9078\n",
            "Epoch 112/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.7438 - accuracy: 0.7375\n",
            "Epoch 112: val_loss improved from 0.36677 to 0.34715, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.7438 - accuracy: 0.7378 - val_loss: 0.3471 - val_accuracy: 0.8936\n",
            "Epoch 113/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.6255 - accuracy: 0.7648\n",
            "Epoch 113: val_loss improved from 0.34715 to 0.30634, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.6154 - accuracy: 0.7751 - val_loss: 0.3063 - val_accuracy: 0.9043\n",
            "Epoch 114/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.7742\n",
            "Epoch 114: val_loss improved from 0.30634 to 0.29405, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.6418 - accuracy: 0.7742 - val_loss: 0.2941 - val_accuracy: 0.9113\n",
            "Epoch 115/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5974 - accuracy: 0.8018\n",
            "Epoch 115: val_loss improved from 0.29405 to 0.27302, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5974 - accuracy: 0.8018 - val_loss: 0.2730 - val_accuracy: 0.9255\n",
            "Epoch 116/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.5880 - accuracy: 0.7788\n",
            "Epoch 116: val_loss improved from 0.27302 to 0.27173, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.5924 - accuracy: 0.7813 - val_loss: 0.2717 - val_accuracy: 0.9220\n",
            "Epoch 117/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.5449 - accuracy: 0.8043\n",
            "Epoch 117: val_loss improved from 0.27173 to 0.27090, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5332 - accuracy: 0.8080 - val_loss: 0.2709 - val_accuracy: 0.9184\n",
            "Epoch 118/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.5360 - accuracy: 0.8219\n",
            "Epoch 118: val_loss improved from 0.27090 to 0.26942, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.5999 - accuracy: 0.8009 - val_loss: 0.2694 - val_accuracy: 0.9220\n",
            "Epoch 119/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.5186 - accuracy: 0.8257\n",
            "Epoch 119: val_loss did not improve from 0.26942\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.8053 - val_loss: 0.2736 - val_accuracy: 0.9397\n",
            "Epoch 120/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.5171 - accuracy: 0.8207\n",
            "Epoch 120: val_loss improved from 0.26942 to 0.23329, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.8196 - val_loss: 0.2333 - val_accuracy: 0.9326\n",
            "Epoch 121/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.5062 - accuracy: 0.8236\n",
            "Epoch 121: val_loss did not improve from 0.23329\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5202 - accuracy: 0.8187 - val_loss: 0.2741 - val_accuracy: 0.9184\n",
            "Epoch 122/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.8258\n",
            "Epoch 122: val_loss improved from 0.23329 to 0.22862, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.5134 - accuracy: 0.8258 - val_loss: 0.2286 - val_accuracy: 0.9362\n",
            "Epoch 123/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.5262 - accuracy: 0.8031\n",
            "Epoch 123: val_loss did not improve from 0.22862\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.4873 - accuracy: 0.8276 - val_loss: 0.2533 - val_accuracy: 0.9113\n",
            "Epoch 124/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.5201 - accuracy: 0.8073\n",
            "Epoch 124: val_loss improved from 0.22862 to 0.21934, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4791 - accuracy: 0.8249 - val_loss: 0.2193 - val_accuracy: 0.9291\n",
            "Epoch 125/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.4176 - accuracy: 0.8618\n",
            "Epoch 125: val_loss did not improve from 0.21934\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.8489 - val_loss: 0.2200 - val_accuracy: 0.9184\n",
            "Epoch 126/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.4204 - accuracy: 0.8421\n",
            "Epoch 126: val_loss did not improve from 0.21934\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.8409 - val_loss: 0.2298 - val_accuracy: 0.9362\n",
            "Epoch 127/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.4550 - accuracy: 0.8426\n",
            "Epoch 127: val_loss improved from 0.21934 to 0.21885, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.4548 - accuracy: 0.8436 - val_loss: 0.2189 - val_accuracy: 0.9220\n",
            "Epoch 128/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.4614 - accuracy: 0.8359\n",
            "Epoch 128: val_loss improved from 0.21885 to 0.18943, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4244 - accuracy: 0.8533 - val_loss: 0.1894 - val_accuracy: 0.9433\n",
            "Epoch 129/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.4588 - accuracy: 0.8507\n",
            "Epoch 129: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4437 - accuracy: 0.8569 - val_loss: 0.2205 - val_accuracy: 0.9220\n",
            "Epoch 130/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.4250 - accuracy: 0.8472\n",
            "Epoch 130: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8658 - val_loss: 0.1966 - val_accuracy: 0.9397\n",
            "Epoch 131/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.4354 - accuracy: 0.8651\n",
            "Epoch 131: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.4527 - accuracy: 0.8578 - val_loss: 0.2350 - val_accuracy: 0.9255\n",
            "Epoch 132/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.3900 - accuracy: 0.8759\n",
            "Epoch 132: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8720 - val_loss: 0.2110 - val_accuracy: 0.9255\n",
            "Epoch 133/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.3949 - accuracy: 0.8616\n",
            "Epoch 133: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.4007 - accuracy: 0.8596 - val_loss: 0.1989 - val_accuracy: 0.9362\n",
            "Epoch 134/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.3400 - accuracy: 0.8785\n",
            "Epoch 134: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3853 - accuracy: 0.8684 - val_loss: 0.2048 - val_accuracy: 0.9291\n",
            "Epoch 135/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.3687 - accuracy: 0.8682\n",
            "Epoch 135: val_loss did not improve from 0.18943\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3756 - accuracy: 0.8702 - val_loss: 0.2077 - val_accuracy: 0.9362\n",
            "Epoch 136/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.3830 - accuracy: 0.8715\n",
            "Epoch 136: val_loss improved from 0.18943 to 0.18558, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.8702 - val_loss: 0.1856 - val_accuracy: 0.9539\n",
            "Epoch 137/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.3673 - accuracy: 0.8676\n",
            "Epoch 137: val_loss improved from 0.18558 to 0.18021, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3700 - accuracy: 0.8667 - val_loss: 0.1802 - val_accuracy: 0.9468\n",
            "Epoch 138/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.3730 - accuracy: 0.8804\n",
            "Epoch 138: val_loss did not improve from 0.18021\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3816 - accuracy: 0.8773 - val_loss: 0.1989 - val_accuracy: 0.9397\n",
            "Epoch 139/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.3034 - accuracy: 0.8844\n",
            "Epoch 139: val_loss improved from 0.18021 to 0.15952, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3654 - accuracy: 0.8729 - val_loss: 0.1595 - val_accuracy: 0.9468\n",
            "Epoch 140/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.3427 - accuracy: 0.8797\n",
            "Epoch 140: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.8818 - val_loss: 0.2034 - val_accuracy: 0.9504\n",
            "Epoch 141/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.3487 - accuracy: 0.8881\n",
            "Epoch 141: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3612 - accuracy: 0.8827 - val_loss: 0.2038 - val_accuracy: 0.9433\n",
            "Epoch 142/1000\n",
            "20/36 [===============>..............] - ETA: 0s - loss: 0.3639 - accuracy: 0.8813\n",
            "Epoch 142: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.8756 - val_loss: 0.1767 - val_accuracy: 0.9468\n",
            "Epoch 143/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.3374 - accuracy: 0.8893\n",
            "Epoch 143: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3377 - accuracy: 0.8889 - val_loss: 0.1626 - val_accuracy: 0.9539\n",
            "Epoch 144/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.3694 - accuracy: 0.8741\n",
            "Epoch 144: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3603 - accuracy: 0.8747 - val_loss: 0.1942 - val_accuracy: 0.9433\n",
            "Epoch 145/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.3156 - accuracy: 0.8717\n",
            "Epoch 145: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3452 - accuracy: 0.8676 - val_loss: 0.1700 - val_accuracy: 0.9504\n",
            "Epoch 146/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.3040 - accuracy: 0.8958\n",
            "Epoch 146: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3044 - accuracy: 0.8933 - val_loss: 0.1723 - val_accuracy: 0.9362\n",
            "Epoch 147/1000\n",
            "17/36 [=============>................] - ETA: 0s - loss: 0.3575 - accuracy: 0.8842\n",
            "Epoch 147: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3472 - accuracy: 0.8818 - val_loss: 0.1959 - val_accuracy: 0.9326\n",
            "Epoch 148/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.3741 - accuracy: 0.8693\n",
            "Epoch 148: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3714 - accuracy: 0.8684 - val_loss: 0.2022 - val_accuracy: 0.9433\n",
            "Epoch 149/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.3202 - accuracy: 0.8781\n",
            "Epoch 149: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.3276 - accuracy: 0.8773 - val_loss: 0.2030 - val_accuracy: 0.9397\n",
            "Epoch 150/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8871\n",
            "Epoch 150: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.8871 - val_loss: 0.1916 - val_accuracy: 0.9397\n",
            "Epoch 151/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8871\n",
            "Epoch 151: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8871 - val_loss: 0.1715 - val_accuracy: 0.9468\n",
            "Epoch 152/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.3070 - accuracy: 0.8958\n",
            "Epoch 152: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3035 - accuracy: 0.8924 - val_loss: 0.1777 - val_accuracy: 0.9433\n",
            "Epoch 153/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.2632 - accuracy: 0.9115\n",
            "Epoch 153: val_loss did not improve from 0.15952\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8889 - val_loss: 0.1631 - val_accuracy: 0.9468\n",
            "Epoch 154/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.2721 - accuracy: 0.9062\n",
            "Epoch 154: val_loss improved from 0.15952 to 0.15323, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2922 - accuracy: 0.9031 - val_loss: 0.1532 - val_accuracy: 0.9539\n",
            "Epoch 155/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.3020 - accuracy: 0.8924\n",
            "Epoch 155: val_loss did not improve from 0.15323\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.8880 - val_loss: 0.1935 - val_accuracy: 0.9326\n",
            "Epoch 156/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2872 - accuracy: 0.8964\n",
            "Epoch 156: val_loss did not improve from 0.15323\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2868 - accuracy: 0.8969 - val_loss: 0.1752 - val_accuracy: 0.9468\n",
            "Epoch 157/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.3155 - accuracy: 0.8920\n",
            "Epoch 157: val_loss did not improve from 0.15323\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.8916 - val_loss: 0.1731 - val_accuracy: 0.9504\n",
            "Epoch 158/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2857 - accuracy: 0.9000\n",
            "Epoch 158: val_loss did not improve from 0.15323\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2831 - accuracy: 0.9013 - val_loss: 0.1732 - val_accuracy: 0.9255\n",
            "Epoch 159/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9009\n",
            "Epoch 159: val_loss improved from 0.15323 to 0.13342, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2843 - accuracy: 0.9004 - val_loss: 0.1334 - val_accuracy: 0.9574\n",
            "Epoch 160/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.8898\n",
            "Epoch 160: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.3015 - accuracy: 0.8898 - val_loss: 0.1756 - val_accuracy: 0.9433\n",
            "Epoch 161/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.3236 - accuracy: 0.8750\n",
            "Epoch 161: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.3173 - accuracy: 0.8853 - val_loss: 0.1514 - val_accuracy: 0.9539\n",
            "Epoch 162/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.3374 - accuracy: 0.8936\n",
            "Epoch 162: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.3250 - accuracy: 0.8987 - val_loss: 0.1518 - val_accuracy: 0.9610\n",
            "Epoch 163/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2897 - accuracy: 0.8984\n",
            "Epoch 163: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.3019 - accuracy: 0.8951 - val_loss: 0.1515 - val_accuracy: 0.9468\n",
            "Epoch 164/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.3055 - accuracy: 0.8833\n",
            "Epoch 164: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.3017 - accuracy: 0.8853 - val_loss: 0.1654 - val_accuracy: 0.9539\n",
            "Epoch 165/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.8946\n",
            "Epoch 165: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2833 - accuracy: 0.8951 - val_loss: 0.1427 - val_accuracy: 0.9574\n",
            "Epoch 166/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2672 - accuracy: 0.9044\n",
            "Epoch 166: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2661 - accuracy: 0.9031 - val_loss: 0.1591 - val_accuracy: 0.9468\n",
            "Epoch 167/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.3348 - accuracy: 0.8901\n",
            "Epoch 167: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.3270 - accuracy: 0.8933 - val_loss: 0.2092 - val_accuracy: 0.9326\n",
            "Epoch 168/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2717 - accuracy: 0.9083\n",
            "Epoch 168: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2789 - accuracy: 0.9058 - val_loss: 0.1692 - val_accuracy: 0.9397\n",
            "Epoch 169/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.2606 - accuracy: 0.9115\n",
            "Epoch 169: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2716 - accuracy: 0.9022 - val_loss: 0.1645 - val_accuracy: 0.9433\n",
            "Epoch 170/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.9071\n",
            "Epoch 170: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.2679 - accuracy: 0.9076 - val_loss: 0.1544 - val_accuracy: 0.9433\n",
            "Epoch 171/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2528 - accuracy: 0.9009\n",
            "Epoch 171: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2532 - accuracy: 0.9004 - val_loss: 0.1568 - val_accuracy: 0.9504\n",
            "Epoch 172/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.8991\n",
            "Epoch 172: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2855 - accuracy: 0.8987 - val_loss: 0.1590 - val_accuracy: 0.9504\n",
            "Epoch 173/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2319 - accuracy: 0.9173\n",
            "Epoch 173: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2426 - accuracy: 0.9129 - val_loss: 0.1585 - val_accuracy: 0.9397\n",
            "Epoch 174/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.2682 - accuracy: 0.8945\n",
            "Epoch 174: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2775 - accuracy: 0.8969 - val_loss: 0.2092 - val_accuracy: 0.9326\n",
            "Epoch 175/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8924\n",
            "Epoch 175: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3084 - accuracy: 0.8924 - val_loss: 0.1675 - val_accuracy: 0.9397\n",
            "Epoch 176/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.2431 - accuracy: 0.9115\n",
            "Epoch 176: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.9031 - val_loss: 0.1422 - val_accuracy: 0.9468\n",
            "Epoch 177/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8924\n",
            "Epoch 177: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2779 - accuracy: 0.8924 - val_loss: 0.1472 - val_accuracy: 0.9397\n",
            "Epoch 178/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9040\n",
            "Epoch 178: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2633 - accuracy: 0.9040 - val_loss: 0.1468 - val_accuracy: 0.9468\n",
            "Epoch 179/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2841 - accuracy: 0.9016\n",
            "Epoch 179: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2949 - accuracy: 0.8969 - val_loss: 0.1849 - val_accuracy: 0.9397\n",
            "Epoch 180/1000\n",
            "22/36 [=================>............] - ETA: 0s - loss: 0.2624 - accuracy: 0.9077\n",
            "Epoch 180: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2523 - accuracy: 0.9129 - val_loss: 0.1553 - val_accuracy: 0.9433\n",
            "Epoch 181/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.3096 - accuracy: 0.8809\n",
            "Epoch 181: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.3038 - accuracy: 0.8862 - val_loss: 0.1733 - val_accuracy: 0.9468\n",
            "Epoch 182/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8836\n",
            "Epoch 182: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8836 - val_loss: 0.1801 - val_accuracy: 0.9326\n",
            "Epoch 183/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2784 - accuracy: 0.8961\n",
            "Epoch 183: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2738 - accuracy: 0.8969 - val_loss: 0.1469 - val_accuracy: 0.9504\n",
            "Epoch 184/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.2728 - accuracy: 0.8931\n",
            "Epoch 184: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2722 - accuracy: 0.8978 - val_loss: 0.1729 - val_accuracy: 0.9468\n",
            "Epoch 185/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.2973 - accuracy: 0.8945\n",
            "Epoch 185: val_loss did not improve from 0.13342\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2894 - accuracy: 0.9031 - val_loss: 0.1710 - val_accuracy: 0.9291\n",
            "Epoch 186/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.2891 - accuracy: 0.8854\n",
            "Epoch 186: val_loss improved from 0.13342 to 0.11542, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2855 - accuracy: 0.8907 - val_loss: 0.1154 - val_accuracy: 0.9610\n",
            "Epoch 187/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2557 - accuracy: 0.9145\n",
            "Epoch 187: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2517 - accuracy: 0.9164 - val_loss: 0.1470 - val_accuracy: 0.9504\n",
            "Epoch 188/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.9116\n",
            "Epoch 188: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2449 - accuracy: 0.9120 - val_loss: 0.1518 - val_accuracy: 0.9397\n",
            "Epoch 189/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2922 - accuracy: 0.8971\n",
            "Epoch 189: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2865 - accuracy: 0.8996 - val_loss: 0.1344 - val_accuracy: 0.9468\n",
            "Epoch 190/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2357 - accuracy: 0.9150\n",
            "Epoch 190: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2336 - accuracy: 0.9164 - val_loss: 0.1652 - val_accuracy: 0.9326\n",
            "Epoch 191/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.2665 - accuracy: 0.9023\n",
            "Epoch 191: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2449 - accuracy: 0.9084 - val_loss: 0.1442 - val_accuracy: 0.9433\n",
            "Epoch 192/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2541 - accuracy: 0.9089\n",
            "Epoch 192: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2529 - accuracy: 0.9093 - val_loss: 0.1757 - val_accuracy: 0.9468\n",
            "Epoch 193/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.2613 - accuracy: 0.8964\n",
            "Epoch 193: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2535 - accuracy: 0.9013 - val_loss: 0.1516 - val_accuracy: 0.9468\n",
            "Epoch 194/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2571 - accuracy: 0.9062\n",
            "Epoch 194: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2419 - accuracy: 0.9093 - val_loss: 0.1675 - val_accuracy: 0.9504\n",
            "Epoch 195/1000\n",
            "19/36 [==============>...............] - ETA: 0s - loss: 0.2265 - accuracy: 0.9260\n",
            "Epoch 195: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2192 - accuracy: 0.9253 - val_loss: 0.1471 - val_accuracy: 0.9504\n",
            "Epoch 196/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2503 - accuracy: 0.9099\n",
            "Epoch 196: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2580 - accuracy: 0.9084 - val_loss: 0.1543 - val_accuracy: 0.9397\n",
            "Epoch 197/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9089\n",
            "Epoch 197: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9093 - val_loss: 0.1266 - val_accuracy: 0.9681\n",
            "Epoch 198/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9218\n",
            "Epoch 198: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9218 - val_loss: 0.1431 - val_accuracy: 0.9574\n",
            "Epoch 199/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1963 - accuracy: 0.9234\n",
            "Epoch 199: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9253 - val_loss: 0.1330 - val_accuracy: 0.9574\n",
            "Epoch 200/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2409 - accuracy: 0.9035\n",
            "Epoch 200: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2623 - accuracy: 0.8996 - val_loss: 0.1416 - val_accuracy: 0.9539\n",
            "Epoch 201/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2843 - accuracy: 0.9163\n",
            "Epoch 201: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.9120 - val_loss: 0.1726 - val_accuracy: 0.9468\n",
            "Epoch 202/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2650 - accuracy: 0.9097\n",
            "Epoch 202: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2674 - accuracy: 0.9129 - val_loss: 0.1681 - val_accuracy: 0.9574\n",
            "Epoch 203/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9018\n",
            "Epoch 203: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2645 - accuracy: 0.9022 - val_loss: 0.1404 - val_accuracy: 0.9504\n",
            "Epoch 204/1000\n",
            "18/36 [==============>...............] - ETA: 0s - loss: 0.2572 - accuracy: 0.9010\n",
            "Epoch 204: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2399 - accuracy: 0.9049 - val_loss: 0.1436 - val_accuracy: 0.9574\n",
            "Epoch 205/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2514 - accuracy: 0.9120\n",
            "Epoch 205: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2514 - accuracy: 0.9120 - val_loss: 0.1540 - val_accuracy: 0.9468\n",
            "Epoch 206/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2702 - accuracy: 0.8964\n",
            "Epoch 206: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.8942 - val_loss: 0.1529 - val_accuracy: 0.9574\n",
            "Epoch 207/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2910 - accuracy: 0.9087\n",
            "Epoch 207: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2990 - accuracy: 0.9040 - val_loss: 0.1566 - val_accuracy: 0.9574\n",
            "Epoch 208/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2635 - accuracy: 0.8945\n",
            "Epoch 208: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2589 - accuracy: 0.8969 - val_loss: 0.1642 - val_accuracy: 0.9539\n",
            "Epoch 209/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2142 - accuracy: 0.9228\n",
            "Epoch 209: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2137 - accuracy: 0.9236 - val_loss: 0.1425 - val_accuracy: 0.9539\n",
            "Epoch 210/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2540 - accuracy: 0.9053\n",
            "Epoch 210: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2506 - accuracy: 0.9067 - val_loss: 0.1451 - val_accuracy: 0.9574\n",
            "Epoch 211/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.2295 - accuracy: 0.9148\n",
            "Epoch 211: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.9182 - val_loss: 0.1415 - val_accuracy: 0.9574\n",
            "Epoch 212/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.2371 - accuracy: 0.9252\n",
            "Epoch 212: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.9280 - val_loss: 0.1552 - val_accuracy: 0.9504\n",
            "Epoch 213/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2189 - accuracy: 0.9183\n",
            "Epoch 213: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.9209 - val_loss: 0.1297 - val_accuracy: 0.9504\n",
            "Epoch 214/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9111\n",
            "Epoch 214: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2308 - accuracy: 0.9111 - val_loss: 0.1354 - val_accuracy: 0.9574\n",
            "Epoch 215/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2301 - accuracy: 0.9182\n",
            "Epoch 215: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2273 - accuracy: 0.9191 - val_loss: 0.1638 - val_accuracy: 0.9433\n",
            "Epoch 216/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.2584 - accuracy: 0.9110\n",
            "Epoch 216: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2550 - accuracy: 0.9120 - val_loss: 0.1531 - val_accuracy: 0.9433\n",
            "Epoch 217/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2066 - accuracy: 0.9153\n",
            "Epoch 217: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2133 - accuracy: 0.9156 - val_loss: 0.1622 - val_accuracy: 0.9362\n",
            "Epoch 218/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.2684 - accuracy: 0.9008\n",
            "Epoch 218: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2658 - accuracy: 0.8987 - val_loss: 0.1604 - val_accuracy: 0.9397\n",
            "Epoch 219/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2245 - accuracy: 0.9198\n",
            "Epoch 219: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2201 - accuracy: 0.9227 - val_loss: 0.1791 - val_accuracy: 0.9397\n",
            "Epoch 220/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9040\n",
            "Epoch 220: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2709 - accuracy: 0.9040 - val_loss: 0.1706 - val_accuracy: 0.9397\n",
            "Epoch 221/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9080\n",
            "Epoch 221: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2403 - accuracy: 0.9084 - val_loss: 0.1232 - val_accuracy: 0.9610\n",
            "Epoch 222/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.9191\n",
            "Epoch 222: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1989 - accuracy: 0.9191 - val_loss: 0.1168 - val_accuracy: 0.9645\n",
            "Epoch 223/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.2074 - accuracy: 0.9212\n",
            "Epoch 223: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2047 - accuracy: 0.9236 - val_loss: 0.1732 - val_accuracy: 0.9433\n",
            "Epoch 224/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.2127 - accuracy: 0.9170\n",
            "Epoch 224: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2108 - accuracy: 0.9156 - val_loss: 0.1492 - val_accuracy: 0.9504\n",
            "Epoch 225/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.2297 - accuracy: 0.9091\n",
            "Epoch 225: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2292 - accuracy: 0.9093 - val_loss: 0.1159 - val_accuracy: 0.9610\n",
            "Epoch 226/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1879 - accuracy: 0.9304\n",
            "Epoch 226: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9289 - val_loss: 0.1264 - val_accuracy: 0.9645\n",
            "Epoch 227/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9116\n",
            "Epoch 227: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.9120 - val_loss: 0.1636 - val_accuracy: 0.9362\n",
            "Epoch 228/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2451 - accuracy: 0.9244\n",
            "Epoch 228: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2379 - accuracy: 0.9244 - val_loss: 0.1350 - val_accuracy: 0.9574\n",
            "Epoch 229/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2377 - accuracy: 0.9210\n",
            "Epoch 229: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2421 - accuracy: 0.9200 - val_loss: 0.1352 - val_accuracy: 0.9539\n",
            "Epoch 230/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9214\n",
            "Epoch 230: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2210 - accuracy: 0.9209 - val_loss: 0.1378 - val_accuracy: 0.9504\n",
            "Epoch 231/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2085 - accuracy: 0.9289\n",
            "Epoch 231: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2085 - accuracy: 0.9289 - val_loss: 0.1329 - val_accuracy: 0.9539\n",
            "Epoch 232/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9253\n",
            "Epoch 232: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2167 - accuracy: 0.9253 - val_loss: 0.1459 - val_accuracy: 0.9433\n",
            "Epoch 233/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9250\n",
            "Epoch 233: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1882 - accuracy: 0.9253 - val_loss: 0.1503 - val_accuracy: 0.9504\n",
            "Epoch 234/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.2307 - accuracy: 0.9062\n",
            "Epoch 234: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2230 - accuracy: 0.9084 - val_loss: 0.1373 - val_accuracy: 0.9716\n",
            "Epoch 235/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1949 - accuracy: 0.9325\n",
            "Epoch 235: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2064 - accuracy: 0.9280 - val_loss: 0.1222 - val_accuracy: 0.9610\n",
            "Epoch 236/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.2035 - accuracy: 0.9203\n",
            "Epoch 236: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2224 - accuracy: 0.9147 - val_loss: 0.1212 - val_accuracy: 0.9645\n",
            "Epoch 237/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2115 - accuracy: 0.9195\n",
            "Epoch 237: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2079 - accuracy: 0.9182 - val_loss: 0.1382 - val_accuracy: 0.9574\n",
            "Epoch 238/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1801 - accuracy: 0.9362\n",
            "Epoch 238: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1897 - accuracy: 0.9342 - val_loss: 0.1487 - val_accuracy: 0.9574\n",
            "Epoch 239/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2157 - accuracy: 0.9191\n",
            "Epoch 239: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2133 - accuracy: 0.9209 - val_loss: 0.1365 - val_accuracy: 0.9574\n",
            "Epoch 240/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2126 - accuracy: 0.9231\n",
            "Epoch 240: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.2129 - accuracy: 0.9218 - val_loss: 0.1202 - val_accuracy: 0.9645\n",
            "Epoch 241/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2211 - accuracy: 0.9238\n",
            "Epoch 241: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2285 - accuracy: 0.9182 - val_loss: 0.1297 - val_accuracy: 0.9504\n",
            "Epoch 242/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2358 - accuracy: 0.9118\n",
            "Epoch 242: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2325 - accuracy: 0.9120 - val_loss: 0.1424 - val_accuracy: 0.9468\n",
            "Epoch 243/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2468 - accuracy: 0.9194\n",
            "Epoch 243: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.2421 - accuracy: 0.9182 - val_loss: 0.1532 - val_accuracy: 0.9539\n",
            "Epoch 244/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.2338 - accuracy: 0.9085\n",
            "Epoch 244: val_loss did not improve from 0.11542\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2124 - accuracy: 0.9156 - val_loss: 0.1514 - val_accuracy: 0.9433\n",
            "Epoch 245/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.2204 - accuracy: 0.9185\n",
            "Epoch 245: val_loss improved from 0.11542 to 0.11059, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.2173 - accuracy: 0.9191 - val_loss: 0.1106 - val_accuracy: 0.9681\n",
            "Epoch 246/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1995 - accuracy: 0.9229\n",
            "Epoch 246: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2054 - accuracy: 0.9244 - val_loss: 0.1479 - val_accuracy: 0.9504\n",
            "Epoch 247/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1996 - accuracy: 0.9290\n",
            "Epoch 247: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9280 - val_loss: 0.1414 - val_accuracy: 0.9468\n",
            "Epoch 248/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1964 - accuracy: 0.9290\n",
            "Epoch 248: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1984 - accuracy: 0.9280 - val_loss: 0.1536 - val_accuracy: 0.9504\n",
            "Epoch 249/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2379 - accuracy: 0.9182\n",
            "Epoch 249: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2333 - accuracy: 0.9200 - val_loss: 0.1309 - val_accuracy: 0.9574\n",
            "Epoch 250/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2208 - accuracy: 0.9125\n",
            "Epoch 250: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2190 - accuracy: 0.9138 - val_loss: 0.1624 - val_accuracy: 0.9433\n",
            "Epoch 251/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2077 - accuracy: 0.9244\n",
            "Epoch 251: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1986 - accuracy: 0.9271 - val_loss: 0.1271 - val_accuracy: 0.9610\n",
            "Epoch 252/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2591 - accuracy: 0.9194\n",
            "Epoch 252: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2477 - accuracy: 0.9209 - val_loss: 0.1642 - val_accuracy: 0.9468\n",
            "Epoch 253/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2092 - accuracy: 0.9113\n",
            "Epoch 253: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9147 - val_loss: 0.1413 - val_accuracy: 0.9610\n",
            "Epoch 254/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2128 - accuracy: 0.9208\n",
            "Epoch 254: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2143 - accuracy: 0.9200 - val_loss: 0.1424 - val_accuracy: 0.9645\n",
            "Epoch 255/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.2031 - accuracy: 0.9203\n",
            "Epoch 255: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2052 - accuracy: 0.9200 - val_loss: 0.1125 - val_accuracy: 0.9716\n",
            "Epoch 256/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1917 - accuracy: 0.9307\n",
            "Epoch 256: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9262 - val_loss: 0.1113 - val_accuracy: 0.9645\n",
            "Epoch 257/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9143\n",
            "Epoch 257: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2152 - accuracy: 0.9138 - val_loss: 0.1258 - val_accuracy: 0.9574\n",
            "Epoch 258/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9107\n",
            "Epoch 258: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2427 - accuracy: 0.9111 - val_loss: 0.1748 - val_accuracy: 0.9539\n",
            "Epoch 259/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1915 - accuracy: 0.9248\n",
            "Epoch 259: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1896 - accuracy: 0.9253 - val_loss: 0.1473 - val_accuracy: 0.9539\n",
            "Epoch 260/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1976 - accuracy: 0.9205\n",
            "Epoch 260: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1943 - accuracy: 0.9218 - val_loss: 0.1421 - val_accuracy: 0.9574\n",
            "Epoch 261/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1966 - accuracy: 0.9213\n",
            "Epoch 261: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2207 - accuracy: 0.9182 - val_loss: 0.1525 - val_accuracy: 0.9539\n",
            "Epoch 262/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1902 - accuracy: 0.9350\n",
            "Epoch 262: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.9289 - val_loss: 0.1304 - val_accuracy: 0.9610\n",
            "Epoch 263/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1677 - accuracy: 0.9411\n",
            "Epoch 263: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1863 - accuracy: 0.9360 - val_loss: 0.1223 - val_accuracy: 0.9681\n",
            "Epoch 264/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1689 - accuracy: 0.9375\n",
            "Epoch 264: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.9324 - val_loss: 0.1493 - val_accuracy: 0.9539\n",
            "Epoch 265/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1905 - accuracy: 0.9256\n",
            "Epoch 265: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1934 - accuracy: 0.9236 - val_loss: 0.1479 - val_accuracy: 0.9539\n",
            "Epoch 266/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2280 - accuracy: 0.9277\n",
            "Epoch 266: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2202 - accuracy: 0.9289 - val_loss: 0.1386 - val_accuracy: 0.9504\n",
            "Epoch 267/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1895 - accuracy: 0.9297\n",
            "Epoch 267: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1921 - accuracy: 0.9298 - val_loss: 0.1260 - val_accuracy: 0.9539\n",
            "Epoch 268/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.2102 - accuracy: 0.9280\n",
            "Epoch 268: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1985 - accuracy: 0.9298 - val_loss: 0.1115 - val_accuracy: 0.9574\n",
            "Epoch 269/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2124 - accuracy: 0.9219\n",
            "Epoch 269: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2202 - accuracy: 0.9200 - val_loss: 0.1146 - val_accuracy: 0.9645\n",
            "Epoch 270/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2027 - accuracy: 0.9274\n",
            "Epoch 270: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2048 - accuracy: 0.9280 - val_loss: 0.1336 - val_accuracy: 0.9610\n",
            "Epoch 271/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.9393\n",
            "Epoch 271: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.1513 - accuracy: 0.9396 - val_loss: 0.1363 - val_accuracy: 0.9433\n",
            "Epoch 272/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2269 - accuracy: 0.9178\n",
            "Epoch 272: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 0.9200 - val_loss: 0.1290 - val_accuracy: 0.9574\n",
            "Epoch 273/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2011 - accuracy: 0.9291\n",
            "Epoch 273: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1914 - accuracy: 0.9316 - val_loss: 0.1721 - val_accuracy: 0.9504\n",
            "Epoch 274/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.2286 - accuracy: 0.9261\n",
            "Epoch 274: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2266 - accuracy: 0.9253 - val_loss: 0.1578 - val_accuracy: 0.9504\n",
            "Epoch 275/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.2364 - accuracy: 0.9228\n",
            "Epoch 275: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 0.2344 - accuracy: 0.9227 - val_loss: 0.1209 - val_accuracy: 0.9539\n",
            "Epoch 276/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2630 - accuracy: 0.9042\n",
            "Epoch 276: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2393 - accuracy: 0.9138 - val_loss: 0.1156 - val_accuracy: 0.9645\n",
            "Epoch 277/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9281\n",
            "Epoch 277: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1916 - accuracy: 0.9253 - val_loss: 0.1153 - val_accuracy: 0.9574\n",
            "Epoch 278/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1839 - accuracy: 0.9344\n",
            "Epoch 278: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1767 - accuracy: 0.9387 - val_loss: 0.1369 - val_accuracy: 0.9610\n",
            "Epoch 279/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2025 - accuracy: 0.9365\n",
            "Epoch 279: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2163 - accuracy: 0.9342 - val_loss: 0.1240 - val_accuracy: 0.9610\n",
            "Epoch 280/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2155 - accuracy: 0.9111\n",
            "Epoch 280: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2116 - accuracy: 0.9138 - val_loss: 0.1285 - val_accuracy: 0.9504\n",
            "Epoch 281/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1847 - accuracy: 0.9248\n",
            "Epoch 281: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1808 - accuracy: 0.9280 - val_loss: 0.1234 - val_accuracy: 0.9610\n",
            "Epoch 282/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9375\n",
            "Epoch 282: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9378 - val_loss: 0.1225 - val_accuracy: 0.9610\n",
            "Epoch 283/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.2325 - accuracy: 0.9241\n",
            "Epoch 283: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2287 - accuracy: 0.9218 - val_loss: 0.1315 - val_accuracy: 0.9645\n",
            "Epoch 284/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 0.2318 - accuracy: 0.9211\n",
            "Epoch 284: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1971 - accuracy: 0.9307 - val_loss: 0.1384 - val_accuracy: 0.9504\n",
            "Epoch 285/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2075 - accuracy: 0.9271\n",
            "Epoch 285: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2261 - accuracy: 0.9209 - val_loss: 0.1458 - val_accuracy: 0.9397\n",
            "Epoch 286/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9312\n",
            "Epoch 286: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1951 - accuracy: 0.9307 - val_loss: 0.1372 - val_accuracy: 0.9574\n",
            "Epoch 287/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.9375\n",
            "Epoch 287: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1663 - accuracy: 0.9333 - val_loss: 0.1443 - val_accuracy: 0.9468\n",
            "Epoch 288/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9253\n",
            "Epoch 288: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2010 - accuracy: 0.9253 - val_loss: 0.1404 - val_accuracy: 0.9504\n",
            "Epoch 289/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9049\n",
            "Epoch 289: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2048 - accuracy: 0.9049 - val_loss: 0.1174 - val_accuracy: 0.9645\n",
            "Epoch 290/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1643 - accuracy: 0.9375\n",
            "Epoch 290: val_loss did not improve from 0.11059\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1837 - accuracy: 0.9333 - val_loss: 0.1372 - val_accuracy: 0.9610\n",
            "Epoch 291/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9250\n",
            "Epoch 291: val_loss improved from 0.11059 to 0.09046, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1826 - accuracy: 0.9253 - val_loss: 0.0905 - val_accuracy: 0.9645\n",
            "Epoch 292/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1820 - accuracy: 0.9311\n",
            "Epoch 292: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1795 - accuracy: 0.9316 - val_loss: 0.1134 - val_accuracy: 0.9610\n",
            "Epoch 293/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1825 - accuracy: 0.9337\n",
            "Epoch 293: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2055 - accuracy: 0.9298 - val_loss: 0.1171 - val_accuracy: 0.9574\n",
            "Epoch 294/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2264 - accuracy: 0.9209\n",
            "Epoch 294: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2251 - accuracy: 0.9191 - val_loss: 0.1416 - val_accuracy: 0.9539\n",
            "Epoch 295/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2076 - accuracy: 0.9292\n",
            "Epoch 295: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.2034 - accuracy: 0.9289 - val_loss: 0.1267 - val_accuracy: 0.9574\n",
            "Epoch 296/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9307\n",
            "Epoch 296: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1851 - accuracy: 0.9307 - val_loss: 0.1602 - val_accuracy: 0.9397\n",
            "Epoch 297/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9365\n",
            "Epoch 297: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1703 - accuracy: 0.9378 - val_loss: 0.1310 - val_accuracy: 0.9433\n",
            "Epoch 298/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1942 - accuracy: 0.9385\n",
            "Epoch 298: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1822 - accuracy: 0.9413 - val_loss: 0.1554 - val_accuracy: 0.9468\n",
            "Epoch 299/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1733 - accuracy: 0.9330\n",
            "Epoch 299: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1819 - accuracy: 0.9316 - val_loss: 0.1425 - val_accuracy: 0.9716\n",
            "Epoch 300/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1746 - accuracy: 0.9323\n",
            "Epoch 300: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1722 - accuracy: 0.9316 - val_loss: 0.1028 - val_accuracy: 0.9681\n",
            "Epoch 301/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2296 - accuracy: 0.9271\n",
            "Epoch 301: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.2264 - accuracy: 0.9262 - val_loss: 0.1250 - val_accuracy: 0.9645\n",
            "Epoch 302/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1736 - accuracy: 0.9304\n",
            "Epoch 302: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1674 - accuracy: 0.9324 - val_loss: 0.1222 - val_accuracy: 0.9681\n",
            "Epoch 303/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9385\n",
            "Epoch 303: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1750 - accuracy: 0.9413 - val_loss: 0.0991 - val_accuracy: 0.9645\n",
            "Epoch 304/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1482 - accuracy: 0.9401\n",
            "Epoch 304: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1449 - accuracy: 0.9431 - val_loss: 0.1290 - val_accuracy: 0.9539\n",
            "Epoch 305/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9268\n",
            "Epoch 305: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1837 - accuracy: 0.9271 - val_loss: 0.1130 - val_accuracy: 0.9645\n",
            "Epoch 306/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1894 - accuracy: 0.9432\n",
            "Epoch 306: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1905 - accuracy: 0.9431 - val_loss: 0.1297 - val_accuracy: 0.9681\n",
            "Epoch 307/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1585 - accuracy: 0.9375\n",
            "Epoch 307: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1635 - accuracy: 0.9351 - val_loss: 0.1452 - val_accuracy: 0.9645\n",
            "Epoch 308/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1178 - accuracy: 0.9577\n",
            "Epoch 308: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1238 - accuracy: 0.9556 - val_loss: 0.1651 - val_accuracy: 0.9574\n",
            "Epoch 309/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1397 - accuracy: 0.9444\n",
            "Epoch 309: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1419 - accuracy: 0.9431 - val_loss: 0.1614 - val_accuracy: 0.9574\n",
            "Epoch 310/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1527 - accuracy: 0.9441\n",
            "Epoch 310: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1505 - accuracy: 0.9449 - val_loss: 0.1048 - val_accuracy: 0.9716\n",
            "Epoch 311/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9289\n",
            "Epoch 311: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9289 - val_loss: 0.1877 - val_accuracy: 0.9468\n",
            "Epoch 312/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1745 - accuracy: 0.9350\n",
            "Epoch 312: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1924 - accuracy: 0.9271 - val_loss: 0.1359 - val_accuracy: 0.9645\n",
            "Epoch 313/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1873 - accuracy: 0.9353\n",
            "Epoch 313: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1886 - accuracy: 0.9342 - val_loss: 0.1375 - val_accuracy: 0.9681\n",
            "Epoch 314/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1822 - accuracy: 0.9207\n",
            "Epoch 314: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1741 - accuracy: 0.9253 - val_loss: 0.1359 - val_accuracy: 0.9610\n",
            "Epoch 315/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1637 - accuracy: 0.9366\n",
            "Epoch 315: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1626 - accuracy: 0.9378 - val_loss: 0.1407 - val_accuracy: 0.9681\n",
            "Epoch 316/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9312\n",
            "Epoch 316: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1838 - accuracy: 0.9316 - val_loss: 0.1151 - val_accuracy: 0.9574\n",
            "Epoch 317/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.9355\n",
            "Epoch 317: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1656 - accuracy: 0.9387 - val_loss: 0.1041 - val_accuracy: 0.9681\n",
            "Epoch 318/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9345\n",
            "Epoch 318: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1759 - accuracy: 0.9360 - val_loss: 0.1313 - val_accuracy: 0.9574\n",
            "Epoch 319/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1561 - accuracy: 0.9403\n",
            "Epoch 319: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1575 - accuracy: 0.9404 - val_loss: 0.1191 - val_accuracy: 0.9610\n",
            "Epoch 320/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9375\n",
            "Epoch 320: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9378 - val_loss: 0.1197 - val_accuracy: 0.9610\n",
            "Epoch 321/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2085 - accuracy: 0.9167\n",
            "Epoch 321: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1877 - accuracy: 0.9244 - val_loss: 0.1315 - val_accuracy: 0.9574\n",
            "Epoch 322/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2235 - accuracy: 0.9315\n",
            "Epoch 322: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2195 - accuracy: 0.9316 - val_loss: 0.1298 - val_accuracy: 0.9468\n",
            "Epoch 323/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1771 - accuracy: 0.9398\n",
            "Epoch 323: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1784 - accuracy: 0.9387 - val_loss: 0.1173 - val_accuracy: 0.9539\n",
            "Epoch 324/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1517 - accuracy: 0.9365\n",
            "Epoch 324: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9351 - val_loss: 0.1219 - val_accuracy: 0.9681\n",
            "Epoch 325/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1556 - accuracy: 0.9375\n",
            "Epoch 325: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1540 - accuracy: 0.9378 - val_loss: 0.1511 - val_accuracy: 0.9574\n",
            "Epoch 326/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1644 - accuracy: 0.9417\n",
            "Epoch 326: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9422 - val_loss: 0.1411 - val_accuracy: 0.9610\n",
            "Epoch 327/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1836 - accuracy: 0.9393\n",
            "Epoch 327: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1782 - accuracy: 0.9413 - val_loss: 0.1021 - val_accuracy: 0.9787\n",
            "Epoch 328/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1281 - accuracy: 0.9463\n",
            "Epoch 328: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1566 - accuracy: 0.9369 - val_loss: 0.1495 - val_accuracy: 0.9610\n",
            "Epoch 329/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1635 - accuracy: 0.9385\n",
            "Epoch 329: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1735 - accuracy: 0.9378 - val_loss: 0.1085 - val_accuracy: 0.9645\n",
            "Epoch 330/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1545 - accuracy: 0.9458\n",
            "Epoch 330: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9449 - val_loss: 0.1365 - val_accuracy: 0.9610\n",
            "Epoch 331/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1831 - accuracy: 0.9264\n",
            "Epoch 331: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1972 - accuracy: 0.9271 - val_loss: 0.1263 - val_accuracy: 0.9610\n",
            "Epoch 332/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1484 - accuracy: 0.9464\n",
            "Epoch 332: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1655 - accuracy: 0.9387 - val_loss: 0.1452 - val_accuracy: 0.9645\n",
            "Epoch 333/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1643 - accuracy: 0.9347\n",
            "Epoch 333: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1637 - accuracy: 0.9351 - val_loss: 0.1140 - val_accuracy: 0.9645\n",
            "Epoch 334/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1563 - accuracy: 0.9417\n",
            "Epoch 334: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9396 - val_loss: 0.1064 - val_accuracy: 0.9574\n",
            "Epoch 335/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1942 - accuracy: 0.9284\n",
            "Epoch 335: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1950 - accuracy: 0.9271 - val_loss: 0.1366 - val_accuracy: 0.9504\n",
            "Epoch 336/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1754 - accuracy: 0.9284\n",
            "Epoch 336: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9316 - val_loss: 0.1271 - val_accuracy: 0.9574\n",
            "Epoch 337/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1610 - accuracy: 0.9342\n",
            "Epoch 337: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.9324 - val_loss: 0.1730 - val_accuracy: 0.9610\n",
            "Epoch 338/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.2282 - accuracy: 0.9253\n",
            "Epoch 338: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2138 - accuracy: 0.9262 - val_loss: 0.1604 - val_accuracy: 0.9539\n",
            "Epoch 339/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1499 - accuracy: 0.9403\n",
            "Epoch 339: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9369 - val_loss: 0.1343 - val_accuracy: 0.9610\n",
            "Epoch 340/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1751 - accuracy: 0.9357\n",
            "Epoch 340: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9342 - val_loss: 0.1625 - val_accuracy: 0.9716\n",
            "Epoch 341/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1740 - accuracy: 0.9326\n",
            "Epoch 341: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.9351 - val_loss: 0.1584 - val_accuracy: 0.9645\n",
            "Epoch 342/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1827 - accuracy: 0.9278\n",
            "Epoch 342: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9262 - val_loss: 0.1257 - val_accuracy: 0.9645\n",
            "Epoch 343/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1895 - accuracy: 0.9256\n",
            "Epoch 343: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1889 - accuracy: 0.9253 - val_loss: 0.1186 - val_accuracy: 0.9539\n",
            "Epoch 344/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1842 - accuracy: 0.9332\n",
            "Epoch 344: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9333 - val_loss: 0.1396 - val_accuracy: 0.9610\n",
            "Epoch 345/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1842 - accuracy: 0.9318\n",
            "Epoch 345: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1804 - accuracy: 0.9324 - val_loss: 0.1677 - val_accuracy: 0.9539\n",
            "Epoch 346/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1834 - accuracy: 0.9356\n",
            "Epoch 346: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1860 - accuracy: 0.9324 - val_loss: 0.1493 - val_accuracy: 0.9433\n",
            "Epoch 347/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1740 - accuracy: 0.9343\n",
            "Epoch 347: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9289 - val_loss: 0.1078 - val_accuracy: 0.9752\n",
            "Epoch 348/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2163 - accuracy: 0.9326\n",
            "Epoch 348: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.2172 - accuracy: 0.9324 - val_loss: 0.1271 - val_accuracy: 0.9574\n",
            "Epoch 349/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1661 - accuracy: 0.9365\n",
            "Epoch 349: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.9360 - val_loss: 0.1481 - val_accuracy: 0.9610\n",
            "Epoch 350/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1851 - accuracy: 0.9286\n",
            "Epoch 350: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9316 - val_loss: 0.1452 - val_accuracy: 0.9716\n",
            "Epoch 351/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2024 - accuracy: 0.9404\n",
            "Epoch 351: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9387 - val_loss: 0.1725 - val_accuracy: 0.9433\n",
            "Epoch 352/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1908 - accuracy: 0.9325\n",
            "Epoch 352: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1873 - accuracy: 0.9324 - val_loss: 0.1604 - val_accuracy: 0.9574\n",
            "Epoch 353/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1612 - accuracy: 0.9356\n",
            "Epoch 353: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9360 - val_loss: 0.1576 - val_accuracy: 0.9645\n",
            "Epoch 354/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1650 - accuracy: 0.9344\n",
            "Epoch 354: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1686 - accuracy: 0.9316 - val_loss: 0.1673 - val_accuracy: 0.9539\n",
            "Epoch 355/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1716 - accuracy: 0.9333\n",
            "Epoch 355: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9324 - val_loss: 0.1347 - val_accuracy: 0.9645\n",
            "Epoch 356/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.9476\n",
            "Epoch 356: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9476 - val_loss: 0.1202 - val_accuracy: 0.9645\n",
            "Epoch 357/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1875 - accuracy: 0.9366\n",
            "Epoch 357: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1822 - accuracy: 0.9369 - val_loss: 0.1782 - val_accuracy: 0.9504\n",
            "Epoch 358/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1651 - accuracy: 0.9352\n",
            "Epoch 358: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1557 - accuracy: 0.9387 - val_loss: 0.1306 - val_accuracy: 0.9539\n",
            "Epoch 359/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1666 - accuracy: 0.9447\n",
            "Epoch 359: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9413 - val_loss: 0.1273 - val_accuracy: 0.9645\n",
            "Epoch 360/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1735 - accuracy: 0.9346\n",
            "Epoch 360: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9342 - val_loss: 0.1149 - val_accuracy: 0.9716\n",
            "Epoch 361/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1834 - accuracy: 0.9318\n",
            "Epoch 361: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9324 - val_loss: 0.1327 - val_accuracy: 0.9610\n",
            "Epoch 362/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1434 - accuracy: 0.9473\n",
            "Epoch 362: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9458 - val_loss: 0.1458 - val_accuracy: 0.9610\n",
            "Epoch 363/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1703 - accuracy: 0.9385\n",
            "Epoch 363: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1625 - accuracy: 0.9413 - val_loss: 0.1493 - val_accuracy: 0.9468\n",
            "Epoch 364/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1511 - accuracy: 0.9440\n",
            "Epoch 364: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9369 - val_loss: 0.1563 - val_accuracy: 0.9539\n",
            "Epoch 365/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1683 - accuracy: 0.9414\n",
            "Epoch 365: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1666 - accuracy: 0.9396 - val_loss: 0.1574 - val_accuracy: 0.9504\n",
            "Epoch 366/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9424\n",
            "Epoch 366: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1617 - accuracy: 0.9413 - val_loss: 0.1581 - val_accuracy: 0.9574\n",
            "Epoch 367/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9458\n",
            "Epoch 367: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.9440 - val_loss: 0.1820 - val_accuracy: 0.9504\n",
            "Epoch 368/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1772 - accuracy: 0.9356\n",
            "Epoch 368: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1805 - accuracy: 0.9324 - val_loss: 0.1848 - val_accuracy: 0.9468\n",
            "Epoch 369/1000\n",
            "21/36 [================>.............] - ETA: 0s - loss: 0.1816 - accuracy: 0.9390\n",
            "Epoch 369: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9476 - val_loss: 0.1416 - val_accuracy: 0.9610\n",
            "Epoch 370/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1807 - accuracy: 0.9356\n",
            "Epoch 370: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1838 - accuracy: 0.9342 - val_loss: 0.1715 - val_accuracy: 0.9468\n",
            "Epoch 371/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.2210 - accuracy: 0.9261\n",
            "Epoch 371: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2175 - accuracy: 0.9271 - val_loss: 0.1415 - val_accuracy: 0.9539\n",
            "Epoch 372/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1759 - accuracy: 0.9267\n",
            "Epoch 372: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1761 - accuracy: 0.9307 - val_loss: 0.1318 - val_accuracy: 0.9645\n",
            "Epoch 373/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1771 - accuracy: 0.9329\n",
            "Epoch 373: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1805 - accuracy: 0.9316 - val_loss: 0.1232 - val_accuracy: 0.9574\n",
            "Epoch 374/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1430 - accuracy: 0.9447\n",
            "Epoch 374: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1582 - accuracy: 0.9351 - val_loss: 0.1322 - val_accuracy: 0.9610\n",
            "Epoch 375/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1813 - accuracy: 0.9282\n",
            "Epoch 375: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1727 - accuracy: 0.9271 - val_loss: 0.1231 - val_accuracy: 0.9610\n",
            "Epoch 376/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9393\n",
            "Epoch 376: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1772 - accuracy: 0.9404 - val_loss: 0.1056 - val_accuracy: 0.9610\n",
            "Epoch 377/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1311 - accuracy: 0.9459\n",
            "Epoch 377: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1134 - accuracy: 0.9538 - val_loss: 0.1144 - val_accuracy: 0.9716\n",
            "Epoch 378/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1662 - accuracy: 0.9375\n",
            "Epoch 378: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1643 - accuracy: 0.9351 - val_loss: 0.1536 - val_accuracy: 0.9574\n",
            "Epoch 379/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9348\n",
            "Epoch 379: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2079 - accuracy: 0.9351 - val_loss: 0.1460 - val_accuracy: 0.9645\n",
            "Epoch 380/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1645 - accuracy: 0.9384\n",
            "Epoch 380: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1695 - accuracy: 0.9387 - val_loss: 0.1206 - val_accuracy: 0.9610\n",
            "Epoch 381/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9526\n",
            "Epoch 381: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9502 - val_loss: 0.1564 - val_accuracy: 0.9645\n",
            "Epoch 382/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1613 - accuracy: 0.9410\n",
            "Epoch 382: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9404 - val_loss: 0.1564 - val_accuracy: 0.9504\n",
            "Epoch 383/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1991 - accuracy: 0.9241\n",
            "Epoch 383: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1998 - accuracy: 0.9271 - val_loss: 0.1741 - val_accuracy: 0.9610\n",
            "Epoch 384/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.2276 - accuracy: 0.9189\n",
            "Epoch 384: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.2257 - accuracy: 0.9200 - val_loss: 0.1412 - val_accuracy: 0.9539\n",
            "Epoch 385/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1950 - accuracy: 0.9256\n",
            "Epoch 385: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1903 - accuracy: 0.9271 - val_loss: 0.1125 - val_accuracy: 0.9645\n",
            "Epoch 386/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1690 - accuracy: 0.9438\n",
            "Epoch 386: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1700 - accuracy: 0.9413 - val_loss: 0.1208 - val_accuracy: 0.9610\n",
            "Epoch 387/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9347\n",
            "Epoch 387: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1772 - accuracy: 0.9360 - val_loss: 0.1113 - val_accuracy: 0.9716\n",
            "Epoch 388/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9366\n",
            "Epoch 388: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 11ms/step - loss: 0.1751 - accuracy: 0.9369 - val_loss: 0.1269 - val_accuracy: 0.9716\n",
            "Epoch 389/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9214\n",
            "Epoch 389: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.2134 - accuracy: 0.9218 - val_loss: 0.1027 - val_accuracy: 0.9681\n",
            "Epoch 390/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9339\n",
            "Epoch 390: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1613 - accuracy: 0.9342 - val_loss: 0.1042 - val_accuracy: 0.9681\n",
            "Epoch 391/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1380 - accuracy: 0.9422\n",
            "Epoch 391: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1340 - accuracy: 0.9431 - val_loss: 0.1043 - val_accuracy: 0.9681\n",
            "Epoch 392/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1398 - accuracy: 0.9446\n",
            "Epoch 392: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9431 - val_loss: 0.0969 - val_accuracy: 0.9610\n",
            "Epoch 393/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1753 - accuracy: 0.9388\n",
            "Epoch 393: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2016 - accuracy: 0.9289 - val_loss: 0.1423 - val_accuracy: 0.9610\n",
            "Epoch 394/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.2082 - accuracy: 0.9274\n",
            "Epoch 394: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2164 - accuracy: 0.9253 - val_loss: 0.1920 - val_accuracy: 0.9433\n",
            "Epoch 395/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1709 - accuracy: 0.9396\n",
            "Epoch 395: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.9404 - val_loss: 0.1475 - val_accuracy: 0.9539\n",
            "Epoch 396/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1366 - accuracy: 0.9458\n",
            "Epoch 396: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1457 - accuracy: 0.9413 - val_loss: 0.1391 - val_accuracy: 0.9681\n",
            "Epoch 397/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1418 - accuracy: 0.9435\n",
            "Epoch 397: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9404 - val_loss: 0.1160 - val_accuracy: 0.9574\n",
            "Epoch 398/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1978 - accuracy: 0.9435\n",
            "Epoch 398: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1941 - accuracy: 0.9458 - val_loss: 0.1569 - val_accuracy: 0.9645\n",
            "Epoch 399/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1618 - accuracy: 0.9411\n",
            "Epoch 399: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9378 - val_loss: 0.1403 - val_accuracy: 0.9645\n",
            "Epoch 400/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1545 - accuracy: 0.9510\n",
            "Epoch 400: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1573 - accuracy: 0.9493 - val_loss: 0.1405 - val_accuracy: 0.9610\n",
            "Epoch 401/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1930 - accuracy: 0.9294\n",
            "Epoch 401: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.9342 - val_loss: 0.1238 - val_accuracy: 0.9645\n",
            "Epoch 402/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1543 - accuracy: 0.9411\n",
            "Epoch 402: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.9378 - val_loss: 0.1074 - val_accuracy: 0.9716\n",
            "Epoch 403/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9384\n",
            "Epoch 403: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1600 - accuracy: 0.9387 - val_loss: 0.1034 - val_accuracy: 0.9752\n",
            "Epoch 404/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1801 - accuracy: 0.9284\n",
            "Epoch 404: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9342 - val_loss: 0.1049 - val_accuracy: 0.9645\n",
            "Epoch 405/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1476 - accuracy: 0.9483\n",
            "Epoch 405: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9422 - val_loss: 0.1234 - val_accuracy: 0.9716\n",
            "Epoch 406/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1455 - accuracy: 0.9472\n",
            "Epoch 406: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1462 - accuracy: 0.9431 - val_loss: 0.1200 - val_accuracy: 0.9752\n",
            "Epoch 407/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9435\n",
            "Epoch 407: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1523 - accuracy: 0.9422 - val_loss: 0.1207 - val_accuracy: 0.9716\n",
            "Epoch 408/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1492 - accuracy: 0.9447\n",
            "Epoch 408: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9458 - val_loss: 0.0972 - val_accuracy: 0.9716\n",
            "Epoch 409/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1923 - accuracy: 0.9260\n",
            "Epoch 409: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1892 - accuracy: 0.9271 - val_loss: 0.1407 - val_accuracy: 0.9574\n",
            "Epoch 410/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1710 - accuracy: 0.9321\n",
            "Epoch 410: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1847 - accuracy: 0.9289 - val_loss: 0.1455 - val_accuracy: 0.9574\n",
            "Epoch 411/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1965 - accuracy: 0.9469\n",
            "Epoch 411: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1983 - accuracy: 0.9467 - val_loss: 0.1483 - val_accuracy: 0.9539\n",
            "Epoch 412/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.2078 - accuracy: 0.9219\n",
            "Epoch 412: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1934 - accuracy: 0.9280 - val_loss: 0.1170 - val_accuracy: 0.9645\n",
            "Epoch 413/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1618 - accuracy: 0.9375\n",
            "Epoch 413: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1620 - accuracy: 0.9342 - val_loss: 0.1317 - val_accuracy: 0.9645\n",
            "Epoch 414/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1393 - accuracy: 0.9417\n",
            "Epoch 414: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9413 - val_loss: 0.1101 - val_accuracy: 0.9787\n",
            "Epoch 415/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9458\n",
            "Epoch 415: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9431 - val_loss: 0.1332 - val_accuracy: 0.9645\n",
            "Epoch 416/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1726 - accuracy: 0.9365\n",
            "Epoch 416: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1813 - accuracy: 0.9324 - val_loss: 0.1615 - val_accuracy: 0.9468\n",
            "Epoch 417/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1625 - accuracy: 0.9317\n",
            "Epoch 417: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1629 - accuracy: 0.9342 - val_loss: 0.1406 - val_accuracy: 0.9539\n",
            "Epoch 418/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1903 - accuracy: 0.9256\n",
            "Epoch 418: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1950 - accuracy: 0.9227 - val_loss: 0.1370 - val_accuracy: 0.9645\n",
            "Epoch 419/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1939 - accuracy: 0.9317\n",
            "Epoch 419: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9342 - val_loss: 0.1469 - val_accuracy: 0.9610\n",
            "Epoch 420/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1311 - accuracy: 0.9492\n",
            "Epoch 420: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1359 - accuracy: 0.9484 - val_loss: 0.1452 - val_accuracy: 0.9539\n",
            "Epoch 421/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1540 - accuracy: 0.9403\n",
            "Epoch 421: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9396 - val_loss: 0.1690 - val_accuracy: 0.9468\n",
            "Epoch 422/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2194 - accuracy: 0.9375\n",
            "Epoch 422: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2084 - accuracy: 0.9404 - val_loss: 0.1395 - val_accuracy: 0.9610\n",
            "Epoch 423/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1787 - accuracy: 0.9488\n",
            "Epoch 423: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1839 - accuracy: 0.9431 - val_loss: 0.1500 - val_accuracy: 0.9574\n",
            "Epoch 424/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1701 - accuracy: 0.9316\n",
            "Epoch 424: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9289 - val_loss: 0.1444 - val_accuracy: 0.9645\n",
            "Epoch 425/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1420 - accuracy: 0.9469\n",
            "Epoch 425: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.9422 - val_loss: 0.1555 - val_accuracy: 0.9610\n",
            "Epoch 426/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1349 - accuracy: 0.9510\n",
            "Epoch 426: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9476 - val_loss: 0.1429 - val_accuracy: 0.9645\n",
            "Epoch 427/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1504 - accuracy: 0.9375\n",
            "Epoch 427: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1511 - accuracy: 0.9369 - val_loss: 0.1697 - val_accuracy: 0.9539\n",
            "Epoch 428/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9360\n",
            "Epoch 428: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1663 - accuracy: 0.9360 - val_loss: 0.1289 - val_accuracy: 0.9681\n",
            "Epoch 429/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1439 - accuracy: 0.9443\n",
            "Epoch 429: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1496 - accuracy: 0.9422 - val_loss: 0.1346 - val_accuracy: 0.9645\n",
            "Epoch 430/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1417 - accuracy: 0.9468\n",
            "Epoch 430: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9387 - val_loss: 0.1541 - val_accuracy: 0.9681\n",
            "Epoch 431/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1277 - accuracy: 0.9526\n",
            "Epoch 431: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9556 - val_loss: 0.1390 - val_accuracy: 0.9610\n",
            "Epoch 432/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1509 - accuracy: 0.9453\n",
            "Epoch 432: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1459 - accuracy: 0.9476 - val_loss: 0.1524 - val_accuracy: 0.9610\n",
            "Epoch 433/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1732 - accuracy: 0.9413\n",
            "Epoch 433: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1713 - accuracy: 0.9413 - val_loss: 0.1133 - val_accuracy: 0.9610\n",
            "Epoch 434/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1623 - accuracy: 0.9431\n",
            "Epoch 434: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.9431 - val_loss: 0.1104 - val_accuracy: 0.9610\n",
            "Epoch 435/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1361 - accuracy: 0.9473\n",
            "Epoch 435: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1379 - accuracy: 0.9458 - val_loss: 0.1190 - val_accuracy: 0.9681\n",
            "Epoch 436/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1359 - accuracy: 0.9516\n",
            "Epoch 436: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1591 - accuracy: 0.9440 - val_loss: 0.1354 - val_accuracy: 0.9610\n",
            "Epoch 437/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1367 - accuracy: 0.9527\n",
            "Epoch 437: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1287 - accuracy: 0.9556 - val_loss: 0.1309 - val_accuracy: 0.9574\n",
            "Epoch 438/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9472\n",
            "Epoch 438: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1423 - accuracy: 0.9467 - val_loss: 0.1155 - val_accuracy: 0.9645\n",
            "Epoch 439/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1376 - accuracy: 0.9495\n",
            "Epoch 439: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1799 - accuracy: 0.9449 - val_loss: 0.1507 - val_accuracy: 0.9468\n",
            "Epoch 440/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1604 - accuracy: 0.9420\n",
            "Epoch 440: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1667 - accuracy: 0.9396 - val_loss: 0.1210 - val_accuracy: 0.9645\n",
            "Epoch 441/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1692 - accuracy: 0.9438\n",
            "Epoch 441: val_loss did not improve from 0.09046\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1580 - accuracy: 0.9476 - val_loss: 0.1116 - val_accuracy: 0.9681\n",
            "Epoch 442/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1616 - accuracy: 0.9449\n",
            "Epoch 442: val_loss improved from 0.09046 to 0.08704, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1571 - accuracy: 0.9467 - val_loss: 0.0870 - val_accuracy: 0.9716\n",
            "Epoch 443/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9502\n",
            "Epoch 443: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1525 - accuracy: 0.9502 - val_loss: 0.0936 - val_accuracy: 0.9716\n",
            "Epoch 444/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9369\n",
            "Epoch 444: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1646 - accuracy: 0.9369 - val_loss: 0.1015 - val_accuracy: 0.9645\n",
            "Epoch 445/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1782 - accuracy: 0.9364\n",
            "Epoch 445: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9387 - val_loss: 0.1437 - val_accuracy: 0.9539\n",
            "Epoch 446/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1560 - accuracy: 0.9443\n",
            "Epoch 446: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1598 - accuracy: 0.9413 - val_loss: 0.1245 - val_accuracy: 0.9610\n",
            "Epoch 447/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1675 - accuracy: 0.9375\n",
            "Epoch 447: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1580 - accuracy: 0.9422 - val_loss: 0.1026 - val_accuracy: 0.9716\n",
            "Epoch 448/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1287 - accuracy: 0.9486\n",
            "Epoch 448: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1311 - accuracy: 0.9511 - val_loss: 0.1142 - val_accuracy: 0.9716\n",
            "Epoch 449/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1429 - accuracy: 0.9435\n",
            "Epoch 449: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1441 - accuracy: 0.9422 - val_loss: 0.1218 - val_accuracy: 0.9716\n",
            "Epoch 450/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1210 - accuracy: 0.9516\n",
            "Epoch 450: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1214 - accuracy: 0.9538 - val_loss: 0.1310 - val_accuracy: 0.9610\n",
            "Epoch 451/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9531\n",
            "Epoch 451: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1336 - accuracy: 0.9547 - val_loss: 0.1152 - val_accuracy: 0.9610\n",
            "Epoch 452/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.9493\n",
            "Epoch 452: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1373 - accuracy: 0.9493 - val_loss: 0.1004 - val_accuracy: 0.9681\n",
            "Epoch 453/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1529 - accuracy: 0.9446\n",
            "Epoch 453: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1549 - accuracy: 0.9440 - val_loss: 0.1147 - val_accuracy: 0.9645\n",
            "Epoch 454/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9393\n",
            "Epoch 454: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1603 - accuracy: 0.9396 - val_loss: 0.1073 - val_accuracy: 0.9645\n",
            "Epoch 455/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1293 - accuracy: 0.9470\n",
            "Epoch 455: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1307 - accuracy: 0.9476 - val_loss: 0.1278 - val_accuracy: 0.9574\n",
            "Epoch 456/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1530 - accuracy: 0.9407\n",
            "Epoch 456: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1432 - accuracy: 0.9449 - val_loss: 0.1230 - val_accuracy: 0.9681\n",
            "Epoch 457/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1549 - accuracy: 0.9444\n",
            "Epoch 457: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9484 - val_loss: 0.1960 - val_accuracy: 0.9504\n",
            "Epoch 458/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1830 - accuracy: 0.9375\n",
            "Epoch 458: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1814 - accuracy: 0.9369 - val_loss: 0.1247 - val_accuracy: 0.9468\n",
            "Epoch 459/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1431 - accuracy: 0.9385\n",
            "Epoch 459: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1558 - accuracy: 0.9351 - val_loss: 0.1063 - val_accuracy: 0.9681\n",
            "Epoch 460/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1590 - accuracy: 0.9417\n",
            "Epoch 460: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9484 - val_loss: 0.1014 - val_accuracy: 0.9716\n",
            "Epoch 461/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9458\n",
            "Epoch 461: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9458 - val_loss: 0.0941 - val_accuracy: 0.9645\n",
            "Epoch 462/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1164 - accuracy: 0.9537\n",
            "Epoch 462: val_loss did not improve from 0.08704\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9484 - val_loss: 0.1075 - val_accuracy: 0.9610\n",
            "Epoch 463/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1751 - accuracy: 0.9456\n",
            "Epoch 463: val_loss improved from 0.08704 to 0.08500, saving model to saved_models/guitar_chords.hdf5\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1765 - accuracy: 0.9431 - val_loss: 0.0850 - val_accuracy: 0.9716\n",
            "Epoch 464/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1667 - accuracy: 0.9361\n",
            "Epoch 464: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9351 - val_loss: 0.1265 - val_accuracy: 0.9610\n",
            "Epoch 465/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.9483\n",
            "Epoch 465: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1909 - accuracy: 0.9396 - val_loss: 0.1097 - val_accuracy: 0.9610\n",
            "Epoch 466/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1623 - accuracy: 0.9412\n",
            "Epoch 466: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1611 - accuracy: 0.9413 - val_loss: 0.1008 - val_accuracy: 0.9681\n",
            "Epoch 467/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1698 - accuracy: 0.9355\n",
            "Epoch 467: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1772 - accuracy: 0.9342 - val_loss: 0.1007 - val_accuracy: 0.9645\n",
            "Epoch 468/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1685 - accuracy: 0.9550\n",
            "Epoch 468: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9511 - val_loss: 0.1140 - val_accuracy: 0.9539\n",
            "Epoch 469/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.2053 - accuracy: 0.9275\n",
            "Epoch 469: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2042 - accuracy: 0.9271 - val_loss: 0.1164 - val_accuracy: 0.9539\n",
            "Epoch 470/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1418 - accuracy: 0.9440\n",
            "Epoch 470: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9351 - val_loss: 0.1025 - val_accuracy: 0.9610\n",
            "Epoch 471/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1693 - accuracy: 0.9353\n",
            "Epoch 471: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1861 - accuracy: 0.9316 - val_loss: 0.1061 - val_accuracy: 0.9504\n",
            "Epoch 472/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1630 - accuracy: 0.9365\n",
            "Epoch 472: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9369 - val_loss: 0.1260 - val_accuracy: 0.9468\n",
            "Epoch 473/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1467 - accuracy: 0.9461\n",
            "Epoch 473: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1515 - accuracy: 0.9449 - val_loss: 0.1206 - val_accuracy: 0.9610\n",
            "Epoch 474/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1449 - accuracy: 0.9420\n",
            "Epoch 474: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1402 - accuracy: 0.9440 - val_loss: 0.1156 - val_accuracy: 0.9574\n",
            "Epoch 475/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1334 - accuracy: 0.9498\n",
            "Epoch 475: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9467 - val_loss: 0.1117 - val_accuracy: 0.9610\n",
            "Epoch 476/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1358 - accuracy: 0.9483\n",
            "Epoch 476: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9476 - val_loss: 0.1319 - val_accuracy: 0.9681\n",
            "Epoch 477/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.2087 - accuracy: 0.9494\n",
            "Epoch 477: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2046 - accuracy: 0.9476 - val_loss: 0.1617 - val_accuracy: 0.9504\n",
            "Epoch 478/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9330\n",
            "Epoch 478: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1615 - accuracy: 0.9324 - val_loss: 0.1228 - val_accuracy: 0.9610\n",
            "Epoch 479/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1189 - accuracy: 0.9601\n",
            "Epoch 479: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9556 - val_loss: 0.1226 - val_accuracy: 0.9574\n",
            "Epoch 480/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1779 - accuracy: 0.9294\n",
            "Epoch 480: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9298 - val_loss: 0.1127 - val_accuracy: 0.9610\n",
            "Epoch 481/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1314 - accuracy: 0.9413\n",
            "Epoch 481: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1543 - accuracy: 0.9360 - val_loss: 0.1146 - val_accuracy: 0.9574\n",
            "Epoch 482/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1688 - accuracy: 0.9335\n",
            "Epoch 482: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1730 - accuracy: 0.9342 - val_loss: 0.1571 - val_accuracy: 0.9645\n",
            "Epoch 483/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1437 - accuracy: 0.9443\n",
            "Epoch 483: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1536 - accuracy: 0.9431 - val_loss: 0.1272 - val_accuracy: 0.9645\n",
            "Epoch 484/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1727 - accuracy: 0.9334\n",
            "Epoch 484: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.9324 - val_loss: 0.1260 - val_accuracy: 0.9610\n",
            "Epoch 485/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1680 - accuracy: 0.9275\n",
            "Epoch 485: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1702 - accuracy: 0.9262 - val_loss: 0.1165 - val_accuracy: 0.9681\n",
            "Epoch 486/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1283 - accuracy: 0.9497\n",
            "Epoch 486: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9458 - val_loss: 0.1262 - val_accuracy: 0.9610\n",
            "Epoch 487/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1410 - accuracy: 0.9385\n",
            "Epoch 487: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9404 - val_loss: 0.1278 - val_accuracy: 0.9716\n",
            "Epoch 488/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1979 - accuracy: 0.9456\n",
            "Epoch 488: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1830 - accuracy: 0.9493 - val_loss: 0.1214 - val_accuracy: 0.9681\n",
            "Epoch 489/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1359 - accuracy: 0.9468\n",
            "Epoch 489: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1373 - accuracy: 0.9467 - val_loss: 0.1201 - val_accuracy: 0.9681\n",
            "Epoch 490/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1541 - accuracy: 0.9479\n",
            "Epoch 490: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9476 - val_loss: 0.1374 - val_accuracy: 0.9574\n",
            "Epoch 491/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1671 - accuracy: 0.9365\n",
            "Epoch 491: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1634 - accuracy: 0.9360 - val_loss: 0.1443 - val_accuracy: 0.9504\n",
            "Epoch 492/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1397 - accuracy: 0.9496\n",
            "Epoch 492: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9484 - val_loss: 0.1517 - val_accuracy: 0.9681\n",
            "Epoch 493/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9375\n",
            "Epoch 493: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1746 - accuracy: 0.9378 - val_loss: 0.1076 - val_accuracy: 0.9752\n",
            "Epoch 494/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1415 - accuracy: 0.9418\n",
            "Epoch 494: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9431 - val_loss: 0.1077 - val_accuracy: 0.9681\n",
            "Epoch 495/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1475 - accuracy: 0.9555\n",
            "Epoch 495: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1420 - accuracy: 0.9564 - val_loss: 0.1035 - val_accuracy: 0.9681\n",
            "Epoch 496/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1582 - accuracy: 0.9513\n",
            "Epoch 496: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1457 - accuracy: 0.9493 - val_loss: 0.1124 - val_accuracy: 0.9716\n",
            "Epoch 497/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1529 - accuracy: 0.9407\n",
            "Epoch 497: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1474 - accuracy: 0.9431 - val_loss: 0.1424 - val_accuracy: 0.9610\n",
            "Epoch 498/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1774 - accuracy: 0.9418\n",
            "Epoch 498: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.9378 - val_loss: 0.1182 - val_accuracy: 0.9716\n",
            "Epoch 499/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1759 - accuracy: 0.9418\n",
            "Epoch 499: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9413 - val_loss: 0.1146 - val_accuracy: 0.9681\n",
            "Epoch 500/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1282 - accuracy: 0.9486\n",
            "Epoch 500: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9502 - val_loss: 0.1195 - val_accuracy: 0.9574\n",
            "Epoch 501/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1533 - accuracy: 0.9466\n",
            "Epoch 501: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1688 - accuracy: 0.9431 - val_loss: 0.1104 - val_accuracy: 0.9574\n",
            "Epoch 502/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1519 - accuracy: 0.9466\n",
            "Epoch 502: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1603 - accuracy: 0.9449 - val_loss: 0.1160 - val_accuracy: 0.9681\n",
            "Epoch 503/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1682 - accuracy: 0.9435\n",
            "Epoch 503: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1647 - accuracy: 0.9449 - val_loss: 0.1155 - val_accuracy: 0.9681\n",
            "Epoch 504/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9531\n",
            "Epoch 504: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1228 - accuracy: 0.9529 - val_loss: 0.1192 - val_accuracy: 0.9574\n",
            "Epoch 505/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1355 - accuracy: 0.9494\n",
            "Epoch 505: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1454 - accuracy: 0.9458 - val_loss: 0.1072 - val_accuracy: 0.9681\n",
            "Epoch 506/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1381 - accuracy: 0.9442\n",
            "Epoch 506: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1269 - accuracy: 0.9493 - val_loss: 0.0942 - val_accuracy: 0.9645\n",
            "Epoch 507/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1377 - accuracy: 0.9463\n",
            "Epoch 507: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1424 - accuracy: 0.9440 - val_loss: 0.1030 - val_accuracy: 0.9645\n",
            "Epoch 508/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1304 - accuracy: 0.9494\n",
            "Epoch 508: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1289 - accuracy: 0.9502 - val_loss: 0.1156 - val_accuracy: 0.9610\n",
            "Epoch 509/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1147 - accuracy: 0.9500\n",
            "Epoch 509: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1496 - accuracy: 0.9467 - val_loss: 0.1049 - val_accuracy: 0.9645\n",
            "Epoch 510/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1594 - accuracy: 0.9348\n",
            "Epoch 510: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1595 - accuracy: 0.9351 - val_loss: 0.1001 - val_accuracy: 0.9610\n",
            "Epoch 511/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1617 - accuracy: 0.9375\n",
            "Epoch 511: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1599 - accuracy: 0.9413 - val_loss: 0.1178 - val_accuracy: 0.9574\n",
            "Epoch 512/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9411\n",
            "Epoch 512: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1402 - accuracy: 0.9404 - val_loss: 0.1354 - val_accuracy: 0.9645\n",
            "Epoch 513/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1454 - accuracy: 0.9430\n",
            "Epoch 513: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1460 - accuracy: 0.9431 - val_loss: 0.1113 - val_accuracy: 0.9716\n",
            "Epoch 514/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1495 - accuracy: 0.9504\n",
            "Epoch 514: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1492 - accuracy: 0.9502 - val_loss: 0.1122 - val_accuracy: 0.9681\n",
            "Epoch 515/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1248 - accuracy: 0.9498\n",
            "Epoch 515: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1255 - accuracy: 0.9502 - val_loss: 0.1136 - val_accuracy: 0.9716\n",
            "Epoch 516/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1629 - accuracy: 0.9385\n",
            "Epoch 516: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1568 - accuracy: 0.9404 - val_loss: 0.1229 - val_accuracy: 0.9681\n",
            "Epoch 517/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1289 - accuracy: 0.9565\n",
            "Epoch 517: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9520 - val_loss: 0.1124 - val_accuracy: 0.9645\n",
            "Epoch 518/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1416 - accuracy: 0.9508\n",
            "Epoch 518: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1443 - accuracy: 0.9493 - val_loss: 0.0977 - val_accuracy: 0.9752\n",
            "Epoch 519/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.9476\n",
            "Epoch 519: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1478 - accuracy: 0.9476 - val_loss: 0.1318 - val_accuracy: 0.9539\n",
            "Epoch 520/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9393\n",
            "Epoch 520: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1663 - accuracy: 0.9404 - val_loss: 0.1289 - val_accuracy: 0.9610\n",
            "Epoch 521/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1855 - accuracy: 0.9333\n",
            "Epoch 521: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1855 - accuracy: 0.9333 - val_loss: 0.1333 - val_accuracy: 0.9610\n",
            "Epoch 522/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1322 - accuracy: 0.9418\n",
            "Epoch 522: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1425 - accuracy: 0.9387 - val_loss: 0.1144 - val_accuracy: 0.9681\n",
            "Epoch 523/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9482\n",
            "Epoch 523: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9484 - val_loss: 0.0902 - val_accuracy: 0.9752\n",
            "Epoch 524/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1248 - accuracy: 0.9483\n",
            "Epoch 524: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1289 - accuracy: 0.9484 - val_loss: 0.1121 - val_accuracy: 0.9752\n",
            "Epoch 525/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1317 - accuracy: 0.9538\n",
            "Epoch 525: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9529 - val_loss: 0.1086 - val_accuracy: 0.9716\n",
            "Epoch 526/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1750 - accuracy: 0.9387\n",
            "Epoch 526: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9333 - val_loss: 0.0929 - val_accuracy: 0.9681\n",
            "Epoch 527/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1476 - accuracy: 0.9543\n",
            "Epoch 527: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9449 - val_loss: 0.0981 - val_accuracy: 0.9681\n",
            "Epoch 528/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1592 - accuracy: 0.9427\n",
            "Epoch 528: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9449 - val_loss: 0.0995 - val_accuracy: 0.9681\n",
            "Epoch 529/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1660 - accuracy: 0.9411\n",
            "Epoch 529: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1833 - accuracy: 0.9324 - val_loss: 0.0966 - val_accuracy: 0.9752\n",
            "Epoch 530/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1468 - accuracy: 0.9375\n",
            "Epoch 530: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1494 - accuracy: 0.9387 - val_loss: 0.1174 - val_accuracy: 0.9681\n",
            "Epoch 531/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1480 - accuracy: 0.9491\n",
            "Epoch 531: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9449 - val_loss: 0.1291 - val_accuracy: 0.9716\n",
            "Epoch 532/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1629 - accuracy: 0.9516\n",
            "Epoch 532: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1610 - accuracy: 0.9511 - val_loss: 0.1516 - val_accuracy: 0.9539\n",
            "Epoch 533/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1752 - accuracy: 0.9329\n",
            "Epoch 533: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1757 - accuracy: 0.9351 - val_loss: 0.1107 - val_accuracy: 0.9645\n",
            "Epoch 534/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2126 - accuracy: 0.9327\n",
            "Epoch 534: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.2003 - accuracy: 0.9342 - val_loss: 0.1065 - val_accuracy: 0.9752\n",
            "Epoch 535/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1456 - accuracy: 0.9504\n",
            "Epoch 535: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9493 - val_loss: 0.1035 - val_accuracy: 0.9752\n",
            "Epoch 536/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1357 - accuracy: 0.9423\n",
            "Epoch 536: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1310 - accuracy: 0.9422 - val_loss: 0.1161 - val_accuracy: 0.9645\n",
            "Epoch 537/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1515 - accuracy: 0.9413\n",
            "Epoch 537: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1448 - accuracy: 0.9449 - val_loss: 0.1119 - val_accuracy: 0.9645\n",
            "Epoch 538/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1711 - accuracy: 0.9453\n",
            "Epoch 538: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9440 - val_loss: 0.1005 - val_accuracy: 0.9787\n",
            "Epoch 539/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1450 - accuracy: 0.9468\n",
            "Epoch 539: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9458 - val_loss: 0.1183 - val_accuracy: 0.9610\n",
            "Epoch 540/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1396 - accuracy: 0.9488\n",
            "Epoch 540: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9493 - val_loss: 0.1150 - val_accuracy: 0.9681\n",
            "Epoch 541/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1186 - accuracy: 0.9549\n",
            "Epoch 541: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9484 - val_loss: 0.1297 - val_accuracy: 0.9610\n",
            "Epoch 542/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1808 - accuracy: 0.9375\n",
            "Epoch 542: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1844 - accuracy: 0.9351 - val_loss: 0.1014 - val_accuracy: 0.9752\n",
            "Epoch 543/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9438\n",
            "Epoch 543: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1570 - accuracy: 0.9440 - val_loss: 0.1220 - val_accuracy: 0.9681\n",
            "Epoch 544/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9455\n",
            "Epoch 544: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1493 - accuracy: 0.9458 - val_loss: 0.1272 - val_accuracy: 0.9610\n",
            "Epoch 545/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1532 - accuracy: 0.9399\n",
            "Epoch 545: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9458 - val_loss: 0.1092 - val_accuracy: 0.9681\n",
            "Epoch 546/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1279 - accuracy: 0.9487\n",
            "Epoch 546: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1388 - accuracy: 0.9449 - val_loss: 0.1379 - val_accuracy: 0.9610\n",
            "Epoch 547/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1683 - accuracy: 0.9411\n",
            "Epoch 547: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9396 - val_loss: 0.1457 - val_accuracy: 0.9574\n",
            "Epoch 548/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9397\n",
            "Epoch 548: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.9449 - val_loss: 0.1224 - val_accuracy: 0.9681\n",
            "Epoch 549/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9420\n",
            "Epoch 549: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1472 - accuracy: 0.9422 - val_loss: 0.1355 - val_accuracy: 0.9610\n",
            "Epoch 550/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1021 - accuracy: 0.9651\n",
            "Epoch 550: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1017 - accuracy: 0.9609 - val_loss: 0.1388 - val_accuracy: 0.9645\n",
            "Epoch 551/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1337 - accuracy: 0.9564\n",
            "Epoch 551: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9564 - val_loss: 0.1306 - val_accuracy: 0.9645\n",
            "Epoch 552/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1780 - accuracy: 0.9363\n",
            "Epoch 552: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1548 - accuracy: 0.9440 - val_loss: 0.1438 - val_accuracy: 0.9539\n",
            "Epoch 553/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1836 - accuracy: 0.9387\n",
            "Epoch 553: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.9396 - val_loss: 0.1397 - val_accuracy: 0.9610\n",
            "Epoch 554/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1440 - accuracy: 0.9456\n",
            "Epoch 554: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1435 - accuracy: 0.9467 - val_loss: 0.1244 - val_accuracy: 0.9752\n",
            "Epoch 555/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9583\n",
            "Epoch 555: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1195 - accuracy: 0.9564 - val_loss: 0.1032 - val_accuracy: 0.9645\n",
            "Epoch 556/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1466 - accuracy: 0.9362\n",
            "Epoch 556: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.9324 - val_loss: 0.1273 - val_accuracy: 0.9681\n",
            "Epoch 557/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1193 - accuracy: 0.9470\n",
            "Epoch 557: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9484 - val_loss: 0.1002 - val_accuracy: 0.9716\n",
            "Epoch 558/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1095 - accuracy: 0.9595\n",
            "Epoch 558: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1135 - accuracy: 0.9591 - val_loss: 0.1074 - val_accuracy: 0.9610\n",
            "Epoch 559/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9399\n",
            "Epoch 559: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1529 - accuracy: 0.9396 - val_loss: 0.1134 - val_accuracy: 0.9610\n",
            "Epoch 560/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1333 - accuracy: 0.9549\n",
            "Epoch 560: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9484 - val_loss: 0.1234 - val_accuracy: 0.9645\n",
            "Epoch 561/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9500\n",
            "Epoch 561: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9529 - val_loss: 0.1568 - val_accuracy: 0.9610\n",
            "Epoch 562/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.2261 - accuracy: 0.9279\n",
            "Epoch 562: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9316 - val_loss: 0.1137 - val_accuracy: 0.9752\n",
            "Epoch 563/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1300 - accuracy: 0.9514\n",
            "Epoch 563: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9538 - val_loss: 0.1156 - val_accuracy: 0.9681\n",
            "Epoch 564/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1129 - accuracy: 0.9555\n",
            "Epoch 564: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9520 - val_loss: 0.1238 - val_accuracy: 0.9645\n",
            "Epoch 565/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1440 - accuracy: 0.9468\n",
            "Epoch 565: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9467 - val_loss: 0.1446 - val_accuracy: 0.9539\n",
            "Epoch 566/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1893 - accuracy: 0.9423\n",
            "Epoch 566: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1878 - accuracy: 0.9396 - val_loss: 0.1012 - val_accuracy: 0.9752\n",
            "Epoch 567/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1571 - accuracy: 0.9361\n",
            "Epoch 567: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1537 - accuracy: 0.9351 - val_loss: 0.1123 - val_accuracy: 0.9681\n",
            "Epoch 568/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1546 - accuracy: 0.9438\n",
            "Epoch 568: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1515 - accuracy: 0.9431 - val_loss: 0.1220 - val_accuracy: 0.9574\n",
            "Epoch 569/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.2109 - accuracy: 0.9294\n",
            "Epoch 569: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1887 - accuracy: 0.9342 - val_loss: 0.1289 - val_accuracy: 0.9645\n",
            "Epoch 570/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1676 - accuracy: 0.9329\n",
            "Epoch 570: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1710 - accuracy: 0.9316 - val_loss: 0.1531 - val_accuracy: 0.9539\n",
            "Epoch 571/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1617 - accuracy: 0.9479\n",
            "Epoch 571: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1580 - accuracy: 0.9476 - val_loss: 0.1323 - val_accuracy: 0.9716\n",
            "Epoch 572/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1256 - accuracy: 0.9621\n",
            "Epoch 572: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1190 - accuracy: 0.9636 - val_loss: 0.1378 - val_accuracy: 0.9681\n",
            "Epoch 573/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1294 - accuracy: 0.9525\n",
            "Epoch 573: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1336 - accuracy: 0.9511 - val_loss: 0.1408 - val_accuracy: 0.9681\n",
            "Epoch 574/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9554\n",
            "Epoch 574: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1153 - accuracy: 0.9556 - val_loss: 0.1396 - val_accuracy: 0.9787\n",
            "Epoch 575/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9536\n",
            "Epoch 575: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1267 - accuracy: 0.9564 - val_loss: 0.1381 - val_accuracy: 0.9787\n",
            "Epoch 576/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1012 - accuracy: 0.9677\n",
            "Epoch 576: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1052 - accuracy: 0.9671 - val_loss: 0.1485 - val_accuracy: 0.9681\n",
            "Epoch 577/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1366 - accuracy: 0.9500\n",
            "Epoch 577: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1381 - accuracy: 0.9502 - val_loss: 0.1671 - val_accuracy: 0.9681\n",
            "Epoch 578/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1151 - accuracy: 0.9619\n",
            "Epoch 578: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1159 - accuracy: 0.9600 - val_loss: 0.1814 - val_accuracy: 0.9716\n",
            "Epoch 579/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1468 - accuracy: 0.9587\n",
            "Epoch 579: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1416 - accuracy: 0.9591 - val_loss: 0.1628 - val_accuracy: 0.9645\n",
            "Epoch 580/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1737 - accuracy: 0.9468\n",
            "Epoch 580: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1732 - accuracy: 0.9422 - val_loss: 0.1166 - val_accuracy: 0.9610\n",
            "Epoch 581/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1371 - accuracy: 0.9459\n",
            "Epoch 581: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.9458 - val_loss: 0.1586 - val_accuracy: 0.9716\n",
            "Epoch 582/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9446\n",
            "Epoch 582: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1417 - accuracy: 0.9440 - val_loss: 0.1234 - val_accuracy: 0.9752\n",
            "Epoch 583/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1176 - accuracy: 0.9506\n",
            "Epoch 583: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1283 - accuracy: 0.9520 - val_loss: 0.1188 - val_accuracy: 0.9645\n",
            "Epoch 584/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9509\n",
            "Epoch 584: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1403 - accuracy: 0.9502 - val_loss: 0.1148 - val_accuracy: 0.9681\n",
            "Epoch 585/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1761 - accuracy: 0.9330\n",
            "Epoch 585: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1602 - accuracy: 0.9378 - val_loss: 0.1402 - val_accuracy: 0.9645\n",
            "Epoch 586/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1398 - accuracy: 0.9469\n",
            "Epoch 586: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1328 - accuracy: 0.9502 - val_loss: 0.1472 - val_accuracy: 0.9645\n",
            "Epoch 587/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1569 - accuracy: 0.9395\n",
            "Epoch 587: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1568 - accuracy: 0.9396 - val_loss: 0.1572 - val_accuracy: 0.9645\n",
            "Epoch 588/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1472 - accuracy: 0.9473\n",
            "Epoch 588: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1497 - accuracy: 0.9476 - val_loss: 0.1266 - val_accuracy: 0.9787\n",
            "Epoch 589/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1558 - accuracy: 0.9411\n",
            "Epoch 589: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.9458 - val_loss: 0.1667 - val_accuracy: 0.9610\n",
            "Epoch 590/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1426 - accuracy: 0.9513\n",
            "Epoch 590: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9529 - val_loss: 0.1748 - val_accuracy: 0.9610\n",
            "Epoch 591/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1113 - accuracy: 0.9565\n",
            "Epoch 591: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 0.9538 - val_loss: 0.1405 - val_accuracy: 0.9610\n",
            "Epoch 592/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1177 - accuracy: 0.9550\n",
            "Epoch 592: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1334 - accuracy: 0.9484 - val_loss: 0.1566 - val_accuracy: 0.9574\n",
            "Epoch 593/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1570 - accuracy: 0.9583\n",
            "Epoch 593: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1545 - accuracy: 0.9547 - val_loss: 0.1605 - val_accuracy: 0.9716\n",
            "Epoch 594/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1271 - accuracy: 0.9531\n",
            "Epoch 594: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1303 - accuracy: 0.9511 - val_loss: 0.1747 - val_accuracy: 0.9716\n",
            "Epoch 595/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1342 - accuracy: 0.9456\n",
            "Epoch 595: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1560 - accuracy: 0.9467 - val_loss: 0.1339 - val_accuracy: 0.9610\n",
            "Epoch 596/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1028 - accuracy: 0.9639\n",
            "Epoch 596: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1167 - accuracy: 0.9573 - val_loss: 0.1703 - val_accuracy: 0.9610\n",
            "Epoch 597/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9430\n",
            "Epoch 597: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1425 - accuracy: 0.9440 - val_loss: 0.1416 - val_accuracy: 0.9645\n",
            "Epoch 598/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1334 - accuracy: 0.9468\n",
            "Epoch 598: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9493 - val_loss: 0.1329 - val_accuracy: 0.9716\n",
            "Epoch 599/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9491\n",
            "Epoch 599: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9493 - val_loss: 0.1617 - val_accuracy: 0.9752\n",
            "Epoch 600/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1296 - accuracy: 0.9525\n",
            "Epoch 600: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1320 - accuracy: 0.9529 - val_loss: 0.1265 - val_accuracy: 0.9787\n",
            "Epoch 601/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1389 - accuracy: 0.9472\n",
            "Epoch 601: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1367 - accuracy: 0.9484 - val_loss: 0.1189 - val_accuracy: 0.9787\n",
            "Epoch 602/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1310 - accuracy: 0.9521\n",
            "Epoch 602: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1226 - accuracy: 0.9547 - val_loss: 0.1146 - val_accuracy: 0.9787\n",
            "Epoch 603/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1133 - accuracy: 0.9639\n",
            "Epoch 603: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9644 - val_loss: 0.1315 - val_accuracy: 0.9752\n",
            "Epoch 604/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9580\n",
            "Epoch 604: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1267 - accuracy: 0.9582 - val_loss: 0.1133 - val_accuracy: 0.9752\n",
            "Epoch 605/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1264 - accuracy: 0.9509\n",
            "Epoch 605: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9502 - val_loss: 0.1155 - val_accuracy: 0.9823\n",
            "Epoch 606/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1431 - accuracy: 0.9450\n",
            "Epoch 606: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1443 - accuracy: 0.9440 - val_loss: 0.1414 - val_accuracy: 0.9574\n",
            "Epoch 607/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1746 - accuracy: 0.9362\n",
            "Epoch 607: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9404 - val_loss: 0.1197 - val_accuracy: 0.9574\n",
            "Epoch 608/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1358 - accuracy: 0.9475\n",
            "Epoch 608: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9431 - val_loss: 0.1326 - val_accuracy: 0.9681\n",
            "Epoch 609/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1804 - accuracy: 0.9399\n",
            "Epoch 609: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1573 - accuracy: 0.9476 - val_loss: 0.1262 - val_accuracy: 0.9574\n",
            "Epoch 610/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1093 - accuracy: 0.9565\n",
            "Epoch 610: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9538 - val_loss: 0.1252 - val_accuracy: 0.9716\n",
            "Epoch 611/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1094 - accuracy: 0.9596\n",
            "Epoch 611: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1072 - accuracy: 0.9600 - val_loss: 0.1040 - val_accuracy: 0.9645\n",
            "Epoch 612/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1644 - accuracy: 0.9483\n",
            "Epoch 612: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9556 - val_loss: 0.1091 - val_accuracy: 0.9504\n",
            "Epoch 613/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1433 - accuracy: 0.9552\n",
            "Epoch 613: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9511 - val_loss: 0.1106 - val_accuracy: 0.9574\n",
            "Epoch 614/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1477 - accuracy: 0.9491\n",
            "Epoch 614: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1481 - accuracy: 0.9476 - val_loss: 0.1535 - val_accuracy: 0.9574\n",
            "Epoch 615/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1868 - accuracy: 0.9438\n",
            "Epoch 615: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 0.9476 - val_loss: 0.1045 - val_accuracy: 0.9681\n",
            "Epoch 616/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1464 - accuracy: 0.9433\n",
            "Epoch 616: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1465 - accuracy: 0.9449 - val_loss: 0.0943 - val_accuracy: 0.9645\n",
            "Epoch 617/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1650 - accuracy: 0.9329\n",
            "Epoch 617: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1554 - accuracy: 0.9396 - val_loss: 0.1003 - val_accuracy: 0.9681\n",
            "Epoch 618/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9404\n",
            "Epoch 618: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9404 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
            "Epoch 619/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1647 - accuracy: 0.9423\n",
            "Epoch 619: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9396 - val_loss: 0.1392 - val_accuracy: 0.9752\n",
            "Epoch 620/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1581 - accuracy: 0.9482\n",
            "Epoch 620: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1579 - accuracy: 0.9484 - val_loss: 0.1407 - val_accuracy: 0.9645\n",
            "Epoch 621/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9458\n",
            "Epoch 621: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1768 - accuracy: 0.9440 - val_loss: 0.1423 - val_accuracy: 0.9539\n",
            "Epoch 622/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1609 - accuracy: 0.9408\n",
            "Epoch 622: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9413 - val_loss: 0.1394 - val_accuracy: 0.9681\n",
            "Epoch 623/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1584 - accuracy: 0.9413\n",
            "Epoch 623: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2008 - accuracy: 0.9289 - val_loss: 0.1461 - val_accuracy: 0.9539\n",
            "Epoch 624/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1569 - accuracy: 0.9363\n",
            "Epoch 624: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1542 - accuracy: 0.9387 - val_loss: 0.1323 - val_accuracy: 0.9681\n",
            "Epoch 625/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1729 - accuracy: 0.9446\n",
            "Epoch 625: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1727 - accuracy: 0.9449 - val_loss: 0.1358 - val_accuracy: 0.9574\n",
            "Epoch 626/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1144 - accuracy: 0.9537\n",
            "Epoch 626: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9511 - val_loss: 0.1141 - val_accuracy: 0.9716\n",
            "Epoch 627/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1062 - accuracy: 0.9550\n",
            "Epoch 627: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1270 - accuracy: 0.9556 - val_loss: 0.1371 - val_accuracy: 0.9716\n",
            "Epoch 628/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9500\n",
            "Epoch 628: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1660 - accuracy: 0.9502 - val_loss: 0.1388 - val_accuracy: 0.9752\n",
            "Epoch 629/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9449\n",
            "Epoch 629: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9449 - val_loss: 0.1344 - val_accuracy: 0.9610\n",
            "Epoch 630/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9458\n",
            "Epoch 630: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1523 - accuracy: 0.9458 - val_loss: 0.1201 - val_accuracy: 0.9610\n",
            "Epoch 631/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1215 - accuracy: 0.9555\n",
            "Epoch 631: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1477 - accuracy: 0.9467 - val_loss: 0.1263 - val_accuracy: 0.9539\n",
            "Epoch 632/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1941 - accuracy: 0.9353\n",
            "Epoch 632: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1856 - accuracy: 0.9387 - val_loss: 0.1155 - val_accuracy: 0.9681\n",
            "Epoch 633/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1741 - accuracy: 0.9439\n",
            "Epoch 633: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9431 - val_loss: 0.1190 - val_accuracy: 0.9610\n",
            "Epoch 634/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1951 - accuracy: 0.9385\n",
            "Epoch 634: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1816 - accuracy: 0.9422 - val_loss: 0.1393 - val_accuracy: 0.9468\n",
            "Epoch 635/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1759 - accuracy: 0.9375\n",
            "Epoch 635: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1765 - accuracy: 0.9360 - val_loss: 0.1253 - val_accuracy: 0.9681\n",
            "Epoch 636/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9486\n",
            "Epoch 636: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1407 - accuracy: 0.9467 - val_loss: 0.1223 - val_accuracy: 0.9610\n",
            "Epoch 637/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1307 - accuracy: 0.9609\n",
            "Epoch 637: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1307 - accuracy: 0.9609 - val_loss: 0.1348 - val_accuracy: 0.9610\n",
            "Epoch 638/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9563\n",
            "Epoch 638: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1057 - accuracy: 0.9564 - val_loss: 0.1349 - val_accuracy: 0.9645\n",
            "Epoch 639/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1226 - accuracy: 0.9580\n",
            "Epoch 639: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9556 - val_loss: 0.1270 - val_accuracy: 0.9752\n",
            "Epoch 640/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1282 - accuracy: 0.9526\n",
            "Epoch 640: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9529 - val_loss: 0.1449 - val_accuracy: 0.9681\n",
            "Epoch 641/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9467\n",
            "Epoch 641: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1433 - accuracy: 0.9467 - val_loss: 0.0973 - val_accuracy: 0.9645\n",
            "Epoch 642/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1538 - accuracy: 0.9519\n",
            "Epoch 642: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1345 - accuracy: 0.9573 - val_loss: 0.1515 - val_accuracy: 0.9504\n",
            "Epoch 643/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.9464\n",
            "Epoch 643: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1728 - accuracy: 0.9467 - val_loss: 0.1576 - val_accuracy: 0.9468\n",
            "Epoch 644/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1493 - accuracy: 0.9400\n",
            "Epoch 644: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1741 - accuracy: 0.9378 - val_loss: 0.1441 - val_accuracy: 0.9645\n",
            "Epoch 645/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1476 - accuracy: 0.9506\n",
            "Epoch 645: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1526 - accuracy: 0.9484 - val_loss: 0.1313 - val_accuracy: 0.9716\n",
            "Epoch 646/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1507 - accuracy: 0.9408\n",
            "Epoch 646: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1430 - accuracy: 0.9422 - val_loss: 0.1290 - val_accuracy: 0.9681\n",
            "Epoch 647/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1738 - accuracy: 0.9467\n",
            "Epoch 647: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1736 - accuracy: 0.9458 - val_loss: 0.1551 - val_accuracy: 0.9645\n",
            "Epoch 648/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9513\n",
            "Epoch 648: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9502 - val_loss: 0.1452 - val_accuracy: 0.9610\n",
            "Epoch 649/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9555\n",
            "Epoch 649: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1044 - accuracy: 0.9556 - val_loss: 0.1220 - val_accuracy: 0.9752\n",
            "Epoch 650/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9488\n",
            "Epoch 650: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1287 - accuracy: 0.9502 - val_loss: 0.1505 - val_accuracy: 0.9752\n",
            "Epoch 651/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9531\n",
            "Epoch 651: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1182 - accuracy: 0.9547 - val_loss: 0.1459 - val_accuracy: 0.9681\n",
            "Epoch 652/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1447 - accuracy: 0.9508\n",
            "Epoch 652: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1513 - accuracy: 0.9484 - val_loss: 0.1070 - val_accuracy: 0.9716\n",
            "Epoch 653/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9449\n",
            "Epoch 653: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1500 - accuracy: 0.9449 - val_loss: 0.1112 - val_accuracy: 0.9752\n",
            "Epoch 654/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1515 - accuracy: 0.9442\n",
            "Epoch 654: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9493 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
            "Epoch 655/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1520 - accuracy: 0.9558\n",
            "Epoch 655: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1353 - accuracy: 0.9591 - val_loss: 0.1076 - val_accuracy: 0.9752\n",
            "Epoch 656/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1362 - accuracy: 0.9552\n",
            "Epoch 656: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1448 - accuracy: 0.9502 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
            "Epoch 657/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1219 - accuracy: 0.9444\n",
            "Epoch 657: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1309 - accuracy: 0.9440 - val_loss: 0.1284 - val_accuracy: 0.9716\n",
            "Epoch 658/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1466 - accuracy: 0.9476\n",
            "Epoch 658: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1458 - accuracy: 0.9476 - val_loss: 0.1258 - val_accuracy: 0.9752\n",
            "Epoch 659/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1322 - accuracy: 0.9507\n",
            "Epoch 659: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1347 - accuracy: 0.9493 - val_loss: 0.1183 - val_accuracy: 0.9716\n",
            "Epoch 660/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1157 - accuracy: 0.9586\n",
            "Epoch 660: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1156 - accuracy: 0.9591 - val_loss: 0.1329 - val_accuracy: 0.9645\n",
            "Epoch 661/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1518 - accuracy: 0.9447\n",
            "Epoch 661: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9449 - val_loss: 0.1220 - val_accuracy: 0.9681\n",
            "Epoch 662/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1117 - accuracy: 0.9502\n",
            "Epoch 662: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9502 - val_loss: 0.1077 - val_accuracy: 0.9752\n",
            "Epoch 663/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1265 - accuracy: 0.9554\n",
            "Epoch 663: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1293 - accuracy: 0.9547 - val_loss: 0.1428 - val_accuracy: 0.9681\n",
            "Epoch 664/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1713 - accuracy: 0.9388\n",
            "Epoch 664: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.9431 - val_loss: 0.1573 - val_accuracy: 0.9681\n",
            "Epoch 665/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9402\n",
            "Epoch 665: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1458 - accuracy: 0.9396 - val_loss: 0.1671 - val_accuracy: 0.9645\n",
            "Epoch 666/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1990 - accuracy: 0.9500\n",
            "Epoch 666: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1828 - accuracy: 0.9511 - val_loss: 0.1429 - val_accuracy: 0.9752\n",
            "Epoch 667/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1395 - accuracy: 0.9558\n",
            "Epoch 667: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1353 - accuracy: 0.9573 - val_loss: 0.1702 - val_accuracy: 0.9681\n",
            "Epoch 668/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.9464\n",
            "Epoch 668: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1606 - accuracy: 0.9467 - val_loss: 0.1080 - val_accuracy: 0.9716\n",
            "Epoch 669/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9600\n",
            "Epoch 669: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9600 - val_loss: 0.0994 - val_accuracy: 0.9752\n",
            "Epoch 670/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1223 - accuracy: 0.9487\n",
            "Epoch 670: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9431 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
            "Epoch 671/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1009 - accuracy: 0.9615\n",
            "Epoch 671: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9591 - val_loss: 0.1340 - val_accuracy: 0.9610\n",
            "Epoch 672/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1517 - accuracy: 0.9509\n",
            "Epoch 672: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1573 - accuracy: 0.9502 - val_loss: 0.1206 - val_accuracy: 0.9716\n",
            "Epoch 673/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1419 - accuracy: 0.9487\n",
            "Epoch 673: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9502 - val_loss: 0.0954 - val_accuracy: 0.9823\n",
            "Epoch 674/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1199 - accuracy: 0.9519\n",
            "Epoch 674: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9440 - val_loss: 0.1016 - val_accuracy: 0.9610\n",
            "Epoch 675/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1245 - accuracy: 0.9567\n",
            "Epoch 675: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9520 - val_loss: 0.1152 - val_accuracy: 0.9752\n",
            "Epoch 676/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1303 - accuracy: 0.9525\n",
            "Epoch 676: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9529 - val_loss: 0.1375 - val_accuracy: 0.9681\n",
            "Epoch 677/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1147 - accuracy: 0.9509\n",
            "Epoch 677: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1105 - accuracy: 0.9538 - val_loss: 0.1133 - val_accuracy: 0.9752\n",
            "Epoch 678/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1086 - accuracy: 0.9591\n",
            "Epoch 678: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9582 - val_loss: 0.1447 - val_accuracy: 0.9610\n",
            "Epoch 679/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1329 - accuracy: 0.9536\n",
            "Epoch 679: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1324 - accuracy: 0.9538 - val_loss: 0.1379 - val_accuracy: 0.9645\n",
            "Epoch 680/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9504\n",
            "Epoch 680: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1202 - accuracy: 0.9493 - val_loss: 0.1475 - val_accuracy: 0.9645\n",
            "Epoch 681/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1421 - accuracy: 0.9612\n",
            "Epoch 681: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1388 - accuracy: 0.9618 - val_loss: 0.1333 - val_accuracy: 0.9716\n",
            "Epoch 682/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1233 - accuracy: 0.9555\n",
            "Epoch 682: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9502 - val_loss: 0.1218 - val_accuracy: 0.9716\n",
            "Epoch 683/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1119 - accuracy: 0.9560\n",
            "Epoch 683: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1108 - accuracy: 0.9556 - val_loss: 0.1083 - val_accuracy: 0.9645\n",
            "Epoch 684/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1420 - accuracy: 0.9531\n",
            "Epoch 684: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 0.9520 - val_loss: 0.1589 - val_accuracy: 0.9574\n",
            "Epoch 685/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1317 - accuracy: 0.9630\n",
            "Epoch 685: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9644 - val_loss: 0.1159 - val_accuracy: 0.9645\n",
            "Epoch 686/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1126 - accuracy: 0.9531\n",
            "Epoch 686: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1190 - accuracy: 0.9511 - val_loss: 0.1249 - val_accuracy: 0.9716\n",
            "Epoch 687/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1600 - accuracy: 0.9537\n",
            "Epoch 687: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9529 - val_loss: 0.1468 - val_accuracy: 0.9539\n",
            "Epoch 688/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1444 - accuracy: 0.9435\n",
            "Epoch 688: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1401 - accuracy: 0.9458 - val_loss: 0.1613 - val_accuracy: 0.9504\n",
            "Epoch 689/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1345 - accuracy: 0.9431\n",
            "Epoch 689: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1386 - accuracy: 0.9422 - val_loss: 0.1539 - val_accuracy: 0.9504\n",
            "Epoch 690/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1266 - accuracy: 0.9504\n",
            "Epoch 690: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9538 - val_loss: 0.1378 - val_accuracy: 0.9610\n",
            "Epoch 691/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1480 - accuracy: 0.9456\n",
            "Epoch 691: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9476 - val_loss: 0.1298 - val_accuracy: 0.9645\n",
            "Epoch 692/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1164 - accuracy: 0.9547\n",
            "Epoch 692: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9520 - val_loss: 0.1167 - val_accuracy: 0.9716\n",
            "Epoch 693/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1077 - accuracy: 0.9603\n",
            "Epoch 693: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9609 - val_loss: 0.1337 - val_accuracy: 0.9681\n",
            "Epoch 694/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.0931 - accuracy: 0.9648\n",
            "Epoch 694: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.0950 - accuracy: 0.9636 - val_loss: 0.1392 - val_accuracy: 0.9752\n",
            "Epoch 695/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1393 - accuracy: 0.9602\n",
            "Epoch 695: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1371 - accuracy: 0.9600 - val_loss: 0.1238 - val_accuracy: 0.9681\n",
            "Epoch 696/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1396 - accuracy: 0.9438\n",
            "Epoch 696: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9458 - val_loss: 0.1197 - val_accuracy: 0.9645\n",
            "Epoch 697/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1494 - accuracy: 0.9507\n",
            "Epoch 697: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1498 - accuracy: 0.9493 - val_loss: 0.1323 - val_accuracy: 0.9681\n",
            "Epoch 698/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1527 - accuracy: 0.9453\n",
            "Epoch 698: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1497 - accuracy: 0.9440 - val_loss: 0.1329 - val_accuracy: 0.9610\n",
            "Epoch 699/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1839 - accuracy: 0.9418\n",
            "Epoch 699: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1635 - accuracy: 0.9476 - val_loss: 0.1418 - val_accuracy: 0.9716\n",
            "Epoch 700/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1557 - accuracy: 0.9522\n",
            "Epoch 700: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1563 - accuracy: 0.9511 - val_loss: 0.1145 - val_accuracy: 0.9716\n",
            "Epoch 701/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.0993 - accuracy: 0.9667\n",
            "Epoch 701: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1093 - accuracy: 0.9636 - val_loss: 0.1242 - val_accuracy: 0.9645\n",
            "Epoch 702/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1012 - accuracy: 0.9595\n",
            "Epoch 702: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1107 - accuracy: 0.9573 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
            "Epoch 703/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1084 - accuracy: 0.9629\n",
            "Epoch 703: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1108 - accuracy: 0.9618 - val_loss: 0.1200 - val_accuracy: 0.9645\n",
            "Epoch 704/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1182 - accuracy: 0.9550\n",
            "Epoch 704: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1193 - accuracy: 0.9520 - val_loss: 0.1181 - val_accuracy: 0.9645\n",
            "Epoch 705/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1320 - accuracy: 0.9525\n",
            "Epoch 705: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9547 - val_loss: 0.1313 - val_accuracy: 0.9574\n",
            "Epoch 706/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1386 - accuracy: 0.9470\n",
            "Epoch 706: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1358 - accuracy: 0.9484 - val_loss: 0.1150 - val_accuracy: 0.9716\n",
            "Epoch 707/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1172 - accuracy: 0.9561\n",
            "Epoch 707: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1167 - accuracy: 0.9582 - val_loss: 0.0974 - val_accuracy: 0.9716\n",
            "Epoch 708/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.0976 - accuracy: 0.9543\n",
            "Epoch 708: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1015 - accuracy: 0.9547 - val_loss: 0.1134 - val_accuracy: 0.9681\n",
            "Epoch 709/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.0962 - accuracy: 0.9647\n",
            "Epoch 709: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.0919 - accuracy: 0.9662 - val_loss: 0.1170 - val_accuracy: 0.9716\n",
            "Epoch 710/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1071 - accuracy: 0.9516\n",
            "Epoch 710: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1080 - accuracy: 0.9511 - val_loss: 0.1042 - val_accuracy: 0.9681\n",
            "Epoch 711/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9509\n",
            "Epoch 711: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1234 - accuracy: 0.9502 - val_loss: 0.1195 - val_accuracy: 0.9787\n",
            "Epoch 712/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9586\n",
            "Epoch 712: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1192 - accuracy: 0.9573 - val_loss: 0.1250 - val_accuracy: 0.9716\n",
            "Epoch 713/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9483\n",
            "Epoch 713: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1593 - accuracy: 0.9440 - val_loss: 0.1609 - val_accuracy: 0.9610\n",
            "Epoch 714/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9518\n",
            "Epoch 714: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1325 - accuracy: 0.9520 - val_loss: 0.1438 - val_accuracy: 0.9610\n",
            "Epoch 715/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1289 - accuracy: 0.9570\n",
            "Epoch 715: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1369 - accuracy: 0.9529 - val_loss: 0.1305 - val_accuracy: 0.9681\n",
            "Epoch 716/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1178 - accuracy: 0.9575\n",
            "Epoch 716: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1158 - accuracy: 0.9582 - val_loss: 0.1517 - val_accuracy: 0.9681\n",
            "Epoch 717/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1195 - accuracy: 0.9615\n",
            "Epoch 717: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1233 - accuracy: 0.9591 - val_loss: 0.1485 - val_accuracy: 0.9645\n",
            "Epoch 718/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9567\n",
            "Epoch 718: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9556 - val_loss: 0.1678 - val_accuracy: 0.9681\n",
            "Epoch 719/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1235 - accuracy: 0.9526\n",
            "Epoch 719: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1186 - accuracy: 0.9538 - val_loss: 0.1344 - val_accuracy: 0.9645\n",
            "Epoch 720/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9502\n",
            "Epoch 720: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1398 - accuracy: 0.9520 - val_loss: 0.1479 - val_accuracy: 0.9574\n",
            "Epoch 721/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1710 - accuracy: 0.9410\n",
            "Epoch 721: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1471 - accuracy: 0.9484 - val_loss: 0.1190 - val_accuracy: 0.9610\n",
            "Epoch 722/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.0957 - accuracy: 0.9625\n",
            "Epoch 722: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9511 - val_loss: 0.1279 - val_accuracy: 0.9610\n",
            "Epoch 723/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.9502\n",
            "Epoch 723: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9502 - val_loss: 0.1051 - val_accuracy: 0.9645\n",
            "Epoch 724/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1425 - accuracy: 0.9547\n",
            "Epoch 724: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1476 - accuracy: 0.9538 - val_loss: 0.1534 - val_accuracy: 0.9610\n",
            "Epoch 725/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1440 - accuracy: 0.9459\n",
            "Epoch 725: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1381 - accuracy: 0.9476 - val_loss: 0.1403 - val_accuracy: 0.9610\n",
            "Epoch 726/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1355 - accuracy: 0.9479\n",
            "Epoch 726: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1368 - accuracy: 0.9476 - val_loss: 0.0927 - val_accuracy: 0.9716\n",
            "Epoch 727/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1232 - accuracy: 0.9471\n",
            "Epoch 727: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1266 - accuracy: 0.9449 - val_loss: 0.1026 - val_accuracy: 0.9645\n",
            "Epoch 728/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1217 - accuracy: 0.9506\n",
            "Epoch 728: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1222 - accuracy: 0.9493 - val_loss: 0.1048 - val_accuracy: 0.9716\n",
            "Epoch 729/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1456 - accuracy: 0.9464\n",
            "Epoch 729: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1446 - accuracy: 0.9458 - val_loss: 0.1166 - val_accuracy: 0.9610\n",
            "Epoch 730/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1208 - accuracy: 0.9507\n",
            "Epoch 730: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1228 - accuracy: 0.9529 - val_loss: 0.1426 - val_accuracy: 0.9681\n",
            "Epoch 731/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1339 - accuracy: 0.9471\n",
            "Epoch 731: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9493 - val_loss: 0.1907 - val_accuracy: 0.9681\n",
            "Epoch 732/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1464 - accuracy: 0.9536\n",
            "Epoch 732: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 1s 19ms/step - loss: 0.1672 - accuracy: 0.9502 - val_loss: 0.2305 - val_accuracy: 0.9504\n",
            "Epoch 733/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1661 - accuracy: 0.9336\n",
            "Epoch 733: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1617 - accuracy: 0.9378 - val_loss: 0.1445 - val_accuracy: 0.9716\n",
            "Epoch 734/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1582 - accuracy: 0.9438\n",
            "Epoch 734: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1402 - accuracy: 0.9511 - val_loss: 0.1445 - val_accuracy: 0.9610\n",
            "Epoch 735/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1697 - accuracy: 0.9427\n",
            "Epoch 735: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1676 - accuracy: 0.9422 - val_loss: 0.1562 - val_accuracy: 0.9645\n",
            "Epoch 736/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9440\n",
            "Epoch 736: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9440 - val_loss: 0.1274 - val_accuracy: 0.9681\n",
            "Epoch 737/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1192 - accuracy: 0.9507\n",
            "Epoch 737: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1226 - accuracy: 0.9511 - val_loss: 0.1584 - val_accuracy: 0.9752\n",
            "Epoch 738/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9484\n",
            "Epoch 738: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1604 - accuracy: 0.9484 - val_loss: 0.1218 - val_accuracy: 0.9610\n",
            "Epoch 739/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9538\n",
            "Epoch 739: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1136 - accuracy: 0.9538 - val_loss: 0.1115 - val_accuracy: 0.9645\n",
            "Epoch 740/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1668 - accuracy: 0.9438\n",
            "Epoch 740: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1452 - accuracy: 0.9476 - val_loss: 0.1527 - val_accuracy: 0.9610\n",
            "Epoch 741/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1441 - accuracy: 0.9425\n",
            "Epoch 741: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1386 - accuracy: 0.9484 - val_loss: 0.1930 - val_accuracy: 0.9645\n",
            "Epoch 742/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9432\n",
            "Epoch 742: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1321 - accuracy: 0.9440 - val_loss: 0.1694 - val_accuracy: 0.9716\n",
            "Epoch 743/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1214 - accuracy: 0.9591\n",
            "Epoch 743: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1213 - accuracy: 0.9582 - val_loss: 0.1269 - val_accuracy: 0.9610\n",
            "Epoch 744/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1228 - accuracy: 0.9375\n",
            "Epoch 744: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9404 - val_loss: 0.1195 - val_accuracy: 0.9681\n",
            "Epoch 745/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1150 - accuracy: 0.9612\n",
            "Epoch 745: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9591 - val_loss: 0.1385 - val_accuracy: 0.9610\n",
            "Epoch 746/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1848 - accuracy: 0.9479\n",
            "Epoch 746: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1811 - accuracy: 0.9484 - val_loss: 0.2386 - val_accuracy: 0.9539\n",
            "Epoch 747/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9482\n",
            "Epoch 747: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1691 - accuracy: 0.9476 - val_loss: 0.1271 - val_accuracy: 0.9716\n",
            "Epoch 748/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1400 - accuracy: 0.9506\n",
            "Epoch 748: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1383 - accuracy: 0.9511 - val_loss: 0.1409 - val_accuracy: 0.9681\n",
            "Epoch 749/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1011 - accuracy: 0.9567\n",
            "Epoch 749: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1124 - accuracy: 0.9564 - val_loss: 0.1561 - val_accuracy: 0.9610\n",
            "Epoch 750/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1573 - accuracy: 0.9431\n",
            "Epoch 750: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1438 - accuracy: 0.9467 - val_loss: 0.1366 - val_accuracy: 0.9752\n",
            "Epoch 751/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1438 - accuracy: 0.9483\n",
            "Epoch 751: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9520 - val_loss: 0.1438 - val_accuracy: 0.9716\n",
            "Epoch 752/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1299 - accuracy: 0.9502\n",
            "Epoch 752: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9493 - val_loss: 0.1446 - val_accuracy: 0.9681\n",
            "Epoch 753/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.0989 - accuracy: 0.9655\n",
            "Epoch 753: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1077 - accuracy: 0.9644 - val_loss: 0.1178 - val_accuracy: 0.9716\n",
            "Epoch 754/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9518\n",
            "Epoch 754: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1430 - accuracy: 0.9520 - val_loss: 0.1649 - val_accuracy: 0.9716\n",
            "Epoch 755/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.2394 - accuracy: 0.9271\n",
            "Epoch 755: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.2058 - accuracy: 0.9324 - val_loss: 0.1523 - val_accuracy: 0.9610\n",
            "Epoch 756/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9502\n",
            "Epoch 756: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1409 - accuracy: 0.9502 - val_loss: 0.1481 - val_accuracy: 0.9645\n",
            "Epoch 757/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1091 - accuracy: 0.9596\n",
            "Epoch 757: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1116 - accuracy: 0.9582 - val_loss: 0.1519 - val_accuracy: 0.9645\n",
            "Epoch 758/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9422\n",
            "Epoch 758: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1293 - accuracy: 0.9422 - val_loss: 0.1531 - val_accuracy: 0.9645\n",
            "Epoch 759/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.0862 - accuracy: 0.9675\n",
            "Epoch 759: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0976 - accuracy: 0.9627 - val_loss: 0.1677 - val_accuracy: 0.9752\n",
            "Epoch 760/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.0952 - accuracy: 0.9627\n",
            "Epoch 760: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1045 - accuracy: 0.9582 - val_loss: 0.1661 - val_accuracy: 0.9681\n",
            "Epoch 761/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9587\n",
            "Epoch 761: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1466 - accuracy: 0.9511 - val_loss: 0.1605 - val_accuracy: 0.9716\n",
            "Epoch 762/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.0899 - accuracy: 0.9662\n",
            "Epoch 762: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0899 - accuracy: 0.9662 - val_loss: 0.1689 - val_accuracy: 0.9681\n",
            "Epoch 763/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1131 - accuracy: 0.9631\n",
            "Epoch 763: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1259 - accuracy: 0.9609 - val_loss: 0.1644 - val_accuracy: 0.9752\n",
            "Epoch 764/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1398 - accuracy: 0.9449\n",
            "Epoch 764: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1402 - accuracy: 0.9440 - val_loss: 0.1749 - val_accuracy: 0.9681\n",
            "Epoch 765/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9545\n",
            "Epoch 765: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1520 - accuracy: 0.9547 - val_loss: 0.1404 - val_accuracy: 0.9610\n",
            "Epoch 766/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9555\n",
            "Epoch 766: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1118 - accuracy: 0.9564 - val_loss: 0.1336 - val_accuracy: 0.9716\n",
            "Epoch 767/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1036 - accuracy: 0.9577\n",
            "Epoch 767: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0983 - accuracy: 0.9600 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
            "Epoch 768/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1187 - accuracy: 0.9551\n",
            "Epoch 768: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1169 - accuracy: 0.9564 - val_loss: 0.1784 - val_accuracy: 0.9610\n",
            "Epoch 769/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1203 - accuracy: 0.9526\n",
            "Epoch 769: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1079 - accuracy: 0.9582 - val_loss: 0.1724 - val_accuracy: 0.9716\n",
            "Epoch 770/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9577\n",
            "Epoch 770: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1235 - accuracy: 0.9564 - val_loss: 0.1446 - val_accuracy: 0.9681\n",
            "Epoch 771/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1098 - accuracy: 0.9583\n",
            "Epoch 771: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1117 - accuracy: 0.9582 - val_loss: 0.1697 - val_accuracy: 0.9752\n",
            "Epoch 772/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9600\n",
            "Epoch 772: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1265 - accuracy: 0.9573 - val_loss: 0.1878 - val_accuracy: 0.9574\n",
            "Epoch 773/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1399 - accuracy: 0.9540\n",
            "Epoch 773: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1430 - accuracy: 0.9529 - val_loss: 0.1661 - val_accuracy: 0.9539\n",
            "Epoch 774/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9520\n",
            "Epoch 774: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1277 - accuracy: 0.9520 - val_loss: 0.1682 - val_accuracy: 0.9574\n",
            "Epoch 775/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9596\n",
            "Epoch 775: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1178 - accuracy: 0.9609 - val_loss: 0.1717 - val_accuracy: 0.9610\n",
            "Epoch 776/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1147 - accuracy: 0.9601\n",
            "Epoch 776: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1122 - accuracy: 0.9582 - val_loss: 0.1972 - val_accuracy: 0.9574\n",
            "Epoch 777/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1781 - accuracy: 0.9449\n",
            "Epoch 777: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1760 - accuracy: 0.9449 - val_loss: 0.1320 - val_accuracy: 0.9645\n",
            "Epoch 778/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9489\n",
            "Epoch 778: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1262 - accuracy: 0.9484 - val_loss: 0.1227 - val_accuracy: 0.9716\n",
            "Epoch 779/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1162 - accuracy: 0.9565\n",
            "Epoch 779: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1345 - accuracy: 0.9502 - val_loss: 0.1264 - val_accuracy: 0.9681\n",
            "Epoch 780/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1251 - accuracy: 0.9519\n",
            "Epoch 780: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1288 - accuracy: 0.9529 - val_loss: 0.1491 - val_accuracy: 0.9574\n",
            "Epoch 781/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9550\n",
            "Epoch 781: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9547 - val_loss: 0.1636 - val_accuracy: 0.9539\n",
            "Epoch 782/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1442 - accuracy: 0.9575\n",
            "Epoch 782: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1284 - accuracy: 0.9573 - val_loss: 0.1535 - val_accuracy: 0.9610\n",
            "Epoch 783/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1148 - accuracy: 0.9440\n",
            "Epoch 783: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1231 - accuracy: 0.9458 - val_loss: 0.1543 - val_accuracy: 0.9645\n",
            "Epoch 784/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1227 - accuracy: 0.9567\n",
            "Epoch 784: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9582 - val_loss: 0.1437 - val_accuracy: 0.9610\n",
            "Epoch 785/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1021 - accuracy: 0.9588\n",
            "Epoch 785: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1060 - accuracy: 0.9564 - val_loss: 0.1121 - val_accuracy: 0.9716\n",
            "Epoch 786/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1248 - accuracy: 0.9549\n",
            "Epoch 786: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1272 - accuracy: 0.9564 - val_loss: 0.1499 - val_accuracy: 0.9574\n",
            "Epoch 787/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9509\n",
            "Epoch 787: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9511 - val_loss: 0.1514 - val_accuracy: 0.9681\n",
            "Epoch 788/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9427\n",
            "Epoch 788: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1921 - accuracy: 0.9449 - val_loss: 0.1035 - val_accuracy: 0.9716\n",
            "Epoch 789/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1582 - accuracy: 0.9443\n",
            "Epoch 789: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1709 - accuracy: 0.9440 - val_loss: 0.1019 - val_accuracy: 0.9752\n",
            "Epoch 790/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9448\n",
            "Epoch 790: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1463 - accuracy: 0.9467 - val_loss: 0.1046 - val_accuracy: 0.9610\n",
            "Epoch 791/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1209 - accuracy: 0.9525\n",
            "Epoch 791: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1125 - accuracy: 0.9556 - val_loss: 0.1038 - val_accuracy: 0.9787\n",
            "Epoch 792/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1163 - accuracy: 0.9538\n",
            "Epoch 792: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1328 - accuracy: 0.9511 - val_loss: 0.1510 - val_accuracy: 0.9752\n",
            "Epoch 793/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1418 - accuracy: 0.9531\n",
            "Epoch 793: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1400 - accuracy: 0.9520 - val_loss: 0.1496 - val_accuracy: 0.9610\n",
            "Epoch 794/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1258 - accuracy: 0.9597\n",
            "Epoch 794: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1213 - accuracy: 0.9591 - val_loss: 0.1217 - val_accuracy: 0.9574\n",
            "Epoch 795/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1725 - accuracy: 0.9413\n",
            "Epoch 795: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1651 - accuracy: 0.9440 - val_loss: 0.1274 - val_accuracy: 0.9610\n",
            "Epoch 796/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1351 - accuracy: 0.9483\n",
            "Epoch 796: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1348 - accuracy: 0.9511 - val_loss: 0.1233 - val_accuracy: 0.9716\n",
            "Epoch 797/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1269 - accuracy: 0.9491\n",
            "Epoch 797: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9493 - val_loss: 0.0987 - val_accuracy: 0.9752\n",
            "Epoch 798/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1590 - accuracy: 0.9500\n",
            "Epoch 798: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1483 - accuracy: 0.9493 - val_loss: 0.1178 - val_accuracy: 0.9681\n",
            "Epoch 799/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1520 - accuracy: 0.9491\n",
            "Epoch 799: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1467 - accuracy: 0.9502 - val_loss: 0.1172 - val_accuracy: 0.9645\n",
            "Epoch 800/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.0986 - accuracy: 0.9675\n",
            "Epoch 800: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9627 - val_loss: 0.1304 - val_accuracy: 0.9716\n",
            "Epoch 801/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1090 - accuracy: 0.9638\n",
            "Epoch 801: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.0995 - accuracy: 0.9636 - val_loss: 0.1012 - val_accuracy: 0.9752\n",
            "Epoch 802/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9509\n",
            "Epoch 802: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1299 - accuracy: 0.9511 - val_loss: 0.1242 - val_accuracy: 0.9716\n",
            "Epoch 803/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1348 - accuracy: 0.9487\n",
            "Epoch 803: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9520 - val_loss: 0.1090 - val_accuracy: 0.9752\n",
            "Epoch 804/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1244 - accuracy: 0.9567\n",
            "Epoch 804: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9547 - val_loss: 0.1108 - val_accuracy: 0.9681\n",
            "Epoch 805/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.0956 - accuracy: 0.9630\n",
            "Epoch 805: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1060 - accuracy: 0.9591 - val_loss: 0.1008 - val_accuracy: 0.9716\n",
            "Epoch 806/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1164 - accuracy: 0.9543\n",
            "Epoch 806: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9582 - val_loss: 0.1182 - val_accuracy: 0.9681\n",
            "Epoch 807/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1260 - accuracy: 0.9552\n",
            "Epoch 807: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1303 - accuracy: 0.9529 - val_loss: 0.1527 - val_accuracy: 0.9716\n",
            "Epoch 808/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1355 - accuracy: 0.9563\n",
            "Epoch 808: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9591 - val_loss: 0.1524 - val_accuracy: 0.9645\n",
            "Epoch 809/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1400 - accuracy: 0.9531\n",
            "Epoch 809: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9556 - val_loss: 0.1649 - val_accuracy: 0.9574\n",
            "Epoch 810/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1486 - accuracy: 0.9475\n",
            "Epoch 810: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1445 - accuracy: 0.9458 - val_loss: 0.1496 - val_accuracy: 0.9645\n",
            "Epoch 811/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1100 - accuracy: 0.9504\n",
            "Epoch 811: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1101 - accuracy: 0.9529 - val_loss: 0.1335 - val_accuracy: 0.9645\n",
            "Epoch 812/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1273 - accuracy: 0.9560\n",
            "Epoch 812: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.9529 - val_loss: 0.1277 - val_accuracy: 0.9752\n",
            "Epoch 813/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1247 - accuracy: 0.9570\n",
            "Epoch 813: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1237 - accuracy: 0.9573 - val_loss: 0.1328 - val_accuracy: 0.9681\n",
            "Epoch 814/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1259 - accuracy: 0.9500\n",
            "Epoch 814: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9502 - val_loss: 0.1192 - val_accuracy: 0.9681\n",
            "Epoch 815/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1087 - accuracy: 0.9574\n",
            "Epoch 815: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1084 - accuracy: 0.9582 - val_loss: 0.1451 - val_accuracy: 0.9610\n",
            "Epoch 816/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1474 - accuracy: 0.9498\n",
            "Epoch 816: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1360 - accuracy: 0.9520 - val_loss: 0.1467 - val_accuracy: 0.9681\n",
            "Epoch 817/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1107 - accuracy: 0.9550\n",
            "Epoch 817: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9573 - val_loss: 0.1437 - val_accuracy: 0.9645\n",
            "Epoch 818/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1156 - accuracy: 0.9554\n",
            "Epoch 818: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1212 - accuracy: 0.9520 - val_loss: 0.1310 - val_accuracy: 0.9610\n",
            "Epoch 819/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1760 - accuracy: 0.9394\n",
            "Epoch 819: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1752 - accuracy: 0.9396 - val_loss: 0.1208 - val_accuracy: 0.9681\n",
            "Epoch 820/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1190 - accuracy: 0.9476\n",
            "Epoch 820: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1238 - accuracy: 0.9467 - val_loss: 0.1003 - val_accuracy: 0.9752\n",
            "Epoch 821/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1223 - accuracy: 0.9549\n",
            "Epoch 821: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1243 - accuracy: 0.9538 - val_loss: 0.1290 - val_accuracy: 0.9716\n",
            "Epoch 822/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1819 - accuracy: 0.9350\n",
            "Epoch 822: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1588 - accuracy: 0.9404 - val_loss: 0.1586 - val_accuracy: 0.9681\n",
            "Epoch 823/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1527 - accuracy: 0.9433\n",
            "Epoch 823: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1526 - accuracy: 0.9413 - val_loss: 0.1458 - val_accuracy: 0.9681\n",
            "Epoch 824/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.9455\n",
            "Epoch 824: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1442 - accuracy: 0.9458 - val_loss: 0.1279 - val_accuracy: 0.9716\n",
            "Epoch 825/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1175 - accuracy: 0.9536\n",
            "Epoch 825: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1182 - accuracy: 0.9538 - val_loss: 0.1082 - val_accuracy: 0.9681\n",
            "Epoch 826/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1021 - accuracy: 0.9604\n",
            "Epoch 826: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1009 - accuracy: 0.9600 - val_loss: 0.1056 - val_accuracy: 0.9716\n",
            "Epoch 827/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1082 - accuracy: 0.9563\n",
            "Epoch 827: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9547 - val_loss: 0.1135 - val_accuracy: 0.9716\n",
            "Epoch 828/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1344 - accuracy: 0.9513\n",
            "Epoch 828: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1445 - accuracy: 0.9493 - val_loss: 0.1233 - val_accuracy: 0.9787\n",
            "Epoch 829/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1171 - accuracy: 0.9551\n",
            "Epoch 829: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1221 - accuracy: 0.9547 - val_loss: 0.1062 - val_accuracy: 0.9823\n",
            "Epoch 830/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1079 - accuracy: 0.9577\n",
            "Epoch 830: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9582 - val_loss: 0.0887 - val_accuracy: 0.9787\n",
            "Epoch 831/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1023 - accuracy: 0.9601\n",
            "Epoch 831: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1205 - accuracy: 0.9573 - val_loss: 0.1320 - val_accuracy: 0.9681\n",
            "Epoch 832/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1432 - accuracy: 0.9453\n",
            "Epoch 832: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1383 - accuracy: 0.9476 - val_loss: 0.1074 - val_accuracy: 0.9823\n",
            "Epoch 833/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1156 - accuracy: 0.9609\n",
            "Epoch 833: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1162 - accuracy: 0.9591 - val_loss: 0.0935 - val_accuracy: 0.9752\n",
            "Epoch 834/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.9494\n",
            "Epoch 834: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1351 - accuracy: 0.9502 - val_loss: 0.1080 - val_accuracy: 0.9823\n",
            "Epoch 835/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1592 - accuracy: 0.9453\n",
            "Epoch 835: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1714 - accuracy: 0.9404 - val_loss: 0.2107 - val_accuracy: 0.9610\n",
            "Epoch 836/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1742 - accuracy: 0.9440\n",
            "Epoch 836: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1826 - accuracy: 0.9387 - val_loss: 0.1389 - val_accuracy: 0.9681\n",
            "Epoch 837/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1512 - accuracy: 0.9502\n",
            "Epoch 837: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1601 - accuracy: 0.9502 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
            "Epoch 838/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1371 - accuracy: 0.9472\n",
            "Epoch 838: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1491 - accuracy: 0.9458 - val_loss: 0.1197 - val_accuracy: 0.9716\n",
            "Epoch 839/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1272 - accuracy: 0.9463\n",
            "Epoch 839: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1260 - accuracy: 0.9476 - val_loss: 0.1173 - val_accuracy: 0.9716\n",
            "Epoch 840/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9518\n",
            "Epoch 840: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1273 - accuracy: 0.9520 - val_loss: 0.1207 - val_accuracy: 0.9716\n",
            "Epoch 841/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9580\n",
            "Epoch 841: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1076 - accuracy: 0.9582 - val_loss: 0.1310 - val_accuracy: 0.9681\n",
            "Epoch 842/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1239 - accuracy: 0.9513\n",
            "Epoch 842: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1233 - accuracy: 0.9520 - val_loss: 0.1156 - val_accuracy: 0.9681\n",
            "Epoch 843/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1494 - accuracy: 0.9509\n",
            "Epoch 843: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9520 - val_loss: 0.1117 - val_accuracy: 0.9716\n",
            "Epoch 844/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9527\n",
            "Epoch 844: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1262 - accuracy: 0.9529 - val_loss: 0.1621 - val_accuracy: 0.9681\n",
            "Epoch 845/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1319 - accuracy: 0.9483\n",
            "Epoch 845: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1206 - accuracy: 0.9547 - val_loss: 0.1525 - val_accuracy: 0.9752\n",
            "Epoch 846/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1413 - accuracy: 0.9521\n",
            "Epoch 846: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1515 - accuracy: 0.9511 - val_loss: 0.1766 - val_accuracy: 0.9610\n",
            "Epoch 847/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.2009 - accuracy: 0.9438\n",
            "Epoch 847: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9484 - val_loss: 0.1459 - val_accuracy: 0.9681\n",
            "Epoch 848/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.0877 - accuracy: 0.9560\n",
            "Epoch 848: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.0919 - accuracy: 0.9573 - val_loss: 0.1489 - val_accuracy: 0.9681\n",
            "Epoch 849/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1428 - accuracy: 0.9488\n",
            "Epoch 849: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9511 - val_loss: 0.1568 - val_accuracy: 0.9716\n",
            "Epoch 850/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1310 - accuracy: 0.9520\n",
            "Epoch 850: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1209 - accuracy: 0.9547 - val_loss: 0.1498 - val_accuracy: 0.9681\n",
            "Epoch 851/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1255 - accuracy: 0.9537\n",
            "Epoch 851: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1367 - accuracy: 0.9538 - val_loss: 0.1344 - val_accuracy: 0.9716\n",
            "Epoch 852/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1037 - accuracy: 0.9576\n",
            "Epoch 852: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1149 - accuracy: 0.9529 - val_loss: 0.1354 - val_accuracy: 0.9645\n",
            "Epoch 853/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1366 - accuracy: 0.9576\n",
            "Epoch 853: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1330 - accuracy: 0.9582 - val_loss: 0.1479 - val_accuracy: 0.9645\n",
            "Epoch 854/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1249 - accuracy: 0.9498\n",
            "Epoch 854: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.9538 - val_loss: 0.1437 - val_accuracy: 0.9752\n",
            "Epoch 855/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9536\n",
            "Epoch 855: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1162 - accuracy: 0.9538 - val_loss: 0.1466 - val_accuracy: 0.9681\n",
            "Epoch 856/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1177 - accuracy: 0.9654\n",
            "Epoch 856: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9680 - val_loss: 0.1564 - val_accuracy: 0.9787\n",
            "Epoch 857/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1044 - accuracy: 0.9526\n",
            "Epoch 857: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9547 - val_loss: 0.1814 - val_accuracy: 0.9645\n",
            "Epoch 858/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.0944 - accuracy: 0.9688\n",
            "Epoch 858: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1096 - accuracy: 0.9644 - val_loss: 0.2049 - val_accuracy: 0.9539\n",
            "Epoch 859/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1486 - accuracy: 0.9509\n",
            "Epoch 859: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1633 - accuracy: 0.9476 - val_loss: 0.1626 - val_accuracy: 0.9610\n",
            "Epoch 860/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9431\n",
            "Epoch 860: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9431 - val_loss: 0.1420 - val_accuracy: 0.9681\n",
            "Epoch 861/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1033 - accuracy: 0.9618\n",
            "Epoch 861: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9636 - val_loss: 0.1434 - val_accuracy: 0.9787\n",
            "Epoch 862/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1476 - accuracy: 0.9542\n",
            "Epoch 862: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1415 - accuracy: 0.9556 - val_loss: 0.1586 - val_accuracy: 0.9468\n",
            "Epoch 863/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1150 - accuracy: 0.9546\n",
            "Epoch 863: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1240 - accuracy: 0.9493 - val_loss: 0.1734 - val_accuracy: 0.9574\n",
            "Epoch 864/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1245 - accuracy: 0.9525\n",
            "Epoch 864: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1256 - accuracy: 0.9520 - val_loss: 0.1395 - val_accuracy: 0.9681\n",
            "Epoch 865/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1548 - accuracy: 0.9491\n",
            "Epoch 865: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1391 - accuracy: 0.9520 - val_loss: 0.1752 - val_accuracy: 0.9645\n",
            "Epoch 866/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1080 - accuracy: 0.9641\n",
            "Epoch 866: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1155 - accuracy: 0.9600 - val_loss: 0.1502 - val_accuracy: 0.9752\n",
            "Epoch 867/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1120 - accuracy: 0.9537\n",
            "Epoch 867: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1047 - accuracy: 0.9591 - val_loss: 0.1548 - val_accuracy: 0.9645\n",
            "Epoch 868/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9556\n",
            "Epoch 868: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1126 - accuracy: 0.9556 - val_loss: 0.1688 - val_accuracy: 0.9681\n",
            "Epoch 869/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.0928 - accuracy: 0.9674\n",
            "Epoch 869: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1072 - accuracy: 0.9600 - val_loss: 0.1256 - val_accuracy: 0.9716\n",
            "Epoch 870/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9538\n",
            "Epoch 870: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1359 - accuracy: 0.9538 - val_loss: 0.1737 - val_accuracy: 0.9610\n",
            "Epoch 871/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1119 - accuracy: 0.9625\n",
            "Epoch 871: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1071 - accuracy: 0.9636 - val_loss: 0.1713 - val_accuracy: 0.9645\n",
            "Epoch 872/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.0867 - accuracy: 0.9676\n",
            "Epoch 872: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9609 - val_loss: 0.1756 - val_accuracy: 0.9716\n",
            "Epoch 873/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1447 - accuracy: 0.9567\n",
            "Epoch 873: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9529 - val_loss: 0.1465 - val_accuracy: 0.9574\n",
            "Epoch 874/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1864 - accuracy: 0.9495\n",
            "Epoch 874: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1864 - accuracy: 0.9493 - val_loss: 0.1409 - val_accuracy: 0.9468\n",
            "Epoch 875/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1240 - accuracy: 0.9531\n",
            "Epoch 875: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1327 - accuracy: 0.9502 - val_loss: 0.1339 - val_accuracy: 0.9610\n",
            "Epoch 876/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1367 - accuracy: 0.9513\n",
            "Epoch 876: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1335 - accuracy: 0.9564 - val_loss: 0.1494 - val_accuracy: 0.9574\n",
            "Epoch 877/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9579\n",
            "Epoch 877: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9609 - val_loss: 0.1416 - val_accuracy: 0.9645\n",
            "Epoch 878/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9491\n",
            "Epoch 878: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9467 - val_loss: 0.1252 - val_accuracy: 0.9681\n",
            "Epoch 879/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9460\n",
            "Epoch 879: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1239 - accuracy: 0.9476 - val_loss: 0.1305 - val_accuracy: 0.9574\n",
            "Epoch 880/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1173 - accuracy: 0.9596\n",
            "Epoch 880: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1148 - accuracy: 0.9609 - val_loss: 0.1706 - val_accuracy: 0.9610\n",
            "Epoch 881/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9688\n",
            "Epoch 881: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0992 - accuracy: 0.9680 - val_loss: 0.1676 - val_accuracy: 0.9823\n",
            "Epoch 882/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1134 - accuracy: 0.9593\n",
            "Epoch 882: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9547 - val_loss: 0.2055 - val_accuracy: 0.9610\n",
            "Epoch 883/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1280 - accuracy: 0.9561\n",
            "Epoch 883: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1245 - accuracy: 0.9573 - val_loss: 0.1277 - val_accuracy: 0.9645\n",
            "Epoch 884/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1114 - accuracy: 0.9588\n",
            "Epoch 884: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1124 - accuracy: 0.9600 - val_loss: 0.1228 - val_accuracy: 0.9716\n",
            "Epoch 885/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.0998 - accuracy: 0.9564\n",
            "Epoch 885: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0988 - accuracy: 0.9582 - val_loss: 0.1238 - val_accuracy: 0.9681\n",
            "Epoch 886/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9580\n",
            "Epoch 886: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9582 - val_loss: 0.1449 - val_accuracy: 0.9645\n",
            "Epoch 887/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1366 - accuracy: 0.9505\n",
            "Epoch 887: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1409 - accuracy: 0.9484 - val_loss: 0.1895 - val_accuracy: 0.9539\n",
            "Epoch 888/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1356 - accuracy: 0.9563\n",
            "Epoch 888: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1351 - accuracy: 0.9564 - val_loss: 0.1537 - val_accuracy: 0.9645\n",
            "Epoch 889/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1040 - accuracy: 0.9586\n",
            "Epoch 889: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9547 - val_loss: 0.1301 - val_accuracy: 0.9645\n",
            "Epoch 890/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1030 - accuracy: 0.9603\n",
            "Epoch 890: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1010 - accuracy: 0.9591 - val_loss: 0.1436 - val_accuracy: 0.9645\n",
            "Epoch 891/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1431 - accuracy: 0.9540\n",
            "Epoch 891: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1424 - accuracy: 0.9538 - val_loss: 0.1437 - val_accuracy: 0.9716\n",
            "Epoch 892/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.0976 - accuracy: 0.9619\n",
            "Epoch 892: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0954 - accuracy: 0.9627 - val_loss: 0.1481 - val_accuracy: 0.9681\n",
            "Epoch 893/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1193 - accuracy: 0.9536\n",
            "Epoch 893: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1170 - accuracy: 0.9547 - val_loss: 0.1530 - val_accuracy: 0.9681\n",
            "Epoch 894/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9671\n",
            "Epoch 894: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1250 - accuracy: 0.9671 - val_loss: 0.1767 - val_accuracy: 0.9574\n",
            "Epoch 895/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1300 - accuracy: 0.9542\n",
            "Epoch 895: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1275 - accuracy: 0.9529 - val_loss: 0.1479 - val_accuracy: 0.9504\n",
            "Epoch 896/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9500\n",
            "Epoch 896: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1294 - accuracy: 0.9529 - val_loss: 0.1550 - val_accuracy: 0.9468\n",
            "Epoch 897/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9573\n",
            "Epoch 897: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1181 - accuracy: 0.9573 - val_loss: 0.1364 - val_accuracy: 0.9610\n",
            "Epoch 898/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9545\n",
            "Epoch 898: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1300 - accuracy: 0.9520 - val_loss: 0.1507 - val_accuracy: 0.9504\n",
            "Epoch 899/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1491 - accuracy: 0.9495\n",
            "Epoch 899: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1526 - accuracy: 0.9484 - val_loss: 0.1527 - val_accuracy: 0.9610\n",
            "Epoch 900/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9623\n",
            "Epoch 900: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1116 - accuracy: 0.9609 - val_loss: 0.1535 - val_accuracy: 0.9539\n",
            "Epoch 901/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1169 - accuracy: 0.9537\n",
            "Epoch 901: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1066 - accuracy: 0.9591 - val_loss: 0.1451 - val_accuracy: 0.9574\n",
            "Epoch 902/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1427 - accuracy: 0.9479\n",
            "Epoch 902: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9484 - val_loss: 0.1447 - val_accuracy: 0.9716\n",
            "Epoch 903/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1013 - accuracy: 0.9580\n",
            "Epoch 903: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1148 - accuracy: 0.9573 - val_loss: 0.1348 - val_accuracy: 0.9645\n",
            "Epoch 904/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9536\n",
            "Epoch 904: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1414 - accuracy: 0.9538 - val_loss: 0.1555 - val_accuracy: 0.9504\n",
            "Epoch 905/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1292 - accuracy: 0.9513\n",
            "Epoch 905: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9556 - val_loss: 0.1376 - val_accuracy: 0.9681\n",
            "Epoch 906/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1106 - accuracy: 0.9606\n",
            "Epoch 906: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0977 - accuracy: 0.9644 - val_loss: 0.1552 - val_accuracy: 0.9610\n",
            "Epoch 907/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9609\n",
            "Epoch 907: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9609 - val_loss: 0.1644 - val_accuracy: 0.9539\n",
            "Epoch 908/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9550\n",
            "Epoch 908: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1283 - accuracy: 0.9547 - val_loss: 0.1733 - val_accuracy: 0.9610\n",
            "Epoch 909/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1163 - accuracy: 0.9527\n",
            "Epoch 909: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1170 - accuracy: 0.9520 - val_loss: 0.1976 - val_accuracy: 0.9504\n",
            "Epoch 910/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9520\n",
            "Epoch 910: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1304 - accuracy: 0.9520 - val_loss: 0.1702 - val_accuracy: 0.9539\n",
            "Epoch 911/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1289 - accuracy: 0.9538\n",
            "Epoch 911: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1402 - accuracy: 0.9511 - val_loss: 0.1689 - val_accuracy: 0.9645\n",
            "Epoch 912/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9609\n",
            "Epoch 912: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1168 - accuracy: 0.9591 - val_loss: 0.1494 - val_accuracy: 0.9645\n",
            "Epoch 913/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9500\n",
            "Epoch 913: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1319 - accuracy: 0.9493 - val_loss: 0.1630 - val_accuracy: 0.9574\n",
            "Epoch 914/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1197 - accuracy: 0.9502\n",
            "Epoch 914: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1285 - accuracy: 0.9467 - val_loss: 0.1514 - val_accuracy: 0.9539\n",
            "Epoch 915/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1384 - accuracy: 0.9600\n",
            "Epoch 915: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1384 - accuracy: 0.9600 - val_loss: 0.1803 - val_accuracy: 0.9433\n",
            "Epoch 916/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1773 - accuracy: 0.9473\n",
            "Epoch 916: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1763 - accuracy: 0.9449 - val_loss: 0.1923 - val_accuracy: 0.9504\n",
            "Epoch 917/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1183 - accuracy: 0.9555\n",
            "Epoch 917: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1223 - accuracy: 0.9538 - val_loss: 0.1795 - val_accuracy: 0.9610\n",
            "Epoch 918/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9536\n",
            "Epoch 918: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9529 - val_loss: 0.1818 - val_accuracy: 0.9504\n",
            "Epoch 919/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1072 - accuracy: 0.9561\n",
            "Epoch 919: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1026 - accuracy: 0.9591 - val_loss: 0.1725 - val_accuracy: 0.9574\n",
            "Epoch 920/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9625\n",
            "Epoch 920: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0957 - accuracy: 0.9618 - val_loss: 0.1803 - val_accuracy: 0.9574\n",
            "Epoch 921/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1318 - accuracy: 0.9555\n",
            "Epoch 921: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9511 - val_loss: 0.1748 - val_accuracy: 0.9610\n",
            "Epoch 922/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1301 - accuracy: 0.9563\n",
            "Epoch 922: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9618 - val_loss: 0.1635 - val_accuracy: 0.9610\n",
            "Epoch 923/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.9529\n",
            "Epoch 923: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1454 - accuracy: 0.9529 - val_loss: 0.1832 - val_accuracy: 0.9539\n",
            "Epoch 924/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9570\n",
            "Epoch 924: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1391 - accuracy: 0.9547 - val_loss: 0.1752 - val_accuracy: 0.9539\n",
            "Epoch 925/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1247 - accuracy: 0.9483\n",
            "Epoch 925: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9502 - val_loss: 0.1549 - val_accuracy: 0.9645\n",
            "Epoch 926/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1116 - accuracy: 0.9479\n",
            "Epoch 926: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1245 - accuracy: 0.9467 - val_loss: 0.1767 - val_accuracy: 0.9468\n",
            "Epoch 927/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1528 - accuracy: 0.9327\n",
            "Epoch 927: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1466 - accuracy: 0.9378 - val_loss: 0.1349 - val_accuracy: 0.9504\n",
            "Epoch 928/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1124 - accuracy: 0.9545\n",
            "Epoch 928: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1095 - accuracy: 0.9547 - val_loss: 0.1218 - val_accuracy: 0.9574\n",
            "Epoch 929/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9618\n",
            "Epoch 929: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9618 - val_loss: 0.1176 - val_accuracy: 0.9574\n",
            "Epoch 930/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9589\n",
            "Epoch 930: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9591 - val_loss: 0.1437 - val_accuracy: 0.9433\n",
            "Epoch 931/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9536\n",
            "Epoch 931: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1280 - accuracy: 0.9520 - val_loss: 0.1439 - val_accuracy: 0.9574\n",
            "Epoch 932/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1065 - accuracy: 0.9598\n",
            "Epoch 932: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1154 - accuracy: 0.9547 - val_loss: 0.1646 - val_accuracy: 0.9504\n",
            "Epoch 933/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1350 - accuracy: 0.9568\n",
            "Epoch 933: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9582 - val_loss: 0.2123 - val_accuracy: 0.9468\n",
            "Epoch 934/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1777 - accuracy: 0.9498\n",
            "Epoch 934: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1868 - accuracy: 0.9467 - val_loss: 0.2129 - val_accuracy: 0.9539\n",
            "Epoch 935/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9529\n",
            "Epoch 935: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1211 - accuracy: 0.9529 - val_loss: 0.1756 - val_accuracy: 0.9539\n",
            "Epoch 936/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9536\n",
            "Epoch 936: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1407 - accuracy: 0.9538 - val_loss: 0.2818 - val_accuracy: 0.9362\n",
            "Epoch 937/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1881 - accuracy: 0.9536\n",
            "Epoch 937: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1771 - accuracy: 0.9556 - val_loss: 0.1292 - val_accuracy: 0.9539\n",
            "Epoch 938/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1101 - accuracy: 0.9567\n",
            "Epoch 938: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9582 - val_loss: 0.1211 - val_accuracy: 0.9574\n",
            "Epoch 939/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.0832 - accuracy: 0.9674\n",
            "Epoch 939: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0970 - accuracy: 0.9618 - val_loss: 0.1353 - val_accuracy: 0.9574\n",
            "Epoch 940/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1485 - accuracy: 0.9470\n",
            "Epoch 940: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1460 - accuracy: 0.9493 - val_loss: 0.1652 - val_accuracy: 0.9468\n",
            "Epoch 941/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1165 - accuracy: 0.9580\n",
            "Epoch 941: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1179 - accuracy: 0.9538 - val_loss: 0.1353 - val_accuracy: 0.9645\n",
            "Epoch 942/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1166 - accuracy: 0.9521\n",
            "Epoch 942: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1342 - accuracy: 0.9520 - val_loss: 0.1447 - val_accuracy: 0.9645\n",
            "Epoch 943/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1213 - accuracy: 0.9527\n",
            "Epoch 943: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1150 - accuracy: 0.9556 - val_loss: 0.1480 - val_accuracy: 0.9645\n",
            "Epoch 944/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1040 - accuracy: 0.9623\n",
            "Epoch 944: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1018 - accuracy: 0.9627 - val_loss: 0.1543 - val_accuracy: 0.9716\n",
            "Epoch 945/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.1183 - accuracy: 0.9600\n",
            "Epoch 945: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1347 - accuracy: 0.9564 - val_loss: 0.1583 - val_accuracy: 0.9610\n",
            "Epoch 946/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1180 - accuracy: 0.9542\n",
            "Epoch 946: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1150 - accuracy: 0.9547 - val_loss: 0.1572 - val_accuracy: 0.9539\n",
            "Epoch 947/1000\n",
            "23/36 [==================>...........] - ETA: 0s - loss: 0.1331 - accuracy: 0.9579\n",
            "Epoch 947: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1161 - accuracy: 0.9591 - val_loss: 0.1452 - val_accuracy: 0.9645\n",
            "Epoch 948/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1235 - accuracy: 0.9618\n",
            "Epoch 948: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1120 - accuracy: 0.9627 - val_loss: 0.1634 - val_accuracy: 0.9645\n",
            "Epoch 949/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1652 - accuracy: 0.9471\n",
            "Epoch 949: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1472 - accuracy: 0.9493 - val_loss: 0.1547 - val_accuracy: 0.9645\n",
            "Epoch 950/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1080 - accuracy: 0.9594\n",
            "Epoch 950: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1015 - accuracy: 0.9609 - val_loss: 0.1575 - val_accuracy: 0.9645\n",
            "Epoch 951/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.0972 - accuracy: 0.9615\n",
            "Epoch 951: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.0943 - accuracy: 0.9627 - val_loss: 0.1560 - val_accuracy: 0.9610\n",
            "Epoch 952/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1196 - accuracy: 0.9577\n",
            "Epoch 952: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1268 - accuracy: 0.9538 - val_loss: 0.1475 - val_accuracy: 0.9645\n",
            "Epoch 953/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1170 - accuracy: 0.9563\n",
            "Epoch 953: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1253 - accuracy: 0.9564 - val_loss: 0.2040 - val_accuracy: 0.9645\n",
            "Epoch 954/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1221 - accuracy: 0.9700\n",
            "Epoch 954: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1258 - accuracy: 0.9636 - val_loss: 0.1990 - val_accuracy: 0.9645\n",
            "Epoch 955/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1008 - accuracy: 0.9605\n",
            "Epoch 955: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1034 - accuracy: 0.9600 - val_loss: 0.1724 - val_accuracy: 0.9610\n",
            "Epoch 956/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1437 - accuracy: 0.9601\n",
            "Epoch 956: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1439 - accuracy: 0.9582 - val_loss: 0.1752 - val_accuracy: 0.9504\n",
            "Epoch 957/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9538\n",
            "Epoch 957: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1467 - accuracy: 0.9538 - val_loss: 0.1193 - val_accuracy: 0.9574\n",
            "Epoch 958/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9520\n",
            "Epoch 958: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1536 - accuracy: 0.9520 - val_loss: 0.1302 - val_accuracy: 0.9574\n",
            "Epoch 959/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9502\n",
            "Epoch 959: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1244 - accuracy: 0.9502 - val_loss: 0.1223 - val_accuracy: 0.9574\n",
            "Epoch 960/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1123 - accuracy: 0.9609\n",
            "Epoch 960: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1208 - accuracy: 0.9564 - val_loss: 0.1228 - val_accuracy: 0.9610\n",
            "Epoch 961/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1022 - accuracy: 0.9622\n",
            "Epoch 961: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.0992 - accuracy: 0.9644 - val_loss: 0.1171 - val_accuracy: 0.9681\n",
            "Epoch 962/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1214 - accuracy: 0.9516\n",
            "Epoch 962: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1215 - accuracy: 0.9502 - val_loss: 0.1354 - val_accuracy: 0.9574\n",
            "Epoch 963/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1067 - accuracy: 0.9563\n",
            "Epoch 963: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1186 - accuracy: 0.9556 - val_loss: 0.1159 - val_accuracy: 0.9610\n",
            "Epoch 964/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1084 - accuracy: 0.9591\n",
            "Epoch 964: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1027 - accuracy: 0.9600 - val_loss: 0.1031 - val_accuracy: 0.9716\n",
            "Epoch 965/1000\n",
            "32/36 [=========================>....] - ETA: 0s - loss: 0.0983 - accuracy: 0.9658\n",
            "Epoch 965: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9644 - val_loss: 0.1082 - val_accuracy: 0.9752\n",
            "Epoch 966/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1267 - accuracy: 0.9650\n",
            "Epoch 966: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9609 - val_loss: 0.1318 - val_accuracy: 0.9645\n",
            "Epoch 967/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.9531\n",
            "Epoch 967: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1331 - accuracy: 0.9529 - val_loss: 0.1096 - val_accuracy: 0.9752\n",
            "Epoch 968/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9555\n",
            "Epoch 968: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1115 - accuracy: 0.9538 - val_loss: 0.1611 - val_accuracy: 0.9645\n",
            "Epoch 969/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1203 - accuracy: 0.9547\n",
            "Epoch 969: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1220 - accuracy: 0.9564 - val_loss: 0.1168 - val_accuracy: 0.9681\n",
            "Epoch 970/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1642 - accuracy: 0.9456\n",
            "Epoch 970: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1471 - accuracy: 0.9511 - val_loss: 0.1342 - val_accuracy: 0.9574\n",
            "Epoch 971/1000\n",
            "31/36 [========================>.....] - ETA: 0s - loss: 0.1199 - accuracy: 0.9516\n",
            "Epoch 971: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1125 - accuracy: 0.9547 - val_loss: 0.1350 - val_accuracy: 0.9610\n",
            "Epoch 972/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9563\n",
            "Epoch 972: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9564 - val_loss: 0.1565 - val_accuracy: 0.9574\n",
            "Epoch 973/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1067 - accuracy: 0.9631\n",
            "Epoch 973: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1151 - accuracy: 0.9600 - val_loss: 0.1506 - val_accuracy: 0.9645\n",
            "Epoch 974/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1443 - accuracy: 0.9487\n",
            "Epoch 974: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1355 - accuracy: 0.9520 - val_loss: 0.1459 - val_accuracy: 0.9539\n",
            "Epoch 975/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1218 - accuracy: 0.9560\n",
            "Epoch 975: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1248 - accuracy: 0.9564 - val_loss: 0.1222 - val_accuracy: 0.9574\n",
            "Epoch 976/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1213 - accuracy: 0.9537\n",
            "Epoch 976: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9556 - val_loss: 0.1230 - val_accuracy: 0.9610\n",
            "Epoch 977/1000\n",
            "34/36 [===========================>..] - ETA: 0s - loss: 0.1746 - accuracy: 0.9494\n",
            "Epoch 977: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1766 - accuracy: 0.9493 - val_loss: 0.1765 - val_accuracy: 0.9574\n",
            "Epoch 978/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1773 - accuracy: 0.9413\n",
            "Epoch 978: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 10ms/step - loss: 0.1761 - accuracy: 0.9431 - val_loss: 0.1555 - val_accuracy: 0.9681\n",
            "Epoch 979/1000\n",
            "33/36 [==========================>...] - ETA: 0s - loss: 0.1337 - accuracy: 0.9564\n",
            "Epoch 979: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1297 - accuracy: 0.9573 - val_loss: 0.1179 - val_accuracy: 0.9645\n",
            "Epoch 980/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1212 - accuracy: 0.9479\n",
            "Epoch 980: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1117 - accuracy: 0.9529 - val_loss: 0.1438 - val_accuracy: 0.9681\n",
            "Epoch 981/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1281 - accuracy: 0.9549\n",
            "Epoch 981: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9511 - val_loss: 0.1384 - val_accuracy: 0.9645\n",
            "Epoch 982/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1373 - accuracy: 0.9479\n",
            "Epoch 982: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1266 - accuracy: 0.9520 - val_loss: 0.1161 - val_accuracy: 0.9610\n",
            "Epoch 983/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1330 - accuracy: 0.9459\n",
            "Epoch 983: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1245 - accuracy: 0.9493 - val_loss: 0.1260 - val_accuracy: 0.9574\n",
            "Epoch 984/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9564\n",
            "Epoch 984: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1239 - accuracy: 0.9564 - val_loss: 0.1244 - val_accuracy: 0.9610\n",
            "Epoch 985/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1268 - accuracy: 0.9572\n",
            "Epoch 985: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9556 - val_loss: 0.1576 - val_accuracy: 0.9574\n",
            "Epoch 986/1000\n",
            "29/36 [=======================>......] - ETA: 0s - loss: 0.1133 - accuracy: 0.9612\n",
            "Epoch 986: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1081 - accuracy: 0.9618 - val_loss: 0.1304 - val_accuracy: 0.9681\n",
            "Epoch 987/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9547\n",
            "Epoch 987: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9547 - val_loss: 0.1402 - val_accuracy: 0.9681\n",
            "Epoch 988/1000\n",
            "24/36 [===================>..........] - ETA: 0s - loss: 0.1095 - accuracy: 0.9518\n",
            "Epoch 988: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1131 - accuracy: 0.9547 - val_loss: 0.1299 - val_accuracy: 0.9681\n",
            "Epoch 989/1000\n",
            "25/36 [===================>..........] - ETA: 0s - loss: 0.1170 - accuracy: 0.9575\n",
            "Epoch 989: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1125 - accuracy: 0.9582 - val_loss: 0.1169 - val_accuracy: 0.9645\n",
            "Epoch 990/1000\n",
            "36/36 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9502\n",
            "Epoch 990: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9502 - val_loss: 0.1467 - val_accuracy: 0.9574\n",
            "Epoch 991/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1399 - accuracy: 0.9542\n",
            "Epoch 991: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1376 - accuracy: 0.9520 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
            "Epoch 992/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9411\n",
            "Epoch 992: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1426 - accuracy: 0.9413 - val_loss: 0.1468 - val_accuracy: 0.9645\n",
            "Epoch 993/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1060 - accuracy: 0.9627\n",
            "Epoch 993: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1110 - accuracy: 0.9582 - val_loss: 0.1287 - val_accuracy: 0.9716\n",
            "Epoch 994/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1039 - accuracy: 0.9615\n",
            "Epoch 994: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1090 - accuracy: 0.9600 - val_loss: 0.1097 - val_accuracy: 0.9716\n",
            "Epoch 995/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.0981 - accuracy: 0.9653\n",
            "Epoch 995: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1092 - accuracy: 0.9600 - val_loss: 0.1060 - val_accuracy: 0.9681\n",
            "Epoch 996/1000\n",
            "35/36 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9634\n",
            "Epoch 996: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9627 - val_loss: 0.1217 - val_accuracy: 0.9716\n",
            "Epoch 997/1000\n",
            "28/36 [======================>.......] - ETA: 0s - loss: 0.1150 - accuracy: 0.9621\n",
            "Epoch 997: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 0.1190 - accuracy: 0.9600 - val_loss: 0.1105 - val_accuracy: 0.9681\n",
            "Epoch 998/1000\n",
            "30/36 [========================>.....] - ETA: 0s - loss: 0.1014 - accuracy: 0.9604\n",
            "Epoch 998: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1030 - accuracy: 0.9591 - val_loss: 0.1207 - val_accuracy: 0.9610\n",
            "Epoch 999/1000\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 0.1165 - accuracy: 0.9507\n",
            "Epoch 999: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9547 - val_loss: 0.1128 - val_accuracy: 0.9716\n",
            "Epoch 1000/1000\n",
            "27/36 [=====================>........] - ETA: 0s - loss: 0.1173 - accuracy: 0.9560\n",
            "Epoch 1000: val_loss did not improve from 0.08500\n",
            "36/36 [==============================] - 0s 9ms/step - loss: 0.1114 - accuracy: 0.9582 - val_loss: 0.1024 - val_accuracy: 0.9787\n",
            "Training completed in time:  0:04:23.000753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = model.evaluate(X_test, y_test)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRj-9a69nNjw",
        "outputId": "f10a71bc-b559-43ca-f5d0-095289e1343b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9787\n",
            "[0.10242694616317749, 0.978723406791687]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gnx_J0lsx2y",
        "outputId": "0515eef9-64d4-4365-da72-9f0b24d89d06"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "ajnkQ0L4un2x",
        "outputId": "827706c8-6b71-4af9-9f99-818e1a19f455"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFT0lEQVR4nO3deXxU5d3///eZPclksi8sCUTZZXEBNWJbqygudaV1uW0L1uWnxQWpda1WsQr1vq0rot/eivau1rpvuCECKgUEFBVkVTAIZGHJnsx6/f7AjEZQcSbkZOD1fDzmYXLOyZnPXDwgbz/Xdc6xjDFGAAAAKchhdwEAAACJIsgAAICURZABAAApiyADAABSFkEGAACkLIIMAABIWQQZAACQsggyAAAgZRFkAABAyiLIAOhS1q9fL8uy9Oijj/7on50zZ44sy9KcOXO+97hHH31UlmVp/fr1CdUIoOsgyAAAgJRFkAEAACmLIAMAAFIWQQZAOzfffLMsy9Lq1av161//WllZWSooKNCNN94oY4w2bNigU089VYFAQMXFxbrzzjt3Okd1dbXOP/98FRUVyefzadiwYXrsscd2Oq62tlbjxo1TVlaWsrOzNXbsWNXW1u6yrpUrV+qXv/ylcnNz5fP5NHz4cL300ksd+tkfeOABHXDAAfJ6verevbvGjx+/Uz1r1qzRmDFjVFxcLJ/Pp549e+rss89WXV1d/JiZM2fqyCOPVHZ2tvx+v/r376/rr7++Q2sFsIPL7gIAdE1nnXWWBg4cqClTpmjGjBn6y1/+otzcXD300EM6+uij9de//lWPP/64rrrqKo0YMUI//elPJUktLS066qijtHbtWl166aUqKyvT008/rXHjxqm2tlZXXHGFJMkYo1NPPVXvvfeeLr74Yg0cOFDPP/+8xo4du1Mty5cv18iRI9WjRw9de+21ysjI0FNPPaXTTjtNzz77rE4//fSkP+/NN9+sW265RaNGjdIll1yiVatWadq0aVq0aJHmzZsnt9utUCik0aNHKxgM6rLLLlNxcbE2btyoV155RbW1tcrKytLy5cv1i1/8QkOHDtWkSZPk9Xq1du1azZs3L+kaAeyCAYBv+POf/2wkmYsuuii+LRKJmJ49exrLssyUKVPi27dv327S0tLM2LFj49vuvvtuI8n885//jG8LhUKmvLzc+P1+U19fb4wx5oUXXjCSzB133NHufX7yk58YSWb69Onx7cccc4wZMmSIaW1tjW+LxWLmiCOOMH379o1vmz17tpFkZs+e/b2fcfr06UaSWbdunTHGmOrqauPxeMxxxx1notFo/Lj777/fSDKPPPKIMcaYDz/80EgyTz/99Hee+6677jKSTE1NzffWAKBjMLUEYJcuuOCC+NdOp1PDhw+XMUbnn39+fHt2drb69++vzz//PL7t1VdfVXFxsc4555z4Nrfbrcsvv1yNjY2aO3du/DiXy6VLLrmk3ftcdtll7erYtm2b3n77bZ155plqaGjQli1btGXLFm3dulWjR4/WmjVrtHHjxqQ+61tvvaVQKKQJEybI4fj6n8ULL7xQgUBAM2bMkCRlZWVJkt544w01Nzfv8lzZ2dmSpBdffFGxWCypugD8MIIMgF0qLS1t931WVpZ8Pp/y8/N32r59+/b491988YX69u3bLhBI0sCBA+P72/7brVs3+f3+dsf179+/3fdr166VMUY33nijCgoK2r3+/Oc/S9qxJicZbTV9+709Ho/222+/+P6ysjJNnDhR//u//6v8/HyNHj1aU6dObbc+5qyzztLIkSN1wQUXqKioSGeffbaeeuopQg2wh7BGBsAuOZ3O3dom7Vjvsqe0BYCrrrpKo0eP3uUxffr02WPv/2133nmnxo0bpxdffFFvvvmmLr/8ck2ePFkLFixQz549lZaWpnfeeUezZ8/WjBkz9Prrr+vf//63jj76aL355pvfOYYAEkNHBkCH6tWrl9asWbNTB2LlypXx/W3/3bx5sxobG9sdt2rVqnbf77fffpJ2TE+NGjVql6/MzMyka97Ve4dCIa1bty6+v82QIUP0pz/9Se+8847effddbdy4UQ8++GB8v8Ph0DHHHKO//e1v+vTTT3Xbbbfp7bff1uzZs5OqE8DOCDIAOtSJJ56oyspK/fvf/45vi0Qiuu++++T3+/Wzn/0sflwkEtG0adPix0WjUd13333tzldYWKijjjpKDz30kDZv3rzT+9XU1CRd86hRo+TxeHTvvfe26y49/PDDqqur00knnSRJqq+vVyQSafezQ4YMkcPhUDAYlLRjTc+3HXjggZIUPwZAx2FqCUCHuuiii/TQQw9p3LhxWrJkiXr37q1nnnlG8+bN09133x3vnpx88skaOXKkrr32Wq1fv16DBg3Sc8891269SZupU6fqyCOP1JAhQ3ThhRdqv/32U1VVlebPn68vv/xSH330UVI1FxQU6LrrrtMtt9yi448/XqeccopWrVqlBx54QCNGjNCvf/1rSdLbb7+tSy+9VL/61a/Ur18/RSIR/d///Z+cTqfGjBkjSZo0aZLeeecdnXTSSerVq5eqq6v1wAMPqGfPnjryyCOTqhPAzggyADpUWlqa5syZo2uvvVaPPfaY6uvr1b9/f02fPl3jxo2LH+dwOPTSSy9pwoQJ+uc//ynLsnTKKafozjvv1EEHHdTunIMGDdLixYt1yy236NFHH9XWrVtVWFiogw46SDfddFOH1H3zzTeroKBA999/v6688krl5ubqoosu0u233y632y1JGjZsmEaPHq2XX35ZGzduVHp6uoYNG6bXXntNhx9+uCTplFNO0fr16/XII49oy5Ytys/P189+9jPdcsst8aueAHQcy+zJVXoAAAB7EGtkAABAyiLIAACAlEWQAQAAKYsgAwAAUhZBBgAApCyCDAAASFl7/X1kYrGYNm3apMzMTFmWZXc5AABgNxhj1NDQoO7du+/0ENpv2uuDzKZNm1RSUmJ3GQAAIAEbNmxQz549v3P/Xh9k2m6HvmHDBgUCAZurAQAAu6O+vl4lJSU/+FDYvT7ItE0nBQIBggwAACnmh5aFsNgXAACkLIIMAABIWQQZAACQsvb6NTK7KxqNKhwO211GSvJ4PN97aRwAAHvKPh9kjDGqrKxUbW2t3aWkLIfDobKyMnk8HrtLAQDsY/b5INMWYgoLC5Wens5N836kthsObt68WaWlpYwfAKBT7dNBJhqNxkNMXl6e3eWkrIKCAm3atEmRSERut9vucgAA+5B9emFD25qY9PR0mytJbW1TStFo1OZKAAD7mn06yLRhOiQ5jB8AwC4EGQAAkLIIMlDv3r119913210GAAA/2j692DeVHXXUUTrwwAM7JIAsWrRIGRkZyRcFAEAnI8gkKBKNKWaMHA5Lri54MzhjjKLRqFyuH/4jLigo6ISKAADoeF3vN3CKqKxv1crKBm1tDHX6e48bN05z587VPffcI8uyZFmWHn30UVmWpddee02HHHKIvF6v3nvvPX322Wc69dRTVVRUJL/frxEjRuitt95qd75vTy1ZlqX//d//1emnn6709HT17dtXL730Uid/SgAAfhhB5huMMWoORXbr1RKKqjUcVUsouts/830vY8xu13nPPfeovLxcF154oTZv3qzNmzerpKREknTttddqypQpWrFihYYOHarGxkadeOKJmjVrlj788EMdf/zxOvnkk1VRUfG973HLLbfozDPP1Mcff6wTTzxR5557rrZt25bU+AIA0NGYWvqGlnBUg256w5b3/nTSaKV7du+PIysrSx6PR+np6SouLpYkrVy5UpI0adIkHXvssfFjc3NzNWzYsPj3t956q55//nm99NJLuvTSS7/zPcaNG6dzzjlHknT77bfr3nvv1fvvv6/jjz/+R382AAD2FDoye5nhw4e3+76xsVFXXXWVBg4cqOzsbPn9fq1YseIHOzJDhw6Nf52RkaFAIKDq6uo9UjMAAImiI/MNaW6nPp00ereO3bS9RduaQyrM9Kkw4O2Q9+4I37766KqrrtLMmTP1P//zP+rTp4/S0tL0y1/+UqHQ96/t+fajBizLUiwW65AaAQDoKASZb7Asa7end9I8TvnCTqV5nLv9Mx3J4/Hs1iMB5s2bp3Hjxun000+XtKNDs379+j1cHQAAnYOppYTZe1v+3r17a+HChVq/fr22bNnynd2Svn376rnnntPSpUv10Ucf6b/+67/orAAA9hoEmSTt/rVGHeuqq66S0+nUoEGDVFBQ8J1rXv72t78pJydHRxxxhE4++WSNHj1aBx98cCdXCwDAnmGZH3Pdbwqqr69XVlaW6urqFAgE2u1rbW3VunXrVFZWJp/P96POu7G2RVsbgyoM+FQc+HE/u7dJZhwBANiV7/v9/U10ZJK1V8dAAAC6NoIMAABIWQQZAACQsggyCbL3miUAACARZAAAQAojyCSN1b4AANiFIAMAAFIWQQYAAKQsggwAAEhZBJkksUIGAAD7EGRS1FFHHaUJEyZ02PnGjRun0047rcPOBwBAZyDIAACAlEWQSUHjxo3T3Llzdc8998iyLFmWpfXr12vZsmU64YQT5Pf7VVRUpN/85jfasmVL/OeeeeYZDRkyRGlpacrLy9OoUaPU1NSkm2++WY899phefPHF+PnmzJlj3wcEAGA3uewuoEsxRgo379ahVrhFVjgkKxyRQrHk39udLlm7d7/ge+65R6tXr9bgwYM1adKkHT/uduvQQw/VBRdcoLvuukstLS265pprdOaZZ+rtt9/W5s2bdc455+iOO+7Q6aefroaGBr377rsyxuiqq67SihUrVF9fr+nTp0uScnNzk/9MAADsYQSZbwo3S7d3361Du3316jDXb5I8Gbt1aFZWljwej9LT01VcXCxJ+stf/qKDDjpIt99+e/y4Rx55RCUlJVq9erUaGxsViUR0xhlnqFevXpKkIUOGxI9NS0tTMBiMnw8AgFTQZaaWpkyZIsuy2i1gbW1t1fjx45WXlye/368xY8aoqqrKviK7sI8++kizZ8+W3++PvwYMGCBJ+uyzzzRs2DAdc8wxGjJkiH71q1/p73//u7Zv325z1QAAJKdLdGQWLVqkhx56SEOHDm23/corr9SMGTP09NNPKysrS5deeqnOOOMMzZs3b88U4k7f0RnZDZX1LappCKkg06PiQFrHvHcSGhsbdfLJJ+uvf/3rTvu6desmp9OpmTNn6j//+Y/efPNN3Xfffbrhhhu0cOFClZWVJfXeAADYxfYg09jYqHPPPVd///vf9Ze//CW+va6uTg8//LCeeOIJHX300ZKk6dOna+DAgVqwYIEOP/zwji/GsnZ7ese4HTJul4zbK3k6IMj8SB6PR9FoNP79wQcfrGeffVa9e/eWy7XrP1bLsjRy5EiNHDlSN910k3r16qXnn39eEydO3Ol8AACkAtunlsaPH6+TTjpJo0aNard9yZIlCofD7bYPGDBApaWlmj9//neeLxgMqr6+vt1rj7Lpjni9e/fWwoULtX79em3ZskXjx4/Xtm3bdM4552jRokX67LPP9MYbb+i8885TNBrVwoULdfvtt2vx4sWqqKjQc889p5qaGg0cODB+vo8//lirVq3Sli1bFA6H7flgAAD8CLYGmSeffFIffPCBJk+evNO+yspKeTweZWdnt9teVFSkysrK7zzn5MmTlZWVFX+VlJR0dNldwlVXXSWn06lBgwapoKBAoVBI8+bNUzQa1XHHHachQ4ZowoQJys7OlsPhUCAQ0DvvvKMTTzxR/fr105/+9CfdeeedOuGEEyRJF154ofr376/hw4eroKBgz03fAQDQgWybWtqwYYOuuOIKzZw5Uz6fr8POe91112nixInx7+vr6/domLHrEQX9+vXbZWfqueee2+XxAwcO1Ouvv/6d5ysoKNCbb77ZYfUBANAZbOvILFmyRNXV1Tr44IPlcrnkcrk0d+5c3XvvvXK5XCoqKlIoFFJtbW27n6uqqvreS4S9Xq8CgUC7FwAA2DvZ1pE55phj9Mknn7Tbdt5552nAgAG65pprVFJSIrfbrVmzZmnMmDGSpFWrVqmiokLl5eV2lAwAALoY24JMZmamBg8e3G5bRkaG8vLy4tvPP/98TZw4Ubm5uQoEArrssstUXl6+Z65YAgAAKcf2y6+/z1133SWHw6ExY8YoGAxq9OjReuCBB+wuCwAAdBFdKsh8+0GFPp9PU6dO1dSpU/fo+xrz45fs7t5TkfYNiYwfAAAdwfb7yNjJ7XZLkpqbd+9Bkdi1UCgkSXI6nTZXAgDY13Spjkxnczqdys7OVnV1tSQpPT1d1m4+gTocCspEQoqEpNbWfbc/E4vFVFNTo/T09O+8ozAAAHvKPv+bp+1S7rYws7vqWsJqaI2o2etSU7p7T5SWMhwOh0pLS3c7BAIA0FH2+SBjWZa6deumwsLCH3Vb/kfeW6fHF27S6Qf10KVH79sPXfR4PHI49ulZSgCATfb5INPG6XT+qDUezVGHNjZE1RhxdOidiQEAwO7jf6MT1DaLwhU7AADYhyCTIFaDAABgP4JMkujHAABgH4JMorhCBwAA2xFkksQSGQAA7EOQSRD9GAAA7EeQSZJhlQwAALYhyCSIJTIAANiPIJMk1sgAAGAfgkyCLFbJAABgO4JMkmjIAABgH4JMglgjAwCA/QgySWKNDAAA9iHIJIiGDAAA9iPIJI2WDAAAdiHIJKhtjQxTSwAA2IcgkyCL1b4AANiOIJMkOjIAANiHIAMAAFIWQSZJPDQSAAD7EGQSxBIZAADsR5BJEmtkAACwD0EmQTw0EgAA+xFkkkRDBgAA+xBkEsQaGQAA7EeQSRJrZAAAsA9BJkE0ZAAAsB9BJkncRwYAAPsQZBLEGhkAAOxHkEkWDRkAAGxDkElQ231kyDEAANiHIJMgppYAALAfQSZJhuuvAQCwDUEGAACkLIJMkujHAABgH4JMgiwWyQAAYDuCTJJYIgMAgH0IMgmiHwMAgP0IMkmiIQMAgH0IMgliiQwAAPYjyCSJ+8gAAGAfgkyCaMgAAGA/gkyS6McAAGAfgkyC4veRIckAAGAbgkyCWOwLAID9CDJJMrRkAACwDUEmQTRkAACwH0EmSVx9DQCAfQgyiWKRDAAAtiPIJImODAAA9iHIJIh+DAAA9iPIJImrlgAAsA9BJkEskQEAwH4EmSSxRgYAAPsQZBJksUoGAADbEWSSREMGAAD7EGQSxBoZAADsR5BJEmtkAACwD0EmQV83ZEgyAADYhSCTIKaWAACwH0EmSUwtAQBgH1uDzLRp0zR06FAFAgEFAgGVl5frtddei+9vbW3V+PHjlZeXJ7/frzFjxqiqqsrGir/G5dcAANjP1iDTs2dPTZkyRUuWLNHixYt19NFH69RTT9Xy5cslSVdeeaVefvllPf3005o7d642bdqkM844w86Sd0JDBgAA+7jsfPOTTz653fe33Xabpk2bpgULFqhnz556+OGH9cQTT+joo4+WJE2fPl0DBw7UggULdPjhh9tR8tdoyAAAYLsus0YmGo3qySefVFNTk8rLy7VkyRKFw2GNGjUqfsyAAQNUWlqq+fPnf+d5gsGg6uvr2732JMMiGQAAbGN7kPnkk0/k9/vl9Xp18cUX6/nnn9egQYNUWVkpj8ej7OzsdscXFRWpsrLyO883efJkZWVlxV8lJSV7pG4aMgAA2M/2INO/f38tXbpUCxcu1CWXXKKxY8fq008/Tfh81113nerq6uKvDRs2dGC1O6MfAwCAfWxdIyNJHo9Hffr0kSQdcsghWrRoke655x6dddZZCoVCqq2tbdeVqaqqUnFx8Xeez+v1yuv17umyZXEjGQAAbGd7R+bbYrGYgsGgDjnkELndbs2aNSu+b9WqVaqoqFB5ebmNFbbHEhkAAOxja0fmuuuu0wknnKDS0lI1NDToiSee0Jw5c/TGG28oKytL559/viZOnKjc3FwFAgFddtllKi8vt/+KJbFGBgCArsDWIFNdXa3f/va32rx5s7KysjR06FC98cYbOvbYYyVJd911lxwOh8aMGaNgMKjRo0frgQcesLPkndCQAQDAPrYGmYcffvh79/t8Pk2dOlVTp07tpIp2H0tkAACwX5dbI5NquI8MAAD2IcgkiI4MAAD2I8gkiIdGAgBgP4JMkphZAgDAPgSZBDG1BACA/QgySTJcgA0AgG0IMgAAIGURZJLEGhkAAOxDkEkQD40EAMB+BJkk0ZEBAMA+BJkE0Y8BAMB+BJkkcdUSAAD2IcgkiCUyAADYjyCTJNbIAABgH4JMgtqetUSOAQDAPgSZBDG1BACA/QgyyaIlAwCAbQgyCaIhAwCA/QgySeLyawAA7EOQSRBrZAAAsB9BJklcfg0AgH0IMgmjJQMAgN0IMkmiIQMAgH0IMglijQwAAPYjyCTJsEgGAADbEGQSREMGAAD7EWSSRD8GAAD7EGQSZLFIBgAA2xFkksQSGQAA7EOQSVBbP4YcAwCAfQgyCWJmCQAA+xFkksXcEgAAtiHIJIiODAAA9iPIJIl+DAAA9iHIJMjilngAANiOIJMklsgAAGAfgkyiaMgAAGA7gkySDKtkAACwDUEmQTRkAACwH0EmSayRAQDAPgSZBPHQSAAA7EeQSRIdGQAA7EOQSRAPjQQAwH4EmQQxswQAgP0IMkkyzC0BAGAbgkyCeEQBAAD2I8gAAICUlVCQeeyxxzRjxoz491dffbWys7N1xBFH6Isvvuiw4roy1sgAAGC/hILM7bffrrS0NEnS/PnzNXXqVN1xxx3Kz8/XlVde2aEFdnUskQEAwD6uRH5ow4YN6tOnjyTphRde0JgxY3TRRRdp5MiROuqoozqyvi6LhgwAAPZLqCPj9/u1detWSdKbb76pY489VpLk8/nU0tLScdWlAB4aCQCAfRLqyBx77LG64IILdNBBB2n16tU68cQTJUnLly9X7969O7K+rouWDAAAtkuoIzN16lSVl5erpqZGzz77rPLy8iRJS5Ys0TnnnNOhBXZ1rJEBAMA+CXVksrOzdf/99++0/ZZbbkm6oFTBfWQAALBfQh2Z119/Xe+99178+6lTp+rAAw/Uf/3Xf2n79u0dVlwqoCEDAIB9Egoyf/zjH1VfXy9J+uSTT/SHP/xBJ554otatW6eJEyd2aIFdFfeRAQDAfglNLa1bt06DBg2SJD377LP6xS9+odtvv10ffPBBfOHvvoJnLQEAYJ+EOjIej0fNzc2SpLfeekvHHXecJCk3NzfeqdnbtTVkiDEAANgnoY7MkUceqYkTJ2rkyJF6//339e9//1uStHr1avXs2bNDC+yqLOaWAACwXUIdmfvvv18ul0vPPPOMpk2bph49ekiSXnvtNR1//PEdWmCXR0sGAADbJNSRKS0t1SuvvLLT9rvuuivpglIFDRkAAOyXUJCRpGg0qhdeeEErVqyQJB1wwAE65ZRT5HQ6O6y4VEBDBgAA+yQUZNauXasTTzxRGzduVP/+/SVJkydPVklJiWbMmKH999+/Q4vsimjIAABgv4TWyFx++eXaf//9tWHDBn3wwQf64IMPVFFRobKyMl1++eUdXWOXxuXXAADYJ6GOzNy5c7VgwQLl5ubGt+Xl5WnKlCkaOXJkhxXXlbFGBgAA+yXUkfF6vWpoaNhpe2NjozweT9JFpRL6MQAA2CehIPOLX/xCF110kRYuXChjjIwxWrBggS6++GKdcsopu32eyZMna8SIEcrMzFRhYaFOO+00rVq1qt0xra2tGj9+vPLy8uT3+zVmzBhVVVUlUnYHoyUDAIDdEgoy9957r/bff3+Vl5fL5/PJ5/PpiCOOUJ8+fXT33Xfv9nnmzp2r8ePHa8GCBZo5c6bC4bCOO+44NTU1xY+58sor9fLLL+vpp5/W3LlztWnTJp1xxhmJlL1HsEQGAAD7WCaJ1apr166NX349cOBA9enTJ6liampqVFhYqLlz5+qnP/2p6urqVFBQoCeeeEK//OUvJUkrV67UwIEDNX/+fB1++OE/eM76+nplZWWprq5OgUAgqfq+6YOK7Trjgf+oNDdd71z98w47LwAA2P3f37u92PeHnmo9e/bs+Nd/+9vfdve07dTV1UlSfBHxkiVLFA6HNWrUqPgxAwYMUGlp6W4HmT3NsEoGAADb7HaQ+fDDD3fruESfQRSLxTRhwgSNHDlSgwcPliRVVlbK4/EoOzu73bFFRUWqrKzc5XmCwaCCwWD8+z31EEtWyAAAYL/dDjLf7LjsCePHj9eyZcv03nvvJXWeyZMn65Zbbumgqn4Ya2QAALBPQot9O9qll16qV155RbNnz2739Ozi4mKFQiHV1ta2O76qqkrFxcW7PNd1112nurq6+GvDhg17pOa2zhNBBgAA+9gaZIwxuvTSS/X888/r7bffVllZWbv9hxxyiNxut2bNmhXftmrVKlVUVKi8vHyX5/R6vQoEAu1eewJTSwAA2C/hh0Z2hPHjx+uJJ57Qiy++qMzMzPi6l6ysLKWlpSkrK0vnn3++Jk6cqNzcXAUCAV122WUqLy/vEgt9AQCAvWwNMtOmTZMkHXXUUe22T58+XePGjZMk3XXXXXI4HBozZoyCwaBGjx6tBx54oJMr3RmPKAAAwH62BpnduYWNz+fT1KlTNXXq1E6o6MfjoZEAANinSyz2TUUWq2QAALAdQSZJ9GMAALAPQSZBrJEBAMB+BJkksUQGAAD7EGQAAEDKIsgkiYdGAgBgH4JMglgjAwCA/QgySWKNDAAA9iHIJKjtPjLkGAAA7EOQSRBTSwAA2I8gkySmlgAAsA9BJkF0ZAAAsB9BJmm0ZAAAsAtBJkE8NBIAAPsRZJLEGhkAAOxDkEkQa2QAALAfQSZJNGQAALAPQSZBNGQAALAfQSZJhkUyAADYhiCTINbIAABgP4JMkujHAABgH4JMwmjJAABgN4JMklgiAwCAfQgyCWpbI8NiXwAA7EOQSRATSwAA2I8gkyT6MQAA2IcgkyCL668BALAdQSZZtGQAALANQSZBjrbFvvaWAQDAPo0gkyDHV1NLMa5aAgDANgSZBLUtkSHIAABgH4JMgr7uyNhcCAAA+zCCTILaggw3xAMAwD4EmQS1LfaN0pIBAMA2BJkEORxMLQEAYDeCTIIc37ghHtNLAADYgyCTIMc3buxLVwYAAHsQZBL0zUcUcAk2AAD2IMgkqH1HhiADAIAdCDIJar9GxsZCAADYhxFkEuRgagkAANsRZBJksdgXAADbEWQSREcGAAD7EWQS9M3FviZmXx0AAOzLCDIJoiMDAID9CDIJsrj8GgAA2xFkEmRZVjzMsNgXAAB7EGSS0Da9xLOWAACwB0EmCQ46MgAA2Iogk4S25y2xRgYAAHsQZJLwdUeGIAMAgB0IMkloWyMT4z4yAADYgiCTBAdTSwAA2IogkwSmlgAAsBdBJgkOR1tHxuZCAADYRxFkksB9ZAAAsBdBJgncRwYAAHsRZJLAfWQAALAXQSYJLPYFAMBeBJkkfL1GxuZCAADYRxFkksB9ZAAAsBdBJgkWi30BALAVQSYJdGQAALAXQSYJbYt9uY8MAAD2IMgk4euOjM2FAACwjyLIJCG+RoYkAwCALQgySaAjAwCAvWwNMu+8845OPvlkde/eXZZl6YUXXmi33xijm266Sd26dVNaWppGjRqlNWvW2FPsLvCsJQAA7GVrkGlqatKwYcM0derUXe6/4447dO+99+rBBx/UwoULlZGRodGjR6u1tbWTK901Lr8GAMBeLjvf/IQTTtAJJ5ywy33GGN19993605/+pFNPPVWS9I9//ENFRUV64YUXdPbZZ3dmqbvU1pGJ0pEBAMAWXXaNzLp161RZWalRo0bFt2VlZemwww7T/Pnzv/PngsGg6uvr2732FMdXo8d9ZAAAsEeXDTKVlZWSpKKionbbi4qK4vt2ZfLkycrKyoq/SkpK9liNTtbIAABgqy4bZBJ13XXXqa6uLv7asGHDHnsvq+2qpdgeewsAAPA9umyQKS4uliRVVVW1215VVRXftyter1eBQKDda09xxBf70pEBAMAOXTbIlJWVqbi4WLNmzYpvq6+v18KFC1VeXm5jZV/jPjIAANjL1quWGhsbtXbt2vj369at09KlS5Wbm6vS0lJNmDBBf/nLX9S3b1+VlZXpxhtvVPfu3XXaaafZV/Q3cB8ZAADsZWuQWbx4sX7+85/Hv584caIkaezYsXr00Ud19dVXq6mpSRdddJFqa2t15JFH6vXXX5fP57Or5Ha4jwwAAPayNcgcddRR39vNsCxLkyZN0qRJkzqxqt339dQSSQYAADt02TUyqYD7yAAAYC+CTBK+XiNjcyEAAOyjCDJJsJhaAgDAVgSZJDhY7AsAgK0IMklgsS8AAPYiyCQh3pGhJQMAgC0IMklwfpVkIgQZAABsQZBJgsu5Y/giUZ4aCQCAHQgySXDTkQEAwFYEmSS0dWTCUYIMAAB2IMgkwe38qiPD1BIAALYgyCTB9dUzCsJMLQEAYAuCTBJcdGQAALAVQSYJ7rarlujIAABgC4JMElxfXbUUpiMDAIAtCDJJ+Po+MnRkAACwA0EmCV/fR4aODAAAdiDIJIH7yAAAYC+CTBLa1shw1RIAAPYgyCSh7fJr7iMDAIA9CDJJaJtaijK1BACALQgySWCxLwAA9iLIJIHFvgAA2Isgk4T4QyPpyAAAYAuCTBLiD42kIwMAgC0IMkngoZEAANiLIJOEr6eW6MgAAGAHgkwSmFoCAMBeBJkkMLUEAIC9CDJJ8Hx1+XWIIAMAgC0IMknwuZ2SpNZw1OZKAADYNxFkkuBz7xi+1jAdGQAA7ECQSYLXRUcGAAA7EWSS0Da1FIzEZAxXLgEA0NkIMklI8zjjXwcjTC8BANDZCDJJ8Lm+Hj6mlwAA6HwEmSS4nA65HDvuJcOCXwAAOh9BJklcgg0AgH0IMkmKX4IdIcgAANDZCDJJ+voSbKaWAADobASZJH19Uzw6MgAAdDaCTJLa1si0EGQAAOh0BJkkZfpckqT6lrDNlQAAsO8hyCQpJ90jSaptJsgAANDZCDJJyv4qyGxvDtlcCQAA+x6CTJJy0t2S6MgAAGAHgkyScujIAABgG4JMkrK/6shsbSTIAADQ2QgySRrYLSBJen/9NtUxvQQAQKciyCTqg/+Tnj5PBzTM034FGQpFYnp//Ta7qwIAYJ9CkEnUxiXS8udkbV6q4b1yJElLN2y3uSgAAPYtBJlEZfXY8d+6jRrRO1eS9NonlYrFjI1FAQCwbyHIJCrwVZCp36gThnST3+vS51uadPSdc3TzS8t191urVd/KmhkAAPYkl90FpKy2IPP5bPln36jn9mvUu2u3KFYn6f0du56cY8nvdSnN7ZDH7VS6xyVjJL/PJcudri9DGXIU9FNx2QHKzO+m7MxMuZ0OFWR6ZYxRzEiWJIfDsutTAgDQpRFkEpW739dfL3hA/ST1c+7iuOhXr9add42QpM2SPpbCxqkNpkDb5NMnytOXsVxVmRw1OzKU5zNyxMKqMjnaFMtRUcCnaFq++g09TCN652q/ggyle/ijBADse/jtl6jsEunM/5MqP5FiX00hma/Xx8RiMbVEYqprDikcjaklFFVrOCJJamiNyB2uV1bDWnWLblSGWuW2ItrPqvzqp9e3n/T79gxV/Y7X6jd66PTQJGVn5+q8kb3VOy9DxwwslGXRwQEA7BssY8xevTq1vr5eWVlZqqurUyAQsLucXTNGqvtS2r5ejQ11at5SoWjdRqUHa6RgvcItjXJEQ3IoKjVvlSfSpPRgtSRpfnSQLg+PV412XDl1+H65Ov6AYp19aKl87l21iAAA6Pp29/c3QSYVGSM9/itp7cz4ps8cZXoiOFL/iB6nsFwa0iNL154wQCP75NtYKAAAiSHIfGWvDDJtPn1Reu0aqWFzfFOjK0czWofpfyJnqkbZ2r8gQ5PPGKpDy3JtLBQAgB+HIPOVvTrISFK4RfpstrTpQ2nJdKmpRpLU6kjXH1ov0IzY4cr0uTRhVD/9tryX3E6uuAcAdH0Ema/s9UHmm8KtUsV/pDf+JFUvlySNC12tObEDJUlH7J+nv44ZqpLcdBuLBADgh+3u72/+93xv4vZJ+x8t/X9zpQG/kCRN996p+4pfk6WY/vPZVv3kjtma+NRSNQYjNhcLAEDyCDJ7I6dbOm2atP8xskxUJ9f+n5b1e0RD82KSpOc+2KjBf35Ds1dW21woAADJIcjsrXwB6dfPSj+5SpKUUfG2XnT8Uc8euloOa0egOf+xRfp/73ymtdUNPCMKAJCSWCOzL1j3rvTi76XaCknS1rzh+snG36tZvvghpbnpenH8SKV7nfK6uP8MAMBeLPb9CkHmK8EGaclj0pwpUqhB0W4H6amCy3Td+76dDh3aM0uX/Gx/HdEnX1lpbhuKBQDs6/aqIDN16lT993//tyorKzVs2DDdd999OvTQQ3frZwky37JhkfTYL6TIjoc/mQG/0EPB4zRlRZ52PKLya06HJafDUigSU2GmV2OP6K2igE8uh6XCTK8kqTjLp6r6oMryM1SctXMo2h3GGB6rAABoZ68JMv/+97/129/+Vg8++KAOO+ww3X333Xr66ae1atUqFRYW/uDPE2R2Ydvn0qtXt7szsCkarHVpg1Wzdas8oVrNjw7Uq039tM0E5LSiSldQG0yhWuSRkUNuRRSOP6rLyKmYirP9ys/0al1No3rmpKspFFE4ElNrJCa3Ijr6gB6qbQ6roqZORTmZ2t4c0ocVtcrN8OjAkmzlZnjkclga2C0gy9oRq3rnZ+iN5ZXKTfcoaow+2VivfoV+HTOwSG6npXfXbNHyTXXav9Cvs4aXqDkUVW1zWKFoVH0LM+VyWmoORZXv96qmoVVfbm9RcZZP3bLS9OX2Zm2ubVVxlk8FmV6tqmzQwG4BZXidcliWIjGjhtawHJal6vqgGlrDOqB7loyM6lsiagxG1KfQL8uSXA5LX25vUZ7fo3SPS1sag/J7Xd/7mIhozKi2OaQ8v3eX+5tDkXYPAw1FYorEYj/4gFBjjLY3h5Wb4ZEkxWJGRlI4GlNtc1jFWT61hKJK8zjV0BpWhsf1o56wHorE5HF17PK6b4fZrhpuQ5GY3E5rj9UWi5l2fxZddRx2WzS84/EruWU779u0dMfDd307/l3+9me3XTQsWQ7JsRtT7bGY5NjNvxM/5tgfIxbbcT+xZc9II6+QMou/47jo7n2mLmCvCTKHHXaYRowYofvvv1/SjocxlpSU6LLLLtO11177gz9PkPkOxux44OXiR6SPnpQiLbv1Y62WVz4TlCQ1G6+2KVNF2i5LRutNsRyKyZJRsbVdq01PbTCF6mVVaoC1QctMmVyKaKBVocWmvzLUIo8iyrBaFTRu1cqvBpOuLKtJrcajamVrm8nU/tYmheVSrfzKVqPqlCFJihqHcq0GFVk73r9ZXn0W665Mq1mVJk8ehRWTpWqTo3yrTpUmV0XWdg1yfKGKWKFKHdXKVb0WxAap2mRrpGO5KkyhIpZLRlJlLEet8qjMqlQ3a6vejw2QQ0aN8smvVhVYtQrKLUuS2+1WY9hosKNClsurL0KZ2mzyJKdLw32bdGxolqJy6DXXKG1z5CpoebUl5FIkFNTB2U3qH/xE6yJ5Wme6yZmWpd6tK5QebVTQ4VWNq7sKrW1qDkmNMbd6OWvkcKcpI7RF2x3ZWukcoGJ3k4pNtQqi1cqPVOmt6IH60ttH4ZilnNBm9bRq5FRMm0ye1nv7aWOLR+kZmRoXfFzNjgzVWHlq9haoyre/0rxuBcJbZNV/qeZQRK60LOVZddraHFWaaZFbEbnT/OrpqtMWR4Eicmm7I0fyBuQNblFTWFqfebC6Na+Rx5emDL9fbodRZYtbh9S+oUBos2bnnqO6mFctDdtUE8tSpLVBlsOpQ4ocijZUKdBcocq0Purn2KQqV0/VRd0KxaT9rUrVK11WTm8FazfrkzqvirP9cvoyVJibK0e4QcHGOmV5jbZG/cqyGtVQX6d6yy/LnabPG5xyub0aalapIRhRpae3euelK8M0qql2i/LSncqO1SloeeRUVMGIUU0soGh6gRprvlBheKPqPcUKOIOKWm7lOxuVEchVjqNFuQ0rld38hdZkHKTNGYOU1bBW+4dXK+jOVlBuFbes0YaMwZInU00FB8pyOLRta41C0Zjc/nxFG2u0oaZO1Rn9NcyxVi1NjaoPW8r2p8vr9arFmaltTWFlOlrVIy9Lhdqqgqr39Kmjr1ocfgXcMWWmebTOKlHIk61Mq0WKhOSJNimzab0ymjaowtFTG9IHye9zKyfDo9LwekWNpZaoQ+mhGkXklCPSokZvsaL+YrmiLdoa9sgZrJMzUKycDJ+aQhGl5XSTN7RNkfUL5HI6lOEIS7GINlvFyoxuV9CXp3pvDw2pel7dWtZoi7NQm1ylajQeDTar1eTOV7fmlZKkRXmnKi9SrU31QRlftnr5Ywo21clnWuRwupUbqVbQlSkTDSlsXHKF6/Vp4Ccq0DZFAqVSuEUfVUfUnNVHBzk/1/pWv5z+fDktqcgXVpOvWLmmTmlbl6spItV5ipXvDikabtXylhzlmVplWi1qzR+qkJzKUb2ioVYN+/JxbbYKtLLHGBW6g/LUr5O/6Uutb/YoJ69Axpcjn4LqVrdUGc1faqF1oA7wVsljxRRzp8tfv1ZV3t76IFqm4kCaclxhda9dLCsWVk3+ocrctkwVgUNkmYjyI1VqcWerLuKVx+VQxFjKdEWU11ohV3OVtnl7yhVpUkZrpaLeHEUtlxQNK+pKU11aiUL+HupR+bZyWyvi/1avyPqpTFqO0qKNcimiYDiiwmCF/C2btDltf1WkHyArv7/SXEZhl1/5G15Xq5UuhzddkZgU3r5R7liLrIIBCnpzlRmqVsSbrW31TQpEtihYdJAivlz53C41pnVX6YAR6tmjR4f+mtorgkwoFFJ6erqeeeYZnXbaafHtY8eOVW1trV588cUfPAdBZjc0b5OWPyc1Vkvbv5A+flJypUkur9Raa3d1AIAubumAP+jAs2/q0HPu7u/v7+9R22zLli2KRqMqKipqt72oqEgrV67c5c8Eg0EFg8H49/X19Xu0xr1Ceq404oKvvz/joa+/btq6ow3pyZBCjTuCTs0qqaCfVLdRigal4qGSO02qWbmjjezNlKpXSi3bpfy+O+5r0xaMwi2Sie14PlRrnZTda0c3qGa15PLsOM7r33GOhkoFWxrlye4my3LsOI/HvyN4xSI7zp/TS8rqKa17R7HNH0neLBnLIWfhAEWcaQqHWuVt3qygK6C08HbJly1Tv0mh9CK5oy0y4RY5vOmK1W2SLKeCxQcr1lAt4/HLHapTrHm7akxAxRkOuVq3qSXqkOXyyLjS5AoUamtdg/K2LpHVWitH4UC1yKtmR4b8dWvUGrMUjYS1KeMAeZs3q2/TElXv/ys56irkjLbK4c2Q02GpMRhVTt2nckSDqs7op0xHUOH0IoVcfgU9ecpprVDYm630xgo1+nurKbC/Whprlb1xrjIitWrtdqi2xjJksnsp7PYru2618hwNamlq0Lb6esWMQ+lpaVJOb/WMfanW2io5WrYpGm6RMy1b1Rl95WisUixQoqzGtVI0opDDK68i8hT1U9OWDWp0ZklZJdreHFRuukdByytX3XpFLI8aIg4VRzapJeaUS1Hlmu3KCFYp5kxXi+XTVmXLaSLyW60KOnwqbl2nBnee0kJb5XA4FU4rkPFkKtTaqBZPvkwkqKgcismhnNhWNfnLFAyH1dwSlCcjS+7GjfI6jZyWUZMy5HXEFA02yh+tVdCVJY/LUtC4FYkZeUxIVlqOHKEGKdSoiJzyW0EF0wrkdUitrU2yJIUcPsUiYRlftlqdfrVGHaoKupSf7lR+/adyxEIKuTJ3dI48GfI4jCI5+6nZ8mtLU1h1MZ9qrFyNcKxRz9BnqnUXaltab6U7wtpuZasp4pC7caP8zrByTL1aI1G5YiGle92qdeQo1NKoMn2p/OCXqk/vpa35wxX25apu+1a5W2rU4gqoyNUsn0sKO9JU1RBUbmiz0lyW6ooPU8P2LWpoala6M6pejhrFnD61OtJ2TIWZoLanl0lNW9Q7uFINylDQkaFsd0jVJle1jmxlOVo1uOFdrfIN0xf+YcoMb1F6a5WalKbidMmSUWtzoyzLIb+aZIUaFJRPFWkD5HK5FZFT6aZJOdEtyo1ulTfWokZ3rjakD5YzPUtWuFk5NYu0PZYhK6ubXKEGFYQ3qsI3QM0xtzKitWqKubXN011fNrtU0eRS/wKfBoSWKytcrbW+wQpm7Pi//fTKheqR6VIwUCpnwyYFQjXKdzXLHW3VFitXjcarArNVrY50mXCz0p1GdSZNWzL6qknpygtvlhULqzKWraLmNcrJ8CgYKJNv+2o5rZiyItuUphb5TFCtGT20WfnaFvEq4IpoczRLn9b7VJzlVZ6zVVF3hjaZPIUjEaVF6pXuiMpvtagx6pLfGVGlu0QF4S+V5vXKirRoU9CnUqta4UhU6T6vNrc4tSGYoZIcnzLcUl3QqDXmUJZpUChmqcLVW9mZ6UqPNMjtduqd7fnKSXOpsalRA3It9YxukLv+C6U7Yqrw9dPjW/vpHM87yovWKCy3Wv0lqo6ky+eI6vM6qaffUqO/tw5pXaAcZ7NCLU3ymlZlROtVbKq1xVmoj7yHyOO0VBCpVHqsUVutXPmcUdVa2XJGg+oeXi853GoyXmWoWcGopUKzRdH8fp36a+ubunRHZtOmTerRo4f+85//qLy8PL796quv1ty5c7Vw4cKdfubmm2/WLbfcstN2OjIAAKSOveIRBfn5+XI6naqqqmq3vaqqSsXFu17IdN1116muri7+2rBhQ2eUCgAAbNClg4zH49EhhxyiWbNmxbfFYjHNmjWrXYfmm7xerwKBQLsXAADYO3XpNTKSNHHiRI0dO1bDhw/XoYceqrvvvltNTU0677zz7C4NAADYrMsHmbPOOks1NTW66aabVFlZqQMPPFCvv/76TguAAQDAvqdLL/btCFx+DQBA6tkrFvsCAAB8H4IMAABIWQQZAACQsggyAAAgZRFkAABAyiLIAACAlEWQAQAAKYsgAwAAUhZBBgAApKwu/4iCZLXduLi+vt7mSgAAwO5q+739Qw8g2OuDTENDgySppKTE5koAAMCP1dDQoKysrO/cv9c/aykWi2nTpk3KzMyUZVkddt76+nqVlJRow4YNPMNpD2OsOwfj3DkY587DWHeOPTXOxhg1NDSoe/fucji+eyXMXt+RcTgc6tmz5x47fyAQ4C9IJ2GsOwfj3DkY587DWHeOPTHO39eJacNiXwAAkLIIMgAAIGURZBLk9Xr15z//WV6v1+5S9nqMdedgnDsH49x5GOvOYfc47/WLfQEAwN6LjgwAAEhZBBkAAJCyCDIAACBlEWQAAEDKIsgkaOrUqerdu7d8Pp8OO+wwvf/++3aXlFImT56sESNGKDMzU4WFhTrttNO0atWqdse0trZq/PjxysvLk9/v15gxY1RVVdXumIqKCp100klKT09XYWGh/vjHPyoSiXTmR0kpU6ZMkWVZmjBhQnwb49wxNm7cqF//+tfKy8tTWlqahgwZosWLF8f3G2N00003qVu3bkpLS9OoUaO0Zs2adufYtm2bzj33XAUCAWVnZ+v8889XY2NjZ3+ULisajerGG29UWVmZ0tLStP/+++vWW29t9ywexjkx77zzjk4++WR1795dlmXphRdeaLe/o8b1448/1k9+8hP5fD6VlJTojjvuSL54gx/tySefNB6PxzzyyCNm+fLl5sILLzTZ2dmmqqrK7tJSxujRo8306dPNsmXLzNKlS82JJ55oSktLTWNjY/yYiy++2JSUlJhZs2aZxYsXm8MPP9wcccQR8f2RSMQMHjzYjBo1ynz44Yfm1VdfNfn5+ea6666z4yN1ee+//77p3bu3GTp0qLniiivi2xnn5G3bts306tXLjBs3zixcuNB8/vnn5o033jBr166NHzNlyhSTlZVlXnjhBfPRRx+ZU045xZSVlZmWlpb4Mccff7wZNmyYWbBggXn33XdNnz59zDnnnGPHR+qSbrvtNpOXl2deeeUVs27dOvP0008bv99v7rnnnvgxjHNiXn31VXPDDTeY5557zkgyzz//fLv9HTGudXV1pqioyJx77rlm2bJl5l//+pdJS0szDz30UFK1E2QScOihh5rx48fHv49Go6Z79+5m8uTJNlaV2qqrq40kM3fuXGOMMbW1tcbtdpunn346fsyKFSuMJDN//nxjzI6/eA6Hw1RWVsaPmTZtmgkEAiYYDHbuB+jiGhoaTN++fc3MmTPNz372s3iQYZw7xjXXXGOOPPLI79wfi8VMcXGx+e///u/4ttraWuP1es2//vUvY4wxn376qZFkFi1aFD/mtddeM5ZlmY0bN+654lPISSedZH73u9+123bGGWeYc8891xjDOHeUbweZjhrXBx54wOTk5LT7d+Oaa64x/fv3T6peppZ+pFAopCVLlmjUqFHxbQ6HQ6NGjdL8+fNtrCy11dXVSZJyc3MlSUuWLFE4HG43zgMGDFBpaWl8nOfPn68hQ4aoqKgofszo0aNVX1+v5cuXd2L1Xd/48eN10kkntRtPiXHuKC+99JKGDx+uX/3qVyosLNRBBx2kv//97/H969atU2VlZbtxzsrK0mGHHdZunLOzszV8+PD4MaNGjZLD4dDChQs778N0YUcccYRmzZql1atXS5I++ugjvffeezrhhBMkMc57SkeN6/z58/XTn/5UHo8nfszo0aO1atUqbd++PeH69vqHRna0LVu2KBqNtvtHXZKKioq0cuVKm6pKbbFYTBMmTNDIkSM1ePBgSVJlZaU8Ho+ys7PbHVtUVKTKysr4Mbv6c2jbhx2efPJJffDBB1q0aNFO+xjnjvH5559r2rRpmjhxoq6//notWrRIl19+uTwej8aOHRsfp12N4zfHubCwsN1+l8ul3Nxcxvkr1157rerr6zVgwAA5nU5Fo1HddtttOvfccyWJcd5DOmpcKysrVVZWttM52vbl5OQkVB9BBrYbP368li1bpvfee8/uUvY6GzZs0BVXXKGZM2fK5/PZXc5eKxaLafjw4br99tslSQcddJCWLVumBx98UGPHjrW5ur3HU089pccff1xPPPGEDjjgAC1dulQTJkxQ9+7dGed9GFNLP1J+fr6cTudOV3VUVVWpuLjYpqpS16WXXqpXXnlFs2fPVs+ePePbi4uLFQqFVFtb2+74b45zcXHxLv8c2vZhx9RRdXW1Dj74YLlcLrlcLs2dO1f33nuvXC6XioqKGOcO0K1bNw0aNKjdtoEDB6qiokLS1+P0ff9uFBcXq7q6ut3+SCSibdu2Mc5f+eMf/6hrr71WZ599toYMGaLf/OY3uvLKKzV58mRJjPOe0lHjuqf+LSHI/Egej0eHHHKIZs2aFd8Wi8U0a9YslZeX21hZajHG6NJLL9Xzzz+vt99+e6d24yGHHCK3291unFetWqWKior4OJeXl+uTTz5p95dn5syZCgQCO/1S2Vcdc8wx+uSTT7R06dL4a/jw4Tr33HPjXzPOyRs5cuROtw9YvXq1evXqJUkqKytTcXFxu3Gur6/XwoUL241zbW2tlixZEj/m7bffViwW02GHHdYJn6Lra25ulsPR/teW0+lULBaTxDjvKR01ruXl5XrnnXcUDofjx8ycOVP9+/dPeFpJEpdfJ+LJJ580Xq/XPProo+bTTz81F110kcnOzm53VQe+3yWXXGKysrLMnDlzzObNm+Ov5ubm+DEXX3yxKS0tNW+//bZZvHixKS8vN+Xl5fH9bZcFH3fccWbp0qXm9ddfNwUFBVwW/AO+edWSMYxzR3j//feNy+Uyt912m1mzZo15/PHHTXp6uvnnP/8ZP2bKlCkmOzvbvPjii+bjjz82p5566i4vXz3ooIPMwoULzXvvvWf69u27z18W/E1jx441PXr0iF9+/dxzz5n8/Hxz9dVXx49hnBPT0NBgPvzwQ/Phhx8aSeZvf/ub+fDDD80XX3xhjOmYca2trTVFRUXmN7/5jVm2bJl58sknTXp6Opdf2+W+++4zpaWlxuPxmEMPPdQsWLDA7pJSiqRdvqZPnx4/pqWlxfz+9783OTk5Jj093Zx++ulm8+bN7c6zfv16c8IJJ5i0tDSTn59v/vCHP5hwONzJnya1fDvIMM4d4+WXXzaDBw82Xq/XDBgwwPy///f/2u2PxWLmxhtvNEVFRcbr9ZpjjjnGrFq1qt0xW7duNeecc47x+/0mEAiY8847zzQ0NHTmx+jS6uvrzRVXXGFKS0uNz+cz++23n7nhhhvaXc7LOCdm9uzZu/w3eezYscaYjhvXjz76yBx55JHG6/WaHj16mClTpiRdu2XMN26JCAAAkEJYIwMAAFIWQQYAAKQsggwAAEhZBBkAAJCyCDIAACBlEWQAAEDKIsgAAICURZABsM+ZM2eOLMva6RlTAFIPQQYAAKQsggwAAEhZBBkAnS4Wi2ny5MkqKytTWlqahg0bpmeeeUbS19M+M2bM0NChQ+Xz+XT44Ydr2bJl7c7x7LPP6oADDpDX61Xv3r115513ttsfDAZ1zTXXqKSkRF6vV3369NHDDz/c7pglS5Zo+PDhSk9P1xFHHLHTE6wBdH0EGQCdbvLkyfrHP/6hBx98UMuXL9eVV16pX//615o7d278mD/+8Y+68847tWjRIhUUFOjkk09WOByWtCOAnHnmmTr77LP1ySef6Oabb9aNN96oRx99NP7zv/3tb/Wvf/1L9957r1asWKGHHnpIfr+/XR033HCD7rzzTi1evFgul0u/+93vOuXzA+g4PDQSQKcKBoPKzc3VW2+9pfLy8vj2Cy64QM3Nzbrooov085//XE8++aTOOussSdK2bdvUs2dPPfroozrzzDN17rnnqqamRm+++Wb856+++mrNmDFDy5cv1+rVq9W/f3/NnDlTo0aN2qmGOXPm6Oc//7neeustHXPMMZKkV199VSeddJJaWlrk8/n28CgA6Ch0ZAB0qrVr16q5uVnHHnus/H5//PWPf/xDn332Wfy4b4ac3Nxc9e/fXytWrJAkrVixQiNHjmx33pEjR2rNmjWKRqNaunSpnE6nfvazn31vLUOHDo1/3a1bN0lSdXV10p8RQOdx2V0AgH1LY2OjJGnGjBnq0aNHu31er7ddmElUWlrabh3ndrvjX1uWJWnH+h0AqYOODIBONWjQIHm9XlVUVKhPnz7tXiUlJfHjFixYEP96+/btWr16tQYOHChJGjhwoObNm9fuvPPmzVO/fv3kdDo1ZMgQxWKxdmtuAOyd6MgA6FSZmZm66qqrdOWVVyoWi+nII49UXV2d5s2bp0AgoF69ekmSJk2apLy8PBUVFemGG25Qfn6+TjvtNEnSH/7wB40YMUK33nqrzjrrLM2fP1/333+/HnjgAUlS7969NXbsWP3ud7/Tvffeq2HDhumLL75QdXW1zjzzTLs+OoA9gCADoNPdeuutKigo0OTJk/X5558rOztbBx98sK6//vr41M6UKVN0xRVXaM2aNTrwwAP18ssvy+PxSJIOPvhgPfXUU7rpppt06623qlu3bpo0aZLGjRsXf49p06bp+uuv1+9//3tt3bpVpaWluv766+34uAD2IK5aAtCltF1RtH37dmVnZ9tdDoAujjUyAAAgZRFkAABAymJqCQAApCw6MgAAIGURZAAAQMoiyAAAgJRFkAEAACmLIAMAAFIWQQYAAKQsggwAAEhZBBkAAJCyCDIAACBl/f8T1gSilT/3OgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = [\"chords/Bdim_RockGB_JO_4.wav\", \"chords/Bb_Electric1_LInda_2.wav\",\n",
        "              \"chords/C_Classic_Jo_1.wav\", \"chords/chord.wav\",\n",
        "              \"chords/G_AcusticVince_JO_1.wav\"]\n",
        "\n",
        "# For loop through test files\n",
        "for file in test_files:\n",
        "    # load file\n",
        "    audio, sample_rate = librosa.load(file)\n",
        "    # Get features\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    # Scaled features\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "\n",
        "    # print(mfccs_scaled_features)\n",
        "    mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
        "    print(mfccs_scaled_features)\n",
        "    print(mfccs_scaled_features.shape)\n",
        "\n",
        "    # Get predicted label\n",
        "    predicted_label=model.predict(mfccs_scaled_features)\n",
        "    predicted_label=np.argmax(predicted_label,axis=1)\n",
        "\n",
        "    print(\"File: \", file)\n",
        "    print(\"predicted label is:\",predicted_label)\n",
        "    # Get predicted class name\n",
        "    prediction_class = label_encoder.inverse_transform(predicted_label)\n",
        "    print(\"predicted class is:\",prediction_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrxdNpzgnZ7u",
        "outputId": "0b39f244-d892-427a-e8e2-b2846a9e8334"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.1220432e+02  7.1093140e+01 -3.0165697e+01  5.5907688e+01\n",
            "  -1.4442878e+01  4.8134212e+00 -2.2546684e+01 -1.3784544e+01\n",
            "  -9.6011734e+00 -1.2725350e+01 -2.0233605e+00 -8.1530256e+00\n",
            "  -1.1482180e+01 -7.9215965e+00 -7.3184156e+00 -5.3567896e+00\n",
            "  -1.1955849e+01 -6.4411297e+00 -1.0386440e+01 -1.8103266e+00\n",
            "  -1.1701501e+01  2.4438069e+00  2.9078271e+00  8.2258396e+00\n",
            "   3.4053659e+00  2.4696894e+00 -3.3231616e+00 -8.4166842e+00\n",
            "  -5.9013968e+00 -4.5755558e+00 -3.3490520e+00 -1.1638453e+00\n",
            "  -5.5432286e+00  1.6126597e+00  3.2152085e+00  2.7009010e-02\n",
            "  -6.8148785e+00 -7.5753398e+00 -5.1289539e+00 -2.5833702e-01]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "File:  chords/Bdim_RockGB_JO_4.wav\n",
            "predicted label is: [2]\n",
            "predicted class is: ['Bdim']\n",
            "[[-5.3456866e+02  8.5230431e+01 -4.7279991e+01  2.0553389e+01\n",
            "   1.3193236e+01  2.3905659e+01 -8.6077099e+00 -5.0670538e+00\n",
            "  -2.3942583e+01 -8.5444021e+00 -1.4136621e+01 -2.3235178e+01\n",
            "  -6.1424060e+00 -9.2839327e+00 -3.9793158e+00 -1.0189395e+01\n",
            "  -3.3851733e+00 -1.6645731e+01 -5.6039944e+00 -1.8311441e+00\n",
            "  -6.8125429e+00 -8.4368312e-01 -2.1010022e+00  2.9404442e+00\n",
            "  -4.8138335e-01  3.3144381e+00 -3.5105278e+00 -1.4788992e+00\n",
            "   2.5869994e+00  8.9469433e+00  9.7369089e+00 -2.4125490e-01\n",
            "  -1.0576669e+01 -1.3869654e+01 -1.0867851e+00 -2.0565486e+00\n",
            "  -9.1935902e+00 -1.2638835e+01  1.9950377e+00  3.4274571e+00]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "File:  chords/Bb_Electric1_LInda_2.wav\n",
            "predicted label is: [1]\n",
            "predicted class is: ['Bb']\n",
            "[[-459.4093     100.89385     46.27863     19.993307     7.479972\n",
            "     0.607794    -5.512344   -10.646012   -11.40859    -10.33945\n",
            "    -9.034883    -7.5329127   -6.2356277   -5.242823    -4.294742\n",
            "    -7.369995   -10.249381    -9.824185   -10.662322    -8.409566\n",
            "    -6.0548015   -6.731301    -5.360643    -4.285399    -3.2345612\n",
            "    -1.9627753   -3.2126467   -6.713152   -12.264038   -16.952154\n",
            "   -17.296358   -12.497961    -7.015413    -5.779436    -6.099116\n",
            "    -5.727608    -5.770198    -6.2465606   -5.6268053   -1.8868704]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "File:  chords/C_Classic_Jo_1.wav\n",
            "predicted label is: [3]\n",
            "predicted class is: ['C']\n",
            "[[-3.5539542e+02  1.3169052e+02  3.4965816e+01  2.6837353e+01\n",
            "   1.2195112e+01  1.1667685e+01  2.4631754e-01 -6.9712467e+00\n",
            "  -1.3169348e+01 -2.1260145e+01 -2.0845222e+01 -2.0024040e+01\n",
            "  -9.0972233e+00 -5.4287081e+00 -6.7676082e+00 -9.5470066e+00\n",
            "  -1.4838221e+01 -6.2020154e+00 -1.6736717e+00  1.1238071e+00\n",
            "   2.7252667e+00 -9.1705379e+00 -9.3424339e+00 -9.0916958e+00\n",
            "  -4.5428867e+00  6.8849926e+00  6.2024670e+00  4.8745170e+00\n",
            "  -7.1169043e+00 -1.4086275e+01 -8.6945419e+00 -4.5524907e+00\n",
            "   2.2783999e+00 -4.5691442e+00 -1.6067802e+01 -1.7444672e+01\n",
            "  -1.3891146e+01 -4.6542044e+00 -5.1116362e+00 -1.2922568e+01]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "File:  chords/chord.wav\n",
            "predicted label is: [0]\n",
            "predicted class is: ['Am']\n",
            "[[-3.9459625e+02  9.3272408e+01  1.2857545e+01  1.5799344e+01\n",
            "   6.3852863e+00  7.3510394e+00  4.1027913e+00  8.0344802e-01\n",
            "  -2.8327088e+00 -5.9228868e+00 -7.3036036e+00 -1.0077367e+01\n",
            "  -1.1252743e+01 -9.3895712e+00 -3.8910646e+00 -5.5302434e+00\n",
            "  -8.3674908e+00 -6.5397110e+00 -4.4298444e+00 -6.0989428e+00\n",
            "  -7.1189094e+00 -6.8409925e+00 -7.1329756e+00 -6.4310102e+00\n",
            "  -6.6145878e+00 -3.6649106e+00 -2.8826141e+00 -1.6737482e-01\n",
            "   4.6281881e+00 -1.0644820e+00 -1.1112585e+01 -1.3723181e+01\n",
            "  -8.2041321e+00  6.3887715e-01  7.1254177e+00  5.5067015e+00\n",
            "  -1.2887650e+00 -6.4937801e+00 -8.9819469e+00 -5.3036275e+00]]\n",
            "(1, 40)\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "File:  chords/G_AcusticVince_JO_1.wav\n",
            "predicted label is: [7]\n",
            "predicted class is: ['G']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# save model\n",
        "pickle.dump(model, open('model.pkl','wb'))"
      ],
      "metadata": {
        "id": "VgZISiNOpR-h"
      },
      "execution_count": 119,
      "outputs": []
    }
  ]
}